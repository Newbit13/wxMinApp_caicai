{"version":3,"sources":["index.js","create_detector.js","blazepose_mediapipe/detector.js","constants.js","blazepose_mediapipe/detector_utils.js","blazepose_mediapipe/constants.js","blazepose_tfjs/detector.js","calculators/constants.js","calculators/convert_image_to_tensor.js","calculators/image_utils.js","calculators/is_video.js","calculators/keypoints_smoothing.js","calculators/get_object_scale.js","calculators/keypoints_one_euro_filter.js","calculators/one_euro_filter.js","calculators/low_pass_filter.js","calculators/keypoints_to_normalized_keypoints.js","calculators/keypoints_velocity_filter.js","calculators/relative_velocity_filter.js","calculators/normalized_keypoints_to_keypoints.js","calculators/shift_image_value.js","blazepose_tfjs/calculators/calculate_alignment_points_rects.js","blazepose_tfjs/calculators/detection_to_rect.js","blazepose_tfjs/calculators/calculate_landmark_projection.js","blazepose_tfjs/calculators/create_ssd_anchors.js","blazepose_tfjs/calculators/detector_inference.js","blazepose_tfjs/calculators/split_detection_result.js","blazepose_tfjs/calculators/landmarks_to_detection.js","blazepose_tfjs/calculators/non_max_suppression.js","blazepose_tfjs/calculators/refine_landmarks_from_heatmap.js","blazepose_tfjs/calculators/remove_detection_letterbox.js","blazepose_tfjs/calculators/remove_landmark_letterbox.js","blazepose_tfjs/calculators/tensors_to_detections.js","blazepose_tfjs/calculators/tensors_to_landmarks.js","calculators/sigmoid.js","blazepose_tfjs/calculators/transform_rect.js","blazepose_tfjs/calculators/visibility_smoothing.js","blazepose_tfjs/constants.js","blazepose_tfjs/detector_utils.js","movenet/detector.js","types.js","util.js","movenet/constants.js","movenet/detector_utils.js","posenet/detector.js","posenet/calculators/decode_multiple_poses.js","posenet/constants.js","posenet/calculators/build_part_with_score_queue.js","posenet/calculators/max_heap.js","posenet/calculators/decode_multiple_poses_util.js","posenet/calculators/decode_single_pose.js","posenet/calculators/decode_single_pose_util.js","posenet/calculators/flip_poses.js","posenet/calculators/scale_poses.js","posenet/detector_utils.js","posenet/load_utils.js"],"names":[],"mappings":";;;;;;;AAAA;AACA;AACA;ACFA,ADGA;ACFA,ADGA;ACFA,ADGA;AELA,ADGA,ADGA;AELA,ADGA,ADGA;AELA,ADGA,ADGA;AELA,ACHA,AFMA,ADGA;AELA,ACHA,AFMA,ADGA;AELA,ACHA,AFMA,ADGA;AELA,AENA,ADGA,AFMA,ADGA;AELA,AENA,ADGA,AFMA,ADGA;AELA,AENA,ADGA,AFMA,ADGA;AKdA,AHSA,AENA,ADGA,AFMA,ADGA;AKdA,AHSA,AENA,ADGA,AFMA,ADGA;AKdA,AHSA,AENA,ADGA,AFMA,ADGA;AKdA,AHSA,AENA,AENA,AHSA,AFMA,ADGA;AKdA,AHSA,AENA,AENA,AHSA,AFMA,ADGA;AKdA,AHSA,AENA,AENA,AHSA,AFMA,ADGA;AKdA,AHSA,AENA,AENA,ACHA,AJYA,AFMA,ADGA;AKdA,AHSA,AENA,AENA,ACHA,AJYA,AFMA,ADGA;AKdA,AHSA,AENA,AENA,ACHA,AJYA,AFMA,ADGA;AKdA,AHSA,AENA,AENA,ACHA,ACHA,ALeA,AFMA,ADGA;AKdA,AHSA,AENA,AENA,ACHA,ACHA,ALeA,AFMA,ADGA;AKdA,AHSA,AENA,AENA,ACHA,ACHA,ALeA,AFMA,ADGA;AELA,AENA,AENA,ACHA,ACHA,ACHA,ANkBA,AFMA;ACFA,AENA,AENA,ACHA,ACHA,ACHA,ANkBA,AFMA;ACFA,AENA,AENA,ACHA,ACHA,ACHA,ANkBA,AFMA;ACFA,AENA,AENA,ACHA,ACHA,ACHA,ACHA,APqBA,AFMA;ACFA,AENA,AENA,ACHA,ACHA,ACHA,ACHA,APqBA,AFMA;ACFA,AENA,AENA,ACHA,ACHA,ACHA,ACHA,APqBA,AFMA;ACFA,AENA,AENA,ACHA,ACHA,ACHA,ACHA,ACHA,ARwBA,AFMA;ACFA,AENA,AENA,ACHA,ACHA,ACHA,ACHA,ACHA,ARwBA,AFMA;ACFA,AENA,AENA,ACHA,ACHA,ACHA,ACHA,ACHA,ARwBA,AFMA;ACFA,AENA,AENA,ACHA,ACHA,AIZA,AHSA,ACHA,ACHA,ARwBA,AFMA;ACFA,AENA,AENA,ACHA,ACHA,AIZA,AHSA,ACHA,ACHA,ARwBA,AFMA;ACFA,AENA,AENA,ACHA,ACHA,AIZA,AHSA,ACHA,ACHA,ARwBA,AFMA;ACFA,AENA,AENA,ACHA,ACHA,AIZA,AHSA,ACHA,AGTA,AFMA,ARwBA,AFMA;ACFA,AENA,AENA,ACHA,ACHA,AIZA,AHSA,ACHA,AGTA,AFMA,ARwBA,AFMA;ACFA,AENA,AENA,ACHA,ACHA,AIZA,AHSA,ACHA,AGTA,AFMA,ARwBA,AFMA;ACFA,AENA,AENA,ACHA,ACHA,AIZA,AHSA,ACHA,AGTA,AFMA,AGTA,AXiCA,AFMA;ACFA,AENA,AENA,AENA,AIZA,AHSA,ACHA,AGTA,AFMA,AGTA,AXiCA,AFMA;ACFA,AENA,AENA,AENA,AIZA,AHSA,ACHA,AGTA,AFMA,AGTA,AXiCA,AFMA;ACFA,AENA,AENA,AENA,AIZA,AHSA,ACHA,AGTA,AFMA,AIZA,ADGA,AXiCA,AFMA;ACFA,AENA,AENA,AENA,AIZA,AHSA,ACHA,AGTA,AFMA,AIZA,ADGA,AXiCA,AFMA;ACFA,AENA,AENA,AENA,AIZA,AHSA,ACHA,AGTA,AFMA,AIZA,ADGA,AXiCA,AFMA;ACFA,AENA,AENA,AENA,AIZA,AHSA,ACHA,AGTA,AFMA,AKfA,ADGA,ADGA,AXiCA,AFMA;ACFA,AENA,AENA,AENA,AIZA,AHSA,ACHA,AGTA,AFMA,AKfA,ADGA,ADGA,AXiCA,AFMA;ACFA,AENA,AENA,AENA,AIZA,AHSA,ACHA,AGTA,AFMA,AKfA,ADGA,ADGA,AXiCA,AFMA;ACFA,AENA,AENA,AENA,AIZA,AHSA,ACHA,AGTA,AFMA,AKfA,ACHA,AFMA,ADGA,AXiCA,AFMA;ACFA,AENA,AENA,AENA,ACHA,ACHA,AGTA,AFMA,AKfA,ACHA,AFMA,ADGA,AXiCA,AFMA;ACFA,AENA,AENA,AENA,ACHA,AIZA,AFMA,AKfA,ACHA,AFMA,ADGA,AXiCA,AFMA;ACFA,AENA,AENA,AENA,ACHA,AIZA,AFMA,AKfA,ACHA,AFMA,ADGA,AIZA,Af6CA,AFMA;ACFA,AENA,AENA,AENA,ACHA,AIZA,AFMA,AKfA,ACHA,AFMA,ADGA,AIZA,Af6CA,AFMA;ACFA,AENA,AENA,AENA,ACHA,AIZA,AFMA,AKfA,ACHA,AFMA,ADGA,AIZA,Af6CA,AFMA;ACFA,AENA,AENA,AENA,ACHA,AIZA,AFMA,AKfA,ACHA,AFMA,AIZA,ALeA,AIZA,Af6CA,AFMA;ACFA,AENA,AENA,AENA,ACHA,AIZA,AFMA,AKfA,ACHA,AFMA,AIZA,ALeA,AIZA,Af6CA,AFMA;ACFA,AENA,AENA,AENA,ACHA,AIZA,AFMA,AKfA,ACHA,AFMA,AIZA,ALeA,AIZA,Af6CA,AFMA;ACFA,AENA,AENA,AENA,ACHA,AIZA,AFMA,AKfA,ACHA,AFMA,AIZA,ALeA,AIZA,AENA,AjBmDA,AFMA;ACFA,AENA,AENA,AENA,ACHA,AIZA,AFMA,AKfA,ACHA,AFMA,AIZA,ALeA,AIZA,AENA,AjBmDA,AFMA;ACFA,AENA,AENA,AENA,ACHA,AIZA,AFMA,AKfA,ACHA,AFMA,AIZA,ALeA,AIZA,AENA,AjBmDA,AFMA;ACFA,AENA,AiBnDA,Af6CA,AENA,ACHA,AIZA,AFMA,AKfA,ACHA,AFMA,AIZA,ALeA,AIZA,AENA,AjBmDA,AFMA;ACFA,AENA,AiBnDA,Af6CA,AENA,ACHA,AIZA,AFMA,AKfA,ACHA,AFMA,AIZA,ALeA,AIZA,AENA,AjBmDA,AFMA;ACFA,AENA,AiBnDA,Af6CA,AENA,ACHA,AIZA,AFMA,AKfA,ACHA,AFMA,AIZA,ALeA,AIZA,AENA,AjBmDA,AFMA;ACFA,AENA,AiBnDA,ACHA,AhBgDA,AENA,ACHA,AIZA,AFMA,AKfA,ACHA,AFMA,AIZA,ALeA,AIZA,AENA,AjBmDA,AFMA;ACFA,AENA,AiBnDA,ACHA,AhBgDA,AENA,ACHA,AIZA,AFMA,AKfA,ACHA,AFMA,AIZA,ALeA,AIZA,AENA,AjBmDA,AFMA;ACFA,AENA,AiBnDA,ACHA,AhBgDA,AENA,ACHA,AIZA,AFMA,AKfA,ACHA,AFMA,AIZA,ALeA,AIZA,AENA,AjBmDA,AFMA;ACFA,AENA,AiBnDA,AENA,ADGA,AhBgDA,AENA,ACHA,AIZA,AFMA,AKfA,ACHA,AFMA,AIZA,ALeA,AIZA,AENA,AjBmDA,AFMA;ACFA,AENA,AiBnDA,AENA,ADGA,AhBgDA,AENA,ACHA,AIZA,AFMA,AKfA,ACHA,AFMA,AIZA,ALeA,AIZA,AENA,AjBmDA,AFMA;ACFA,AENA,AiBnDA,AENA,ADGA,AhBgDA,AENA,ACHA,AIZA,AFMA,AKfA,ACHA,AFMA,AIZA,ALeA,AIZA,AENA,AjBmDA,AFMA;ACFA,AENA,AiBnDA,AENA,ACHA,AFMA,AhBgDA,AENA,ACHA,AIZA,AFMA,AKfA,ACHA,AFMA,AIZA,ALeA,AIZA,AENA,AjBmDA,AFMA;ACFA,AENA,AiBnDA,AENA,ACHA,AFMA,AhBgDA,AENA,ACHA,AIZA,AFMA,AMlBA,AFMA,AIZA,ALeA,AIZA,AENA,AjBmDA,AFMA;ACFA,AENA,AiBnDA,AENA,ACHA,AFMA,AhBgDA,AGTA,AIZA,AFMA,AMlBA,AFMA,AIZA,ALeA,AIZA,AENA,AjBmDA,AFMA;ACFA,AENA,AiBnDA,AENA,ACHA,AFMA,AGTA,AnByDA,AGTA,AIZA,AFMA,AMlBA,AFMA,AIZA,ALeA,AIZA,AENA,AjBmDA,AFMA;ACFA,AENA,AiBnDA,AENA,ACHA,AFMA,AGTA,AnByDA,AGTA,AIZA,AFMA,AMlBA,AFMA,AIZA,ALeA,AIZA,AENA,AjBmDA,AFMA;ACFA,AENA,AiBnDA,AENA,ACHA,AFMA,AGTA,AnByDA,AGTA,AIZA,AFMA,AMlBA,AFMA,AIZA,ALeA,AIZA,AENA,AjBmDA,AFMA;ACFA,AENA,AiBnDA,AENA,ACHA,AFMA,AGTA,ACHA,ApB4DA,AGTA,AIZA,AFMA,AMlBA,AFMA,AIZA,ALeA,AIZA,AENA,AjBmDA,AFMA;ACFA,AmBzDA,AENA,ACHA,AFMA,AGTA,ACHA,ApB4DA,AGTA,AIZA,AFMA,AMlBA,AFMA,AIZA,ALeA,AIZA,AENA,AjBmDA,AFMA;ACFA,AmBzDA,AENA,ACHA,AFMA,AGTA,ACHA,ApB4DA,AGTA,AIZA,AFMA,AMlBA,AFMA,AIZA,ALeA,AIZA,AENA,AjBmDA,AFMA;ACFA,AmBzDA,AENA,ACHA,AFMA,AGTA,AENA,ADGA,ApB4DA,AGTA,AIZA,AFMA,AMlBA,AFMA,AIZA,ALeA,AIZA,AENA,AjBmDA,AFMA;ACFA,AmBzDA,AENA,ACHA,AFMA,AGTA,AENA,ADGA,ApB4DA,AGTA,AIZA,AFMA,AMlBA,AFMA,ADGA,AIZA,AENA,AjBmDA,AFMA;ACFA,AmBzDA,AENA,ACHA,AFMA,AGTA,AENA,ADGA,ApB4DA,AGTA,AIZA,AFMA,AMlBA,AFMA,ADGA,AIZA,AENA,AjBmDA,AFMA;ACFA,AmBzDA,AENA,ACHA,AFMA,AGTA,AENA,ACHA,AFMA,ApB4DA,AGTA,AIZA,AFMA,AMlBA,AFMA,ADGA,AIZA,AENA,AjBmDA,AFMA;ACFA,AmBzDA,AENA,ACHA,AFMA,AGTA,AENA,ACHA,AFMA,ApB4DA,AGTA,AIZA,AFMA,AMlBA,AFMA,ADGA,AIZA,AENA,AjBmDA,AFMA;ACFA,AmBzDA,AENA,ACHA,AFMA,AGTA,AENA,ACHA,AFMA,ApB4DA,AGTA,AIZA,AFMA,AMlBA,AFMA,ADGA,AIZA,AENA,AjBmDA,AFMA;ACFA,AmBzDA,AENA,ACHA,AFMA,AGTA,AENA,ACHA,ACHA,AHSA,ApB4DA,AGTA,AIZA,AFMA,AMlBA,AFMA,ADGA,AIZA,AENA,AjBmDA,AFMA;ACFA,AmBzDA,AENA,ACHA,AFMA,AGTA,AENA,ACHA,ACHA,AHSA,ApB4DA,AGTA,AIZA,AFMA,AMlBA,AFMA,ADGA,AIZA,Af6CA,AFMA;ACFA,AmBzDA,AENA,ACHA,AFMA,AGTA,AENA,ACHA,ACHA,AHSA,ApB4DA,AGTA,AIZA,AFMA,AMlBA,AFMA,ADGA,AIZA,Af6CA,AFMA;ACFA,AmBzDA,AENA,ACHA,AFMA,AGTA,AENA,ACHA,ACHA,ACHA,AJYA,ApB4DA,AGTA,AIZA,AFMA,AMlBA,AFMA,ADGA,AIZA,Af6CA,AFMA;ACFA,AmBzDA,AENA,ACHA,AFMA,AGTA,AENA,ACHA,ACHA,ACHA,AJYA,ApB4DA,AGTA,AIZA,AFMA,AMlBA,AFMA,ADGA,AIZA,Af6CA,AFMA;ACFA,AmBzDA,AENA,ACHA,AFMA,AKfA,ACHA,ACHA,ACHA,AJYA,ApB4DA,AGTA,AIZA,AFMA,AMlBA,AFMA,ADGA,AIZA,Af6CA,AFMA;ACFA,AmBzDA,AENA,ACHA,AFMA,AKfA,ACHA,ACHA,ACHA,ACHA,ALeA,ApB4DA,AGTA,AIZA,AFMA,AMlBA,AFMA,ADGA,AIZA,Af6CA,AFMA;ACFA,AmBzDA,AENA,ACHA,AFMA,AKfA,ACHA,ACHA,ACHA,ACHA,ALeA,ApB4DA,AGTA,AIZA,AFMA,AMlBA,AFMA,ADGA,AIZA,Af6CA,AFMA;ACFA,AmBzDA,AENA,ACHA,AFMA,AKfA,ACHA,ACHA,ACHA,ACHA,ALeA,ApB4DA,AGTA,AIZA,AFMA,AMlBA,AFMA,ADGA,AIZA,AjBmDA;ACFA,AmBzDA,AENA,ACHA,AFMA,AKfA,ACHA,ACHA,ACHA,ACHA,ALeA,AMlBA,A1B8EA,AGTA,AIZA,AFMA,AMlBA,AFMA,ADGA,AIZA,AjBmDA;ACFA,AmBzDA,AENA,ACHA,AFMA,AKfA,ACHA,ACHA,ACHA,ACHA,ALeA,AMlBA,A1B8EA,AGTA,AIZA,AFMA,AMlBA,AFMA,ADGA,AIZA,AjBmDA;ACFA,AmBzDA,AENA,ACHA,AFMA,AKfA,ACHA,ACHA,ACHA,ACHA,ALeA,AMlBA,A1B8EA,AGTA,AIZA,AFMA,AMlBA,AFMA,ADGA,AIZA;AhBiDA,AmBzDA,AENA,ACHA,AFMA,AKfA,ACHA,ACHA,ACHA,ACHA,ALeA,AMlBA,ACHA,A3BiFA,AGTA,AIZA,AFMA,AMlBA,AFMA,ADGA,AIZA;AhBiDA,AmBzDA,AENA,ACHA,AFMA,AKfA,ACHA,ACHA,ACHA,ACHA,ALeA,AMlBA,ACHA,A3BiFA,AGTA,AIZA,AFMA,AMlBA,AFMA,ADGA,AIZA;AhBiDA,AmBzDA,AENA,ACHA,AFMA,AKfA,ACHA,ACHA,ACHA,ACHA,ALeA,AMlBA,ACHA,A3BiFA,AGTA,AIZA,AFMA,AMlBA,AFMA,ADGA,AIZA;AhBiDA,AmBzDA,AENA,ACHA,AFMA,AKfA,ACHA,ACHA,ACHA,ACHA,ALeA,AMlBA,ACHA,A3BiFA,AGTA,AIZA,AFMA,AMlBA,AFMA,ADGA,AIZA,AgBhDA;AhCiGA,AmBzDA,AENA,ACHA,AFMA,AKfA,ACHA,ACHA,ACHA,ACHA,ALeA,AMlBA,ACHA,A3BiFA,AGTA,AIZA,AFMA,AMlBA,AFMA,ADGA,AIZA,AgBhDA;AhCiGA,AmBzDA,AENA,ACHA,AFMA,AKfA,ACHA,ACHA,ACHA,ACHA,ALeA,AMlBA,ACHA,A3BiFA,AGTA,AIZA,AFMA,AMlBA,AFMA,ADGA,AIZA,AgBhDA;AhCiGA,AmBzDA,AENA,ACHA,AFMA,AKfA,ACHA,ACHA,ACHA,ACHA,ALeA,AMlBA,ACHA,AENA,A7BuFA,AGTA,AIZA,AIZA,AFMA,ADGA,AIZA,AgBhDA;AhCiGA,AmBzDA,AENA,ACHA,AFMA,AKfA,ACHA,ACHA,ACHA,ACHA,ALeA,AMlBA,ACHA,AENA,A7BuFA,AGTA,AIZA,AIZA,AFMA,ADGA,AIZA,AgBhDA;AhCiGA,AmBzDA,AENA,ACHA,AGTA,ACHA,ACHA,ACHA,ACHA,ALeA,AMlBA,ACHA,AENA,A7BuFA,AGTA,AIZA,AIZA,AFMA,ADGA,AIZA,AgBhDA;AhCiGA,AqB/DA,ACHA,AGTA,ACHA,ACHA,ACHA,ACHA,ALeA,AMlBA,ACHA,AENA,ACHA,A9B0FA,AGTA,AIZA,AIZA,AFMA,ADGA,AIZA,AgBhDA;AhCiGA,AqB/DA,ACHA,AGTA,ACHA,ACHA,ACHA,ACHA,ALeA,AMlBA,ACHA,AENA,ACHA,A9B0FA,AGTA,AIZA,AIZA,AFMA,ADGA,AIZA,AgBhDA;AhCiGA,AqB/DA,ACHA,AGTA,ACHA,ACHA,ACHA,ACHA,ACHA,ACHA,AENA,ACHA,A9B0FA,AGTA,AIZA,AIZA,AFMA,ADGA,AIZA,AgBhDA;AhCiGA,AqB/DA,ACHA,AGTA,ACHA,ACHA,ACHA,ACHA,ACHA,ACHA,AENA,ACHA,ACHA,A/B6FA,AGTA,AIZA,AIZA,AFMA,ADGA,AIZA,AgBhDA;AhCiGA,AqB/DA,ACHA,AGTA,ACHA,ACHA,ACHA,ACHA,ACHA,ACHA,AENA,ACHA,ACHA,A/B6FA,AGTA,AIZA,AIZA,AFMA,ADGA,AIZA,AgBhDA;AhCiGA,AqB/DA,ACHA,AGTA,ACHA,ACHA,ACHA,ACHA,ACHA,ACHA,AENA,ACHA,ACHA,A/B6FA,AGTA,AIZA,AIZA,AFMA,ADGA,AIZA,AgBhDA;AhCiGA,AqB/DA,ACHA,AGTA,ACHA,ACHA,ACHA,ACHA,ACHA,ACHA,AENA,ACHA,ACHA,A/B6FA,AgChGA,A7BuFA,AIZA,AIZA,AFMA,ADGA,AIZA,AgBhDA;AhCiGA,AqB/DA,ACHA,AGTA,ACHA,ACHA,ACHA,ACHA,ACHA,ACHA,AENA,ACHA,ACHA,A/B6FA,AgChGA,A7BuFA,AIZA,AIZA,AFMA,ADGA,AIZA,AgBhDA;AhCiGA,AqB/DA,ACHA,AGTA,ACHA,ACHA,ACHA,ACHA,ACHA,ACHA,AENA,ACHA,ACHA,A/B6FA,AgChGA,A7BuFA,AIZA,AIZA,AFMA,ADGA,AIZA,AgBhDA;AhCiGA,AqB/DA,ACHA,AGTA,ACHA,ACHA,ACHA,ACHA,ACHA,ACHA,AENA,ACHA,ACHA,A/B6FA,AgChGA,A7BuFA,AIZA,AIZA,AFMA,ADGA,AIZA,AgBhDA,AKfA;ArCgHA,AqB/DA,ACHA,AGTA,ACHA,ACHA,ACHA,ACHA,ACHA,ACHA,AENA,ACHA,ACHA,A/B6FA,AgChGA,A7BuFA,AIZA,AIZA,AFMA,ADGA,AIZA,AgBhDA,AKfA;ArCgHA,AqB/DA,ACHA,AGTA,ACHA,ACHA,ACHA,ACHA,ACHA,ACHA,AENA,ACHA,ACHA,A/B6FA,AgChGA,A7BuFA,AIZA,AIZA,AFMA,ADGA,AIZA,AgBhDA,AKfA;ArCgHA,AqB/DA,ACHA,AGTA,ACHA,ACHA,ACHA,ACHA,ACHA,ACHA,AENA,ACHA,ACHA,A/B6FA,AgChGA,A7BuFA,AIZA,AIZA,AFMA,ADGA,AIZA,AgBhDA,AKfA,ACHA;AtCmHA,AqB/DA,ACHA,AGTA,ACHA,ACHA,ACHA,ACHA,ACHA,ACHA,AENA,ACHA,ACHA,A/B6FA,AgChGA,A7BuFA,AIZA,AIZA,AFMA,ADGA,AIZA,AgBhDA,AKfA,ACHA;AtCmHA,AqB/DA,ACHA,AGTA,ACHA,ACHA,ACHA,ACHA,ACHA,ACHA,AENA,ACHA,ACHA,A/B6FA,AgChGA,A7BuFA,AIZA,AIZA,AFMA,ADGA,AIZA,AgBhDA,AKfA,ACHA;AtCmHA,AqB/DA,ACHA,AGTA,ACHA,ACHA,ACHA,ACHA,ACHA,ACHA,AENA,ACHA,ACHA,A/B6FA,AgChGA,A7BuFA,AIZA,AIZA,AFMA,ADGA,AIZA,AgBhDA,AKfA,ACHA,ACHA;AvCsHA,AqB/DA,ACHA,AGTA,ACHA,ACHA,ACHA,ACHA,ACHA,ACHA,AENA,ACHA,ACHA,A/B6FA,AgChGA,A7BuFA,AIZA,AIZA,AFMA,ADGA,AIZA,AgBhDA,AKfA,ACHA,ACHA;AvCsHA,AqB/DA,ACHA,AGTA,ACHA,ACHA,ACHA,ACHA,ACHA,ACHA,AENA,ACHA,ACHA,A/B6FA,AgChGA,A7BuFA,AIZA,AIZA,AHSA,AIZA,AqB/DA,ACHA,ACHA;AvCsHA,AqB/DA,ACHA,AGTA,ACHA,ACHA,ACHA,ACHA,ACHA,ACHA,AENA,ACHA,ACHA,A/B6FA,AgChGA,A7BuFA,AIZA,AIZA,AHSA,AIZA,AwBxEA,AHSA,ACHA,ACHA;AvCsHA,AqB/DA,ACHA,AGTA,ACHA,ACHA,ACHA,ACHA,ACHA,ACHA,AENA,ACHA,ACHA,A/B6FA,AgChGA,A7BuFA,AIZA,AIZA,AHSA,AIZA,AwBxEA,AHSA,ACHA,ACHA;AvCsHA,AqB/DA,ACHA,AIZA,ACHA,ACHA,ACHA,ACHA,ACHA,AENA,ACHA,ACHA,A/B6FA,AgChGA,A7BuFA,AIZA,ACHA,AIZA,AwBxEA,AHSA,ACHA,ACHA;AvCsHA,AqB/DA,ACHA,AIZA,ACHA,ACHA,ACHA,ACHA,ACHA,AENA,ACHA,ACHA,A/B6FA,AgChGA,A7BuFA,AS3BA,AwBxEA,AHSA,AIZA,AFMA;AvCsHA,AqB/DA,ACHA,AIZA,ACHA,ACHA,ACHA,ACHA,ACHA,AENA,ACHA,ACHA,A/B6FA,AgChGA,A7BuFA,AS3BA,AwBxEA,AHSA,AIZA,AFMA;AvCsHA,AqB/DA,ACHA,AIZA,ACHA,ACHA,ACHA,ACHA,ACHA,AENA,ACHA,ACHA,A/B6FA,AgChGA,A7BuFA,AS3BA,AwBxEA,AHSA,AIZA,AFMA;AvCsHA,AqB/DA,ACHA,AIZA,ACHA,ACHA,ACHA,ACHA,ACHA,AENA,ACHA,ACHA,A/B6FA,AgChGA,A7BuFA,AS3BA,AwBxEA,AHSA,AIZA,ACHA,AHSA;AvCsHA,AsBlEA,AIZA,ACHA,ACHA,ACHA,ACHA,ACHA,AENA,ACHA,ACHA,A/B6FA,AgChGA,A7BuFA,AS3BA,AwBxEA,AHSA,AIZA,ACHA,AHSA;AvCsHA,AsBlEA,AIZA,ACHA,ACHA,ACHA,ACHA,ACHA,AENA,ACHA,ACHA,A/B6FA,AgChGA,A7BuFA,AS3BA,AwBxEA,AHSA,AIZA,ACHA,AHSA;AvCsHA,AsBlEA,AIZA,ACHA,ACHA,ACHA,ACHA,ACHA,AENA,ACHA,ACHA,A/B6FA,AgChGA,A7BuFA,AS3BA,AwBxEA,AHSA,AIZA,AENA,ADGA,AHSA;AvCsHA,AsBlEA,AIZA,ACHA,ACHA,ACHA,ACHA,ACHA,AENA,ACHA,ACHA,A/B6FA,AgChGA,A7BuFA,AS3BA,AwBxEA,AHSA,AIZA,AENA,ADGA,AHSA;AvCsHA,AsBlEA,AIZA,ACHA,ACHA,ACHA,ACHA,ACHA,AENA,ACHA,ACHA,A/B6FA,AgChGA,A7BuFA,AS3BA,AwBxEA,AHSA,AIZA,AENA,ADGA,AHSA;AvCsHA,AsBlEA,AIZA,ACHA,ACHA,ACHA,ACHA,ACHA,AENA,ACHA,ACHA,A/B6FA,AgChGA,A7BuFA,AS3BA,AwBxEA,AHSA,AIZA,AENA,ACHA,AFMA,AHSA;AvCsHA,AsBlEA,AIZA,ACHA,ACHA,ACHA,ACHA,ACHA,AENA,ACHA,ACHA,A/B6FA,AgChGA,A7BuFA,AS3BA,AwBxEA,AHSA,AIZA,AENA,ACHA,AFMA,AHSA;AvCsHA,AsBlEA,AIZA,ACHA,ACHA,ACHA,ACHA,ACHA,AENA,ACHA,ACHA,A/B6FA,AgChGA,A7BuFA,AS3BA,AwBxEA,AHSA,AIZA,AENA,ACHA,AFMA,AHSA;AvCsHA,AsBlEA,AIZA,ACHA,ACHA,ACHA,ACHA,ACHA,AENA,ACHA,ACHA,A/B6FA,AgChGA,A7BuFA,AS3BA,AwBxEA,AHSA,AIZA,AIZA,AFMA,ACHA,AFMA,AHSA;AvCsHA,AsBlEA,AIZA,ACHA,ACHA,ACHA,ACHA,ACHA,AENA,ACHA,ACHA,A/B6FA,AgChGA,A7BuFA,AS3BA,AwBxEA,AHSA,AIZA,AIZA,AFMA,ACHA,AFMA,AHSA;AvCsHA,AsBlEA,AIZA,ACHA,ACHA,ACHA,ACHA,ACHA,AENA,ACHA,ACHA,A/B6FA,AgChGA,A7BuFA,AS3BA,AwBxEA,AHSA,AIZA,AIZA,AFMA,ACHA,AFMA,AHSA;AvCsHA,AsBlEA,AIZA,ACHA,ACHA,ACHA,ACHA,ACHA,AENA,ACHA,ACHA,A/B6FA,AgChGA,A7BuFA,AS3BA,AwBxEA,AHSA,AIZA,AIZA,AFMA,AGTA,AFMA,AFMA,AHSA;AvCsHA,AsBlEA,AIZA,ACHA,ACHA,ACHA,ACHA,ACHA,AENA,ACHA,ACHA,A/B6FA,AgChGA,A7BuFA,AS3BA,AwBxEA,AHSA,AIZA,AIZA,AFMA,AGTA,AFMA,AFMA,AHSA;AvCsHA,AsBlEA,AIZA,ACHA,ACHA,ACHA,ACHA,ACHA,AENA,ACHA,ACHA,A/B6FA,AgChGA,A7BuFA,AS3BA,AwBxEA,AHSA,AIZA,AIZA,AFMA,AGTA,AFMA,AFMA,AHSA;AvCsHA,AsBlEA,AIZA,ACHA,ACHA,AENA,ACHA,AENA,ACHA,ACHA,A/B6FA,AgChGA,A7BuFA,AS3BA,AwBxEA,AHSA,AIZA,AIZA,AFMA,AIZA,ADGA,AFMA,AFMA,AHSA;AvCsHA,AsBlEA,AIZA,ACHA,ACHA,AENA,ACHA,AENA,ACHA,ACHA,A/B6FA,AgChGA,A7BuFA,AS3BA,AwBxEA,AHSA,AIZA,AIZA,AFMA,AIZA,ADGA,AFMA,AFMA,AHSA;AvCsHA,AsBlEA,AIZA,ACHA,ACHA,AENA,ACHA,AENA,ACHA,ACHA,A/B6FA,AgChGA,A7BuFA,AS3BA,AwBxEA,AHSA,AIZA,AIZA,AFMA,AIZA,ADGA,AFMA,AFMA,AHSA;AvCsHA,AsBlEA,AIZA,ACHA,ACHA,AENA,ACHA,AENA,ACHA,ACHA,A/B6FA,AgChGA,A7BuFA,AS3BA,AwBxEA,AHSA,AIZA,AIZA,AFMA,AIZA,ACHA,AFMA,AFMA,AFMA,AHSA;AvCsHA,AsBlEA,AIZA,ACHA,AGTA,ACHA,AENA,ACHA,ACHA,A/B6FA,AgChGA,A7BuFA,AS3BA,AwBxEA,AHSA,AIZA,AIZA,AFMA,AIZA,ACHA,AFMA,AFMA,AFMA,AHSA;AvCsHA,AsBlEA,AIZA,ACHA,AGTA,ACHA,AENA,ACHA,ACHA,A/B6FA,AgChGA,A7BuFA,AS3BA,AwBxEA,AHSA,AIZA,AIZA,AFMA,AIZA,ACHA,AFMA,AFMA,AFMA,AHSA;AvCsHA,AsBlEA,AIZA,ACHA,AGTA,ACHA,AENA,ACHA,ACHA,A/B6FA,AgChGA,A7BuFA,AS3BA,AwBxEA,AHSA,AIZA,AIZA,AFMA,AIZA,ACHA,ACHA,AHSA,AFMA,AFMA,AHSA;AvCsHA,AsBlEA,AIZA,ACHA,AGTA,ACHA,AENA,ACHA,ACHA,A/B6FA,AgChGA,A7BuFA,AS3BA,AwBxEA,AHSA,AIZA,AIZA,AFMA,AIZA,ACHA,ACHA,AHSA,AFMA,AFMA,AHSA;AvCsHA,AsBlEA,AIZA,ACHA,AGTA,ACHA,AENA,ACHA,ACHA,A/B6FA,AgChGA,A7BuFA,AS3BA,AwBxEA,AHSA,AIZA,AIZA,AFMA,AIZA,ACHA,ACHA,AHSA,AFMA,AFMA,AHSA;AvCsHA,AsBlEA,AIZA,ACHA,AGTA,ACHA,AENA,ACHA,ACHA,A/B6FA,AgChGA,A7BuFA,AS3BA,AwBxEA,AHSA,AIZA,AIZA,AFMA,AIZA,ACHA,ACHA,ACHA,AJYA,AFMA,AFMA,AHSA;AvCsHA,AsBlEA,AIZA,ACHA,AGTA,ACHA,AENA,ACHA,ACHA,A/B6FA,AgChGA,A7BuFA,AS3BA,AwBxEA,AHSA,AIZA,AIZA,AFMA,AIZA,ACHA,ACHA,ACHA,AJYA,AFMA,AFMA,AHSA;AvCsHA,AsBlEA,AIZA,ACHA,AGTA,ACHA,AENA,ACHA,ACHA,A/B6FA,AgChGA,A7BuFA,AS3BA,AwBxEA,AHSA,AIZA,AIZA,AFMA,AIZA,ACHA,ACHA,ACHA,AJYA,AFMA,AFMA,AHSA;AvCsHA,AsBlEA,AIZA,ACHA,AGTA,ACHA,AENA,ACHA,ACHA,A/B6FA,AgChGA,A7BuFA,AS3BA,AwBxEA,AHSA,AIZA,AIZA,AFMA,AIZA,ACHA,ACHA,ACHA,AJYA,AKfA,APqBA,AFMA,AHSA;AvCsHA,AsBlEA,AIZA,ACHA,AGTA,ACHA,AENA,ACHA,ACHA,A/B6FA,AgChGA,A7BuFA,AS3BA,AwBxEA,AHSA,AIZA,AIZA,AFMA,AIZA,ACHA,ACHA,ACHA,AJYA,AKfA,APqBA,AFMA,AHSA;AvCsHA,AsBlEA,AIZA,ACHA,AGTA,ACHA,AENA,ACHA,ACHA,A/B6FA,AgChGA,A7BuFA,AS3BA,AwBxEA,AHSA,AIZA,AIZA,AFMA,AIZA,ACHA,ACHA,ACHA,AJYA,AKfA,APqBA,AFMA,AHSA;AvCsHA,AsBlEA,AIZA,ACHA,AGTA,ACHA,AENA,ACHA,ACHA,A/B6FA,AgChGA,A7BuFA,AiCnGA,AHSA,AIZA,AIZA,AFMA,AIZA,ACHA,ACHA,ACHA,AJYA,AKfA,APqBA,AFMA,AU9BA,AbuCA;AvCsHA,AsBlEA,AIZA,ACHA,AGTA,ACHA,AENA,ACHA,ACHA,A/B6FA,AgChGA,A7BuFA,AiCnGA,AHSA,AIZA,AIZA,AFMA,AIZA,ACHA,ACHA,ACHA,AJYA,AKfA,APqBA,AFMA,AU9BA,AbuCA;AvCsHA,AsBlEA,AIZA,ACHA,AGTA,ACHA,AENA,ACHA,ACHA,A/B6FA,AgChGA,A7BuFA,AiCnGA,AHSA,AIZA,AIZA,AFMA,AIZA,ACHA,ACHA,ACHA,AJYA,AKfA,APqBA,AFMA,AU9BA,AbuCA;AvCsHA,AsBlEA,AIZA,ACHA,AGTA,ACHA,AENA,ACHA,ACHA,A/B6FA,AgChGA,A7BuFA,AiCnGA,AHSA,AIZA,AIZA,AFMA,AIZA,ACHA,ACHA,ACHA,AJYA,AKfA,APqBA,AFMA,AU9BA,ACHA,Ad0CA;AvCsHA,AsBlEA,AKfA,AGTA,ACHA,AENA,ACHA,ACHA,A/B6FA,AgChGA,A7BuFA,AiCnGA,AHSA,AIZA,AIZA,AFMA,AIZA,ACHA,ACHA,ACHA,AJYA,AKfA,APqBA,AFMA,AU9BA,ACHA,Ad0CA;AvCsHA,AsBlEA,AKfA,AGTA,ACHA,AENA,ACHA,ACHA,A/B6FA,AgChGA,A7BuFA,AiCnGA,AHSA,AIZA,AIZA,AFMA,AIZA,ACHA,ACHA,ACHA,AJYA,AKfA,APqBA,AFMA,AU9BA,ACHA,Ad0CA;AvCsHA,AsBlEA,AKfA,AGTA,ACHA,AENA,ACHA,ACHA,A/B6FA,AgChGA,A7BuFA,AiCnGA,AHSA,AIZA,AIZA,AFMA,AIZA,ACHA,ACHA,ACHA,AJYA,AKfA,APqBA,AFMA,AU9BA,ACHA,Ad0CA;AvCsHA,AsBlEA,AKfA,AGTA,ACHA,AENA,ACHA,ACHA,A/B6FA,AgChGA,A7BuFA,AiCnGA,AHSA,AIZA,AIZA,AFMA,AIZA,ACHA,ACHA,ACHA,AJYA,AKfA,APqBA,AFMA,AU9BA,ACHA,Ad0CA;AvCsHA,AsBlEA,AKfA,AGTA,ACHA,AENA,ACHA,ACHA,A/B6FA,AgChGA,A7BuFA,A8B1FA,AIZA,AIZA,AFMA,AIZA,ACHA,ACHA,ACHA,AJYA,AKfA,APqBA,AFMA,AU9BA,ACHA,Ad0CA;AvCsHA,AsBlEA,AKfA,AGTA,ACHA,AENA,ACHA,ACHA,A/B6FA,AgChGA,A7BuFA,A8B1FA,AIZA,AIZA,AFMA,AIZA,ACHA,ACHA,ACHA,AJYA,AKfA,APqBA,AFMA,AU9BA,ACHA,Ad0CA;AvCsHA,AsBlEA,AKfA,AGTA,ACHA,AENA,AENA,A/B6FA,AgChGA,A7BuFA,A8B1FA,AIZA,AIZA,AFMA,AIZA,ACHA,ACHA,ACHA,AJYA,AKfA,APqBA,AFMA,AU9BA,ACHA,Ad0CA;AvCsHA,AsBlEA,AKfA,AGTA,ACHA,AENA,AENA,A/B6FA,AgChGA,A7BuFA,A8B1FA,AIZA,AIZA,AFMA,AIZA,ACHA,ACHA,ACHA,AJYA,AKfA,APqBA,AFMA,AU9BA,ACHA,Ad0CA;AvCsHA,AsBlEA,AKfA,AGTA,ACHA,AENA,AENA,A/B6FA,AgChGA,A7BuFA,A8B1FA,AIZA,AIZA,AFMA,AIZA,ACHA,ACHA,ACHA,AJYA,AKfA,APqBA,AFMA,AU9BA,ACHA,Ad0CA;AvCsHA,AsBlEA,AKfA,AGTA,ACHA,AENA,AENA,A/B6FA,AgChGA,A7BuFA,A8B1FA,AIZA,AIZA,AFMA,AIZA,ACHA,ACHA,ACHA,AJYA,AKfA,APqBA,AFMA,AU9BA,ACHA,Ad0CA;AvCsHA,AsBlEA,AKfA,AGTA,ACHA,AENA,AENA,A/B6FA,AgChGA,A7BuFA,A8B1FA,AIZA,AIZA,AFMA,AIZA,ACHA,ACHA,ACHA,AJYA,AKfA,APqBA,AFMA,AU9BA,ACHA,Ad0CA;AvCsHA,A2BjFA,AGTA,ACHA,AENA,AENA,A/B6FA,AgChGA,A7BuFA,A8B1FA,AIZA,AIZA,AFMA,AIZA,ACHA,ACHA,ACHA,AJYA,AKfA,APqBA,AFMA,AU9BA,ACHA,Ad0CA;AvCsHA,A2BjFA,AGTA,ACHA,AENA,AENA,A/B6FA,AgChGA,A7BuFA,A8B1FA,AIZA,AIZA,AFMA,AIZA,ACHA,ACHA,ACHA,AJYA,AKfA,APqBA,AFMA,AU9BA,ACHA,Ad0CA;AvCsHA,A2BjFA,AGTA,ACHA,AENA,AENA,A/B6FA,AgChGA,A7BuFA,A8B1FA,AIZA,AIZA,AFMA,AIZA,ACHA,ACHA,ACHA,AJYA,AKfA,APqBA,AFMA,AU9BA,ACHA,Ad0CA;AvCsHA,A2BjFA,AGTA,ACHA,AIZA,A/B6FA,AgChGA,A7BuFA,A8B1FA,AIZA,AIZA,AFMA,AIZA,ACHA,ACHA,ACHA,AJYA,AKfA,APqBA,AFMA,AU9BA,ACHA,Ad0CA;AvCsHA,A2BjFA,AGTA,ACHA,AIZA,A/B6FA,AgChGA,A7BuFA,A8B1FA,AIZA,AIZA,AFMA,AIZA,ACHA,ACHA,ACHA,AJYA,AKfA,APqBA,AFMA,AU9BA,ACHA,Ad0CA;AvCsHA,A2BjFA,AGTA,ACHA,AIZA,A/B6FA,AgChGA,A7BuFA,A8B1FA,AIZA,AIZA,AFMA,AIZA,ACHA,ACHA,ACHA,AJYA,AKfA,APqBA,AFMA,AU9BA,ACHA,Ad0CA;AvCsHA,A2BjFA,AGTA,ACHA,AIZA,A/B6FA,AgChGA,A7BuFA,A8B1FA,AIZA,AIZA,AFMA,AIZA,ACHA,ACHA,ACHA,AJYA,AKfA,APqBA,AFMA,AU9BA,ACHA,Ad0CA;AvCsHA,A2BjFA,AGTA,ACHA,AIZA,A/B6FA,AgChGA,A7BuFA,A8B1FA,AIZA,AIZA,AFMA,AIZA,ACHA,ACHA,ACHA,AJYA,AKfA,APqBA,AFMA,AU9BA,ACHA,Ad0CA;AvCsHA,A2BjFA,AGTA,ACHA,AIZA,A/B6FA,AgChGA,A7BuFA,A8B1FA,AIZA,AIZA,AFMA,AIZA,ACHA,ACHA,ACHA,AJYA,AKfA,APqBA,AFMA,AU9BA,ACHA,Ad0CA;AvCsHA,A2BjFA,AGTA,ACHA,AIZA,A/B6FA,AgChGA,A7BuFA,A8B1FA,AIZA,AIZA,AFMA,AIZA,ACHA,ACHA,AHSA,AKfA,APqBA,AFMA,AU9BA,ACHA;ArDgKA,A2BjFA,AGTA,ACHA,AIZA,A/B6FA,AgChGA,A7BuFA,A8B1FA,AQxBA,AFMA,AIZA,ACHA,ACHA,AHSA,AKfA,APqBA,AFMA,AU9BA,ACHA;ArDgKA,A2BjFA,AGTA,ACHA,AIZA,A/B6FA,AgChGA,A7BuFA,A8B1FA,AQxBA,AFMA,AIZA,ACHA,ACHA,AHSA,AKfA,APqBA,AFMA,AU9BA,ACHA;ArDgKA,A2BjFA,AGTA,ACHA,AIZA,A/B6FA,AgChGA,A7BuFA,A8B1FA,AQxBA,AFMA,AIZA,ACHA,ACHA,AHSA,AKfA,APqBA,AFMA,AU9BA,ACHA;ArDgKA,A2BjFA,AGTA,ACHA,AIZA,A/B6FA,AgChGA,A7BuFA,A8B1FA,AQxBA,AFMA,AIZA,ACHA,ACHA,AHSA,AKfA,APqBA,AFMA,AU9BA,ACHA;ArDgKA,A2BjFA,AGTA,ACHA,AIZA,A/B6FA,AgChGA,A7BuFA,A8B1FA,AQxBA,AFMA,AIZA,ACHA,ACHA,AHSA,AKfA,APqBA,AFMA,AU9BA,ACHA;ArDgKA,A2BjFA,AGTA,ACHA,AIZA,A/B6FA,AgChGA,A7BuFA,A8B1FA,AQxBA,AFMA,AIZA,ACHA,ACHA,AHSA,AKfA,APqBA,AFMA,AU9BA,ACHA;ArDgKA,A2BjFA,AGTA,ACHA,AIZA,A/B6FA,AgChGA,A7BuFA,A8B1FA,AQxBA,AFMA,AIZA,ACHA,ACHA,AHSA,AKfA,APqBA,AFMA,AU9BA,ACHA;ArDgKA,A2BjFA,AGTA,ACHA,AIZA,A/B6FA,AgChGA,A7BuFA,A8B1FA,AQxBA,AFMA,AIZA,ACHA,ACHA,AHSA,AKfA,APqBA,AFMA,AU9BA,ACHA;ArDgKA,A2BjFA,AGTA,ACHA,AIZA,A/B6FA,AGTA,A8B1FA,AMlBA,AIZA,ACHA,ACHA,AHSA,AKfA,APqBA,AFMA,AU9BA,ACHA;ArDgKA,A2BjFA,AGTA,ACHA,AIZA,A/B6FA,AGTA,A8B1FA,AMlBA,AIZA,ACHA,ACHA,AHSA,AKfA,APqBA,AFMA,AU9BA,ACHA;ArDgKA,A2BjFA,AGTA,ACHA,AIZA,A/B6FA,AGTA,A8B1FA,AMlBA,AIZA,ACHA,ACHA,AHSA,AKfA,APqBA,AFMA,AU9BA,ACHA;ArDgKA,A2BjFA,AGTA,ACHA,AIZA,A/B6FA,AGTA,A8B1FA,AMlBA,AIZA,ACHA,ACHA,AHSA,AFMA,AFMA,AU9BA,ACHA;ArDgKA,A2BjFA,AGTA,ACHA,AIZA,A/B6FA,AGTA,A8B1FA,AMlBA,AIZA,ACHA,ACHA,AHSA,AFMA,AFMA,AU9BA,ACHA;ArDgKA,A2BjFA,AGTA,ACHA,AIZA,A/B6FA,AGTA,A8B1FA,AMlBA,AIZA,ACHA,ACHA,AHSA,AFMA,AFMA,AU9BA,ACHA;ArDgKA,A2BjFA,AGTA,ACHA,AIZA,A/B6FA,AGTA,A8B1FA,AMlBA,AIZA,ACHA,ACHA,AHSA,AFMA,AFMA,AU9BA,ACHA;ArDgKA,A2BjFA,AGTA,ACHA,AIZA,A/B6FA,AGTA,A8B1FA,AMlBA,AIZA,ACHA,ACHA,AHSA,AFMA,AFMA,AU9BA,ACHA;ArDgKA,A2BjFA,AGTA,ACHA,AIZA,A/B6FA,AGTA,A8B1FA,AMlBA,AIZA,ACHA,ACHA,AHSA,AFMA,AFMA,AU9BA,ACHA;A1B+EA,AGTA,ACHA,AIZA,A/B6FA,AGTA,A8B1FA,AMlBA,AIZA,ACHA,ACHA,AHSA,AFMA,AFMA,AU9BA,ACHA;A1B+EA,AGTA,ACHA,AIZA,A/B6FA,AGTA,A8B1FA,AMlBA,AIZA,ACHA,ACHA,AHSA,AFMA,AFMA,AU9BA,ACHA;A1B+EA,AGTA,ACHA,AIZA,A/B6FA,AGTA,A8B1FA,AMlBA,AIZA,ACHA,ACHA,AHSA,AFMA,AFMA,AU9BA,ACHA;A1B+EA,AGTA,ACHA,AIZA,A/B6FA,AGTA,A8B1FA,AMlBA,AIZA,ACHA,ACHA,AHSA,AFMA,AFMA,AU9BA,ACHA;A1B+EA,AGTA,AKfA,A/B6FA,AGTA,A8B1FA,AMlBA,AIZA,ACHA,ACHA,AHSA,AFMA,AFMA,AU9BA,ACHA;A1B+EA,AGTA,AKfA,A/B6FA,AGTA,A8B1FA,AMlBA,AIZA,ACHA,ACHA,AHSA,AFMA,AFMA,AU9BA,ACHA;A1B+EA,AGTA,AKfA,A/B6FA,AGTA,A8B1FA,AMlBA,AIZA,ACHA,ACHA,AHSA,AFMA,AFMA,AU9BA,ACHA;A1B+EA,AGTA,AKfA,A/B6FA,AGTA,A8B1FA,AMlBA,AIZA,ACHA,ACHA,AHSA,AFMA,AFMA,AU9BA,ACHA;A1B+EA,AGTA,AKfA,A/B6FA,AGTA,A8B1FA,AMlBA,AIZA,ACHA,ACHA,AHSA,AFMA,AFMA,AU9BA,ACHA;A1B+EA,AGTA,AKfA,A/B6FA,AGTA,A8B1FA,AMlBA,AIZA,ACHA,ACHA,AHSA,AFMA,AFMA,AU9BA,ACHA;A1B+EA,AGTA,AKfA,A/B6FA,AGTA,A8B1FA,AMlBA,AIZA,ACHA,ACHA,AHSA,AFMA,AFMA,AU9BA,ACHA;A1B+EA,AGTA,AKfA,A/B6FA,AGTA,A8B1FA,AMlBA,AIZA,ACHA,ACHA,AHSA,AFMA,AFMA,AU9BA,ACHA;A1B+EA,AGTA,AKfA,A/B6FA,AGTA,A8B1FA,AMlBA,AIZA,ACHA,ACHA,AHSA,AFMA,AFMA,AU9BA,ACHA;A1B+EA,AGTA,AKfA,A/B6FA,AGTA,A8B1FA,AMlBA,AIZA,ACHA,ACHA,AHSA,AFMA,AFMA,AU9BA,ACHA;A1B+EA,AGTA,AKfA,A/B6FA,AGTA,A8B1FA,AMlBA,AIZA,ACHA,ACHA,AHSA,AJYA,AU9BA,ACHA;A1B+EA,AGTA,AKfA,A/B6FA,AGTA,A8B1FA,AMlBA,AIZA,ACHA,ACHA,AHSA,AJYA,AU9BA,ACHA;A1B+EA,AGTA,AKfA,A/B6FA,AGTA,A8B1FA,AMlBA,AIZA,ACHA,ACHA,AHSA,AJYA,AU9BA,ACHA;A1B+EA,AGTA,AKfA,A/B6FA,AGTA,A8B1FA,AMlBA,AIZA,ACHA,ACHA,AHSA,AJYA,AU9BA,ACHA;A1B+EA,AGTA,AKfA,A/B6FA,AGTA,A8B1FA,AMlBA,AIZA,ACHA,ACHA,AHSA,AJYA,AU9BA,ACHA;A1B+EA,AGTA,AKfA,A/B6FA,AGTA,A8B1FA,AMlBA,AIZA,ACHA,ACHA,AHSA,AJYA,AU9BA,ACHA;A1B+EA,AGTA,AKfA,A/B6FA,AGTA,A8B1FA,AMlBA,AIZA,ACHA,ACHA,AHSA,AJYA,AU9BA,ACHA;A1B+EA,AGTA,AKfA,A/B6FA,AGTA,A8B1FA,AMlBA,AIZA,ACHA,ACHA,AHSA,AJYA,AU9BA,ACHA;A1B+EA,AGTA,AKfA,A/B6FA,AGTA,A8B1FA,AMlBA,AIZA,ACHA,ACHA,AHSA,AJYA,AU9BA,ACHA;A1B+EA,AGTA,AKfA,A/B6FA,AGTA,A8B1FA,AMlBA,AIZA,ACHA,AFMA,AJYA,AU9BA,ACHA;A1B+EA,AGTA,AKfA,A/B6FA,AGTA,A8B1FA,AMlBA,AIZA,ACHA,AFMA,AJYA,AU9BA,ACHA;A1B+EA,AGTA,AKfA,A/B6FA,AGTA,A8B1FA,AMlBA,AIZA,ACHA,AFMA,AJYA,AU9BA,ACHA;A1B+EA,AGTA,AKfA,A/B6FA,AGTA,A8B1FA,AMlBA,AIZA,ACHA,ANkBA,AU9BA,ACHA;A1B+EA,AGTA,AKfA,A/B6FA,AGTA,A8B1FA,AMlBA,AIZA,ACHA,ANkBA,AU9BA;AtBmEA,AKfA,A/B6FA,AGTA,A8B1FA,AMlBA,AIZA,ACHA,ANkBA,AU9BA;AtBmEA,AKfA,A/B6FA,AGTA,A8B1FA,AMlBA,AIZA,ACHA,ANkBA,AU9BA;AtBmEA,AKfA,A/B6FA,AGTA,A8B1FA,AMlBA,AIZA,ACHA,ANkBA,AU9BA;AtBmEA,AKfA,A/B6FA,AGTA,A8B1FA,AMlBA,AIZA,ACHA,ANkBA,AU9BA;AtBmEA,AKfA,A/B6FA,AGTA,A8B1FA,AMlBA,AIZA,ACHA,ANkBA,AU9BA;AtBmEA,AKfA,A/B6FA,AGTA,A8B1FA,AMlBA,AIZA,ACHA,ANkBA,AU9BA;AtBmEA,AKfA,A/B6FA,AGTA,A8B1FA,AMlBA,AIZA,ACHA,ANkBA,AU9BA;AtBmEA,AKfA,A/B6FA,AGTA,A8B1FA,AMlBA,AIZA,ACHA,ANkBA,AU9BA;AtBmEA,AKfA,A/B6FA,AGTA,A8B1FA,AMlBA,AIZA,ACHA,ANkBA,AU9BA;AtBmEA,AKfA,A/B6FA,AGTA,A8B1FA,AMlBA,AIZA,ACHA,ANkBA,AU9BA;AtBmEA,AKfA,A/B6FA,AGTA,A8B1FA,AMlBA,AIZA,ACHA,ANkBA,AU9BA;AtBmEA,A1B8EA,AGTA,A8B1FA,AMlBA,AIZA,ACHA,ANkBA,AU9BA;AtBmEA,A1B8EA,AGTA,A8B1FA,AMlBA,AIZA,ACHA,ANkBA,AU9BA;AtBmEA,A1B8EA,AGTA,A8B1FA,AMlBA,AIZA,ACHA,ANkBA,AU9BA;AtBmEA,A1B8EA,AGTA,A8B1FA,AMlBA,AIZA,ACHA,ANkBA,AU9BA;AtBmEA,A1B8EA,AGTA,A8B1FA,AMlBA,AIZA,ACHA,ANkBA,AU9BA;AtBmEA,A1B8EA,AGTA,A8B1FA,AMlBA,AIZA,ACHA,ANkBA,AU9BA;AtBmEA,A1B8EA,AGTA,A8B1FA,AMlBA,AIZA,ACHA,ANkBA,AU9BA;AtBmEA,A1B8EA,AGTA,A8B1FA,AMlBA,AIZA,ACHA,ANkBA,AU9BA;AtBmEA,A1B8EA,AGTA,A8B1FA,AMlBA,AIZA,ACHA,ANkBA,AU9BA;AtBmEA,A1B8EA,AGTA,A8B1FA,AMlBA,AIZA,ACHA,ANkBA,AU9BA;AtBmEA,A1B8EA,AGTA,A8B1FA,AMlBA,AIZA,ACHA,ANkBA,AU9BA;AtBmEA,A1B8EA,AGTA,A8B1FA,AMlBA,AIZA,ACHA,ANkBA,AU9BA;AtBmEA,A1B8EA,AGTA,A8B1FA,AMlBA,AIZA,ACHA,ANkBA,AU9BA;AtBmEA,A1B8EA,AiCnGA,AMlBA,AIZA,ACHA,ANkBA,AU9BA;AtBmEA,A1B8EA,AiCnGA,AMlBA,AIZA,ACHA,ANkBA,AU9BA;AtBmEA,A1B8EA,AiCnGA,AMlBA,AIZA,ACHA,ANkBA,AU9BA;AtBmEA,A1B8EA,AiCnGA,AMlBA,AIZA,ACHA,ANkBA,AU9BA;AtBmEA,A1B8EA,AiCnGA,AMlBA,AIZA,ACHA,ANkBA,AU9BA;AtBmEA,A1B8EA,AiCnGA,AMlBA,AIZA,ACHA,ANkBA,AU9BA;AtBmEA,A1B8EA,AiCnGA,AMlBA,AIZA,ACHA,ANkBA,AU9BA;AtBmEA,A1B8EA,AiCnGA,AMlBA,AIZA,ACHA,ANkBA,AU9BA;AtBmEA,A1B8EA,AiCnGA,AMlBA,AIZA,ACHA,ANkBA,AU9BA;AtBmEA,A1B8EA,AiCnGA,AMlBA,AIZA,ACHA,ANkBA,AU9BA;AtBmEA,A1B8EA,AiCnGA,AMlBA,AIZA,ACHA,ANkBA,AU9BA;AtBmEA,A1B8EA,AiCnGA,AMlBA,AIZA,ACHA,ANkBA,AU9BA;AtBmEA,A1B8EA,AiCnGA,AMlBA,AIZA,ACHA,ANkBA,AU9BA;AtBmEA,A1B8EA,AiCnGA,AMlBA,AIZA,ACHA,ANkBA,AU9BA;AtBmEA,A1B8EA,AiCnGA,AMlBA,AIZA,ACHA,ANkBA,AU9BA;AtBmEA,A1B8EA,AiCnGA,AMlBA,AIZA,ACHA,ANkBA,AU9BA;AtBmEA,A1B8EA,AiCnGA,AMlBA,AIZA,ACHA,ANkBA,AU9BA;AtBmEA,A1B8EA,AiCnGA,AMlBA,AIZA,ALeA,AU9BA;AtBmEA,A1B8EA,AiCnGA,AMlBA,AIZA,ALeA,AU9BA;AtBmEA,A1B8EA,AiCnGA,AMlBA,AIZA,ALeA,AU9BA;AtBmEA,A1B8EA,AiCnGA,AMlBA,AIZA,ALeA;AZqCA,A1B8EA,AiCnGA,AMlBA,AIZA,ALeA;AZqCA,A1B8EA,AiCnGA,AU9BA,ALeA;AZqCA,A1B8EA,AiCnGA,AU9BA,ALeA;AZqCA,A1B8EA,AiCnGA,AU9BA,ALeA;AZqCA,A1B8EA,AiCnGA,AU9BA,ALeA;AZqCA,A1B8EA,AiCnGA,AU9BA,ALeA;AZqCA,A1B8EA,AiCnGA,AU9BA,ALeA;AZqCA,A1B8EA,AiCnGA,AU9BA,ALeA;AZqCA,A1B8EA,AiCnGA,AU9BA,ALeA;AZqCA,A1B8EA,AiCnGA,AU9BA,ALeA;AZqCA,A1B8EA,AiCnGA,AU9BA,ALeA;AZqCA,A1B8EA,AiCnGA,AU9BA,ALeA;AZqCA,A1B8EA,AiCnGA,AU9BA,ALeA;AZqCA,A1B8EA,AiCnGA,AU9BA,ALeA;AZqCA,A1B8EA,AiCnGA,AU9BA,ALeA;AZqCA,A1B8EA,AiCnGA,AU9BA,ALeA;AZqCA,A1B8EA,AiCnGA,AU9BA,ALeA;AZqCA,A1B8EA,AiCnGA,AU9BA,ALeA;AZqCA,A1B8EA,AiCnGA,AU9BA,ALeA;AZqCA,A1B8EA,AiCnGA,AU9BA,ALeA;AZqCA,A1B8EA,AiCnGA,AU9BA,ALeA;AZqCA,A1B8EA,AiCnGA,AU9BA,ALeA;AZqCA,A1B8EA,AiCnGA,AU9BA,ALeA;AZqCA,A1B8EA,AiCnGA,AU9BA,ALeA;AZqCA,A1B8EA,AiCnGA,AU9BA,ALeA;AZqCA,A1B8EA,AiCnGA,AU9BA,ALeA;AZqCA,A1B8EA,AiCnGA,AU9BA,ALeA;AZqCA,A1B8EA,AiCnGA,AU9BA,ALeA;AZqCA,A1B8EA,AiCnGA,AU9BA,ALeA;AZqCA,A1B8EA,AiCnGA,AU9BA,ALeA;AZqCA,A1B8EA,AiCnGA,AU9BA,ALeA;AZqCA,A1B8EA,AiCnGA,AU9BA,ALeA;AZqCA,A1B8EA,AiCnGA,AU9BA,ALeA;AZqCA,A1B8EA,AiCnGA,AU9BA,ALeA;AZqCA,A1B8EA,AiCnGA,AU9BA,ALeA;AZqCA,A1B8EA,AiCnGA,AU9BA,ALeA;AZqCA,A1B8EA,AiCnGA,AU9BA,ALeA;AZqCA,A1B8EA,AiCnGA,AU9BA,ALeA;AZqCA,A1B8EA,AiCnGA,AU9BA,ALeA;AZqCA,A1B8EA,AiCnGA,AU9BA,ALeA;AZqCA,A1B8EA,AiCnGA,AU9BA,ALeA;AZqCA,A1B8EA,AiCnGA,AU9BA,ALeA;AZqCA,A1B8EA,AiCnGA,AU9BA,ALeA;AZqCA,A1B8EA,AiCnGA,AU9BA,ALeA;AZqCA,A1B8EA,AiCnGA,AU9BA,ALeA;AZqCA,A1B8EA,AiCnGA,AU9BA,ALeA;AZqCA,A1B8EA,AiCnGA,AU9BA,ALeA;AZqCA,A1B8EA,AiCnGA,AU9BA,ALeA;AZqCA,A1B8EA,AiCnGA,AU9BA,ALeA;AZqCA,A1B8EA,AiCnGA,AU9BA,ALeA;AZqCA,A1B8EA,AiCnGA,AU9BA,ALeA;AZqCA,A1B8EA,AiCnGA,AU9BA,ALeA;AZqCA,A1B8EA,AiCnGA,AU9BA,ALeA;AZqCA,A1B8EA,AiCnGA,AU9BA,ALeA;AZqCA,A1B8EA,AiCnGA,AU9BA,ALeA;AZqCA,A1B8EA,AiCnGA,AU9BA,ALeA;AZqCA,A1B8EA,AiCnGA,AU9BA,ALeA;AZqCA,A1B8EA,AiCnGA,AU9BA,ALeA;AtCmHA,AiCnGA,AU9BA,ALeA;AtCmHA,AiCnGA,AU9BA,ALeA;AtCmHA,AiCnGA,AU9BA,ALeA;AtCmHA,AiCnGA,AU9BA,ALeA;AtCmHA,AiCnGA,AU9BA,ALeA;AtCmHA,AiCnGA,AU9BA,ALeA;AtCmHA,AiCnGA,AU9BA,ALeA;AtCmHA,AiCnGA,AU9BA;A3CkIA,AiCnGA,AU9BA;A3CkIA,AiCnGA,AU9BA;A3CkIA,AiCnGA,AU9BA;A3CkIA,AiCnGA,AU9BA;A3CkIA,AiCnGA,AU9BA;A3CkIA,AiCnGA,AU9BA;A3CkIA,AiCnGA,AU9BA;A3CkIA,AiCnGA,AU9BA;A3CkIA,AiCnGA,AU9BA;A3CkIA,AiCnGA,AU9BA;A3CkIA,AiCnGA,AU9BA;A3CkIA,AiCnGA,AU9BA;A3CkIA,AiCnGA,AU9BA;A3CkIA,AiCnGA,AU9BA;A3CkIA,AiCnGA,AU9BA;A3CkIA,AiCnGA,AU9BA;A3CkIA,AiCnGA,AU9BA;A3CkIA,AiCnGA;AjCoGA,AiCnGA;AjCoGA,AiCnGA;AjCoGA,AiCnGA;AjCoGA,AiCnGA;AjCoGA,AiCnGA;AjCoGA,AiCnGA;AjCoGA,AiCnGA;AjCoGA,AiCnGA;AjCoGA,AiCnGA;AjCoGA,AiCnGA;AjCoGA,AiCnGA;AjCoGA,AiCnGA;AjCoGA,AiCnGA;AjCoGA,AiCnGA;AjCoGA,AiCnGA;AjCoGA,AiCnGA;AjCoGA,AiCnGA;AjCoGA,AiCnGA;AjCoGA,AiCnGA;AjCoGA,AiCnGA;AjCoGA,AiCnGA;AjCoGA,AiCnGA;AjCoGA,AiCnGA;AjCoGA,AiCnGA;AjCoGA,AiCnGA;AjCoGA,AiCnGA;AjCoGA,AiCnGA;AjCoGA,AiCnGA;AjCoGA,AiCnGA;AjCoGA,AiCnGA;AjCoGA,AiCnGA;AjCoGA,AiCnGA;AjCoGA,AiCnGA;AjCoGA,AiCnGA;AjCoGA,AiCnGA;AjCoGA,AiCnGA;AjCoGA,AiCnGA;AjCoGA,AiCnGA;AjCoGA,AiCnGA;AjCoGA,AiCnGA;AjCoGA,AiCnGA;AjCoGA,AiCnGA;AjCoGA,AiCnGA;AjCoGA,AiCnGA;AjCoGA,AiCnGA;AjCoGA,AiCnGA;AjCoGA,AiCnGA;AjCoGA,AiCnGA;AjCoGA,AiCnGA;AjCoGA,AiCnGA;AjCoGA,AiCnGA;AjCoGA,AiCnGA;AjCoGA,AiCnGA;AjCoGA,AiCnGA;AjCoGA,AiCnGA;AjCoGA,AiCnGA;AjCoGA,AiCnGA;AjCoGA,AiCnGA;AjCoGA,AiCnGA;AjCoGA,AiCnGA;AjCoGA,AiCnGA;AjCoGA,AiCnGA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA","file":"index.js","sourcesContent":["\nfunction __export(m) {\n    for (var p in m) if (!exports.hasOwnProperty(p)) exports[p] = m[p];\n}\nObject.defineProperty(exports, \"__esModule\", { value: true });\nvar create_detector_1 = require(\"./create_detector\");\nexports.createDetector = create_detector_1.createDetector;\n// Supported models enum.\n__export(require(\"./types\"));\n// Second level exports.\n// Utils for rendering.\nvar util = require(\"./util\");\nexports.util = util;\n// General calculators.\nvar keypoints_to_normalized_keypoints_1 = require(\"./calculators/keypoints_to_normalized_keypoints\");\nvar calculators = { keypointsToNormalizedKeypoints: keypoints_to_normalized_keypoints_1.keypointsToNormalizedKeypoints };\nexports.calculators = calculators;\n// MoveNet model types.\nvar constants_1 = require(\"./movenet/constants\");\nvar movenet = {\n    modelType: {\n        'SINGLEPOSE_LIGHTNING': constants_1.SINGLEPOSE_LIGHTNING,\n        'SINGLEPOSE_THUNDER': constants_1.SINGLEPOSE_THUNDER\n    }\n};\nexports.movenet = movenet;\n//# sourceMappingURL=index.js.map","\n/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nvar __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {\n    return new (P || (P = Promise))(function (resolve, reject) {\n        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }\n        function rejected(value) { try { step(generator[\"throw\"](value)); } catch (e) { reject(e); } }\n        function step(result) { result.done ? resolve(result.value) : new P(function (resolve) { resolve(result.value); }).then(fulfilled, rejected); }\n        step((generator = generator.apply(thisArg, _arguments || [])).next());\n    });\n};\nvar __generator = (this && this.__generator) || function (thisArg, body) {\n    var _ = { label: 0, sent: function() { if (t[0] & 1) throw t[1]; return t[1]; }, trys: [], ops: [] }, f, y, t, g;\n    return g = { next: verb(0), \"throw\": verb(1), \"return\": verb(2) }, typeof Symbol === \"function\" && (g[Symbol.iterator] = function() { return this; }), g;\n    function verb(n) { return function (v) { return step([n, v]); }; }\n    function step(op) {\n        if (f) throw new TypeError(\"Generator is already executing.\");\n        while (_) try {\n            if (f = 1, y && (t = op[0] & 2 ? y[\"return\"] : op[0] ? y[\"throw\"] || ((t = y[\"return\"]) && t.call(y), 0) : y.next) && !(t = t.call(y, op[1])).done) return t;\n            if (y = 0, t) op = [op[0] & 2, t.value];\n            switch (op[0]) {\n                case 0: case 1: t = op; break;\n                case 4: _.label++; return { value: op[1], done: false };\n                case 5: _.label++; y = op[1]; op = [0]; continue;\n                case 7: op = _.ops.pop(); _.trys.pop(); continue;\n                default:\n                    if (!(t = _.trys, t = t.length > 0 && t[t.length - 1]) && (op[0] === 6 || op[0] === 2)) { _ = 0; continue; }\n                    if (op[0] === 3 && (!t || (op[1] > t[0] && op[1] < t[3]))) { _.label = op[1]; break; }\n                    if (op[0] === 6 && _.label < t[1]) { _.label = t[1]; t = op; break; }\n                    if (t && _.label < t[2]) { _.label = t[2]; _.ops.push(op); break; }\n                    if (t[2]) _.ops.pop();\n                    _.trys.pop(); continue;\n            }\n            op = body.call(thisArg, _);\n        } catch (e) { op = [6, e]; y = 0; } finally { f = t = 0; }\n        if (op[0] & 5) throw op[1]; return { value: op[0] ? op[1] : void 0, done: true };\n    }\n};\nObject.defineProperty(exports, \"__esModule\", { value: true });\nvar detector_1 = require(\"./blazepose_mediapipe/detector\");\nvar detector_2 = require(\"./blazepose_tfjs/detector\");\nvar detector_3 = require(\"./movenet/detector\");\nvar detector_4 = require(\"./posenet/detector\");\nvar types_1 = require(\"./types\");\n/**\n * Create a pose detector instance.\n *\n * @param model The name of the pipeline to load.\n */\nfunction createDetector(model, modelConfig) {\n    return __awaiter(this, void 0, void 0, function () {\n        var config, runtime;\n        return __generator(this, function (_a) {\n            switch (model) {\n                case types_1.SupportedModels.PoseNet:\n                    return [2 /*return*/, detector_4.load(modelConfig)];\n                case types_1.SupportedModels.BlazePose:\n                    config = modelConfig;\n                    runtime = void 0;\n                    if (config != null) {\n                        if (config.runtime === 'tfjs') {\n                            return [2 /*return*/, detector_2.load(modelConfig)];\n                        }\n                        if (config.runtime === 'mediapipe') {\n                            return [2 /*return*/, detector_1.load(modelConfig)];\n                        }\n                        runtime = config.runtime;\n                    }\n                    throw new Error(\"Expect modelConfig.runtime to be either 'tfjs' \" +\n                        (\"or 'mediapipe', but got \" + runtime));\n                case types_1.SupportedModels.MoveNet:\n                    return [2 /*return*/, detector_3.load(modelConfig)];\n                default:\n                    throw new Error(model + \" is not a supported model name.\");\n            }\n            return [2 /*return*/];\n        });\n    });\n}\nexports.createDetector = createDetector;\n//# sourceMappingURL=create_detector.js.map","\nvar __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {\n    return new (P || (P = Promise))(function (resolve, reject) {\n        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }\n        function rejected(value) { try { step(generator[\"throw\"](value)); } catch (e) { reject(e); } }\n        function step(result) { result.done ? resolve(result.value) : new P(function (resolve) { resolve(result.value); }).then(fulfilled, rejected); }\n        step((generator = generator.apply(thisArg, _arguments || [])).next());\n    });\n};\nvar __generator = (this && this.__generator) || function (thisArg, body) {\n    var _ = { label: 0, sent: function() { if (t[0] & 1) throw t[1]; return t[1]; }, trys: [], ops: [] }, f, y, t, g;\n    return g = { next: verb(0), \"throw\": verb(1), \"return\": verb(2) }, typeof Symbol === \"function\" && (g[Symbol.iterator] = function() { return this; }), g;\n    function verb(n) { return function (v) { return step([n, v]); }; }\n    function step(op) {\n        if (f) throw new TypeError(\"Generator is already executing.\");\n        while (_) try {\n            if (f = 1, y && (t = op[0] & 2 ? y[\"return\"] : op[0] ? y[\"throw\"] || ((t = y[\"return\"]) && t.call(y), 0) : y.next) && !(t = t.call(y, op[1])).done) return t;\n            if (y = 0, t) op = [op[0] & 2, t.value];\n            switch (op[0]) {\n                case 0: case 1: t = op; break;\n                case 4: _.label++; return { value: op[1], done: false };\n                case 5: _.label++; y = op[1]; op = [0]; continue;\n                case 7: op = _.ops.pop(); _.trys.pop(); continue;\n                default:\n                    if (!(t = _.trys, t = t.length > 0 && t[t.length - 1]) && (op[0] === 6 || op[0] === 2)) { _ = 0; continue; }\n                    if (op[0] === 3 && (!t || (op[1] > t[0] && op[1] < t[3]))) { _.label = op[1]; break; }\n                    if (op[0] === 6 && _.label < t[1]) { _.label = t[1]; t = op; break; }\n                    if (t && _.label < t[2]) { _.label = t[2]; _.ops.push(op); break; }\n                    if (t[2]) _.ops.pop();\n                    _.trys.pop(); continue;\n            }\n            op = body.call(thisArg, _);\n        } catch (e) { op = [6, e]; y = 0; } finally { f = t = 0; }\n        if (op[0] & 5) throw op[1]; return { value: op[0] ? op[1] : void 0, done: true };\n    }\n};\nObject.defineProperty(exports, \"__esModule\", { value: true });\n/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nvar pose = require(\"@mediapipe/pose\");\nvar constants_1 = require(\"../constants\");\nvar detector_utils_1 = require(\"./detector_utils\");\n/**\n * MediaPipe detector class.\n */\nvar BlazePoseMediaPipeDetector = /** @class */ (function () {\n    // Should not be called outside.\n    function BlazePoseMediaPipeDetector(config) {\n        var _this = this;\n        // This will be filled out by asynchronous calls to onResults. They will be\n        // stable after `await send` is called on the pose solution.\n        this.width = 0;\n        this.height = 0;\n        this.selfieMode = false;\n        this.poseSolution = new pose.Pose({\n            locateFile: function (path, base) {\n                if (config.solutionPath) {\n                    var solutionPath = config.solutionPath.replace(/\\/+$/, '');\n                    return solutionPath + \"/\" + path;\n                }\n                return base + \"/\" + path;\n            }\n        });\n        var modelComplexity;\n        switch (config.modelType) {\n            case 'lite':\n                modelComplexity = 0;\n                break;\n            case 'heavy':\n                modelComplexity = 2;\n                break;\n            case 'full':\n            default:\n                modelComplexity = 1;\n                break;\n        }\n        this.poseSolution.setOptions({\n            modelComplexity: modelComplexity,\n            smoothLandmarks: config.enableSmoothing || true,\n            selfieMode: this.selfieMode,\n        });\n        this.poseSolution.onResults(function (results) {\n            _this.height = results.image.height;\n            _this.width = results.image.width;\n            _this.poses = _this.translateOutputs(results);\n        });\n    }\n    BlazePoseMediaPipeDetector.prototype.translateOutputs = function (results) {\n        var _this = this;\n        return results.poseLandmarks != null ? [{\n                keypoints: results.poseLandmarks.map(function (landmark, i) { return ({\n                    x: landmark.x * _this.width,\n                    y: landmark.y * _this.height,\n                    z: landmark.z,\n                    score: landmark.visibility,\n                    name: constants_1.BLAZEPOSE_KEYPOINTS[i]\n                }); })\n            }] :\n            [];\n    };\n    /**\n     * Estimates poses for an image or video frame.\n     *\n     * It returns a single pose or multiple poses based on the maxPose parameter\n     * from the `config`.\n     *\n     * @param image\n     * ImageData|HTMLImageElement|HTMLCanvasElement|HTMLVideoElement The input\n     * image to feed through the network.\n     *\n     * @param config Optional.\n     *       maxPoses: Optional. Max number of poses to estimate.\n     *       When maxPoses = 1, a single pose is detected, it is usually much more\n     *       efficient than maxPoses > 1. When maxPoses > 1, multiple poses are\n     *       detected.\n     *\n     *       flipHorizontal: Optional. Default to false. When image data comes\n     *       from camera, the result has to flip horizontally.\n     *\n     *       enableSmoothing: Optional. Default to true. Smooth pose landmarks\n     *       coordinates and visibility scores to reduce jitter.\n     *\n     * @param timestamp Optional. In milliseconds. This is useful when image is\n     *     a tensor, which doesn't have timestamp info. Or to override timestamp\n     *     in a video.\n     *\n     * @return An array of `Pose`s.\n     */\n    BlazePoseMediaPipeDetector.prototype.estimatePoses = function (image, estimationConfig, timestamp) {\n        return __awaiter(this, void 0, void 0, function () {\n            return __generator(this, function (_a) {\n                switch (_a.label) {\n                    case 0:\n                        if (estimationConfig && estimationConfig.flipHorizontal &&\n                            (estimationConfig.flipHorizontal !== this.selfieMode)) {\n                            this.selfieMode = estimationConfig.flipHorizontal;\n                            this.poseSolution.setOptions({\n                                selfieMode: this.selfieMode,\n                            });\n                        }\n                        return [4 /*yield*/, this.poseSolution.send({ image: image }, timestamp)];\n                    case 1:\n                        _a.sent();\n                        return [2 /*return*/, this.poses];\n                }\n            });\n        });\n    };\n    BlazePoseMediaPipeDetector.prototype.dispose = function () {\n        this.poseSolution.close();\n    };\n    BlazePoseMediaPipeDetector.prototype.reset = function () {\n        this.poseSolution.reset();\n    };\n    BlazePoseMediaPipeDetector.prototype.initialize = function () {\n        return this.poseSolution.initialize();\n    };\n    return BlazePoseMediaPipeDetector;\n}());\n/**\n * Loads the MediaPipe solution.\n *\n * @param modelConfig ModelConfig object that contains parameters for\n * the BlazePose loading process. Please find more details of each parameters\n * in the documentation of the `BlazePoseMediaPipeModelConfig` interface.\n */\nfunction load(modelConfig) {\n    return __awaiter(this, void 0, void 0, function () {\n        var config, result;\n        return __generator(this, function (_a) {\n            switch (_a.label) {\n                case 0:\n                    config = detector_utils_1.validateModelConfig(modelConfig);\n                    result = new BlazePoseMediaPipeDetector(config);\n                    return [4 /*yield*/, result.initialize()];\n                case 1:\n                    _a.sent();\n                    return [2 /*return*/, result];\n            }\n        });\n    });\n}\nexports.load = load;\n//# sourceMappingURL=detector.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\n/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n// Don't change the order. The order needs to be consistent with the model\n// keypoint result list.\nexports.COCO_KEYPOINTS = [\n    'nose', 'left_eye', 'right_eye', 'left_ear', 'right_ear', 'left_shoulder',\n    'right_shoulder', 'left_elbow', 'right_elbow', 'left_wrist', 'right_wrist',\n    'left_hip', 'right_hip', 'left_knee', 'right_knee', 'left_ankle',\n    'right_ankle'\n];\n// Don't change the order. The order needs to be consistent with the model\n// keypoint result list.\nexports.BLAZEPOSE_KEYPOINTS = [\n    'nose',\n    'left_eye_inner',\n    'left_eye',\n    'left_eye_outer',\n    'right_eye_inner',\n    'right_eye',\n    'right_eye_outer',\n    'left_ear',\n    'right_ear',\n    'mouth_left',\n    'mouth_right',\n    'left_shoulder',\n    'right_shoulder',\n    'left_elbow',\n    'right_elbow',\n    'left_wrist',\n    'right_wrist',\n    'left_pinky',\n    'right_pinky',\n    'left_index',\n    'right_index',\n    'left_thumb',\n    'right_thumb',\n    'left_hip',\n    'right_hip',\n    'left_knee',\n    'right_knee',\n    'left_ankle',\n    'right_ankle',\n    'left_heel',\n    'right_heel',\n    'left_foot_index',\n    'right_foot_index'\n];\nexports.BLAZEPOSE_KEYPOINTS_BY_SIDE = {\n    left: [1, 2, 3, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31],\n    right: [4, 5, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32],\n    middle: [0]\n};\nexports.COCO_KEYPOINTS_BY_SIDE = {\n    left: [1, 3, 5, 7, 9, 11, 13, 15],\n    right: [2, 4, 6, 8, 10, 12, 14, 16],\n    middle: [0]\n};\nexports.COCO_CONNECTED_KEYPOINTS_PAIRS = [\n    [0, 1], [0, 2], [1, 3], [2, 4], [5, 6], [5, 7], [5, 11], [6, 8], [6, 12],\n    [7, 9], [8, 10], [11, 12], [11, 13], [12, 14], [13, 15], [14, 16]\n];\nexports.BLAZEPOSE_CONNECTED_KEYPOINTS_PAIRS = [\n    [0, 1], [0, 4], [1, 2], [2, 3], [3, 7], [4, 5],\n    [5, 6], [6, 8], [9, 10], [11, 12], [11, 13], [11, 23],\n    [12, 14], [14, 16], [12, 24], [13, 15], [15, 17], [16, 18],\n    [16, 20], [15, 17], [15, 19], [15, 21], [16, 22], [17, 19],\n    [18, 20], [23, 25], [23, 24], [24, 26], [25, 27], [26, 28],\n    [27, 29], [28, 30], [27, 31], [28, 32], [29, 31], [30, 32]\n];\n//# sourceMappingURL=constants.js.map","\n/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nvar __assign = (this && this.__assign) || function () {\n    __assign = Object.assign || function(t) {\n        for (var s, i = 1, n = arguments.length; i < n; i++) {\n            s = arguments[i];\n            for (var p in s) if (Object.prototype.hasOwnProperty.call(s, p))\n                t[p] = s[p];\n        }\n        return t;\n    };\n    return __assign.apply(this, arguments);\n};\nObject.defineProperty(exports, \"__esModule\", { value: true });\nvar constants_1 = require(\"./constants\");\nfunction validateModelConfig(modelConfig) {\n    if (modelConfig == null) {\n        return __assign({}, constants_1.DEFAULT_BLAZEPOSE_MODEL_CONFIG);\n    }\n    var config = __assign({}, modelConfig);\n    config.runtime = 'mediapipe';\n    if (config.enableSmoothing == null) {\n        config.enableSmoothing = constants_1.DEFAULT_BLAZEPOSE_MODEL_CONFIG.enableSmoothing;\n    }\n    if (config.modelType == null) {\n        config.modelType = constants_1.DEFAULT_BLAZEPOSE_MODEL_CONFIG.modelType;\n    }\n    return config;\n}\nexports.validateModelConfig = validateModelConfig;\nfunction validateEstimationConfig(estimationConfig) {\n    if (estimationConfig == null) {\n        return __assign({}, constants_1.DEFAULT_BLAZEPOSE_ESTIMATION_CONFIG);\n    }\n    var config = __assign({}, estimationConfig);\n    if (config.maxPoses == null) {\n        config.maxPoses = 1;\n    }\n    if (config.maxPoses <= 0) {\n        throw new Error(\"Invalid maxPoses \" + config.maxPoses + \". Should be > 0.\");\n    }\n    if (config.maxPoses > 1) {\n        throw new Error('Multi-pose detection is not implemented yet. Please set maxPoses ' +\n            'to 1.');\n    }\n    if (config.flipHorizontal == null) {\n        config.flipHorizontal = constants_1.DEFAULT_BLAZEPOSE_ESTIMATION_CONFIG.flipHorizontal;\n    }\n    return config;\n}\nexports.validateEstimationConfig = validateEstimationConfig;\n//# sourceMappingURL=detector_utils.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.DEFAULT_BLAZEPOSE_MODEL_CONFIG = {\n    runtime: 'mediapipe',\n    enableSmoothing: true,\n    modelType: 'full'\n};\nexports.DEFAULT_BLAZEPOSE_ESTIMATION_CONFIG = {\n    maxPoses: 1,\n    flipHorizontal: false\n};\n//# sourceMappingURL=constants.js.map","\n/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nvar __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {\n    return new (P || (P = Promise))(function (resolve, reject) {\n        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }\n        function rejected(value) { try { step(generator[\"throw\"](value)); } catch (e) { reject(e); } }\n        function step(result) { result.done ? resolve(result.value) : new P(function (resolve) { resolve(result.value); }).then(fulfilled, rejected); }\n        step((generator = generator.apply(thisArg, _arguments || [])).next());\n    });\n};\nvar __generator = (this && this.__generator) || function (thisArg, body) {\n    var _ = { label: 0, sent: function() { if (t[0] & 1) throw t[1]; return t[1]; }, trys: [], ops: [] }, f, y, t, g;\n    return g = { next: verb(0), \"throw\": verb(1), \"return\": verb(2) }, typeof Symbol === \"function\" && (g[Symbol.iterator] = function() { return this; }), g;\n    function verb(n) { return function (v) { return step([n, v]); }; }\n    function step(op) {\n        if (f) throw new TypeError(\"Generator is already executing.\");\n        while (_) try {\n            if (f = 1, y && (t = op[0] & 2 ? y[\"return\"] : op[0] ? y[\"throw\"] || ((t = y[\"return\"]) && t.call(y), 0) : y.next) && !(t = t.call(y, op[1])).done) return t;\n            if (y = 0, t) op = [op[0] & 2, t.value];\n            switch (op[0]) {\n                case 0: case 1: t = op; break;\n                case 4: _.label++; return { value: op[1], done: false };\n                case 5: _.label++; y = op[1]; op = [0]; continue;\n                case 7: op = _.ops.pop(); _.trys.pop(); continue;\n                default:\n                    if (!(t = _.trys, t = t.length > 0 && t[t.length - 1]) && (op[0] === 6 || op[0] === 2)) { _ = 0; continue; }\n                    if (op[0] === 3 && (!t || (op[1] > t[0] && op[1] < t[3]))) { _.label = op[1]; break; }\n                    if (op[0] === 6 && _.label < t[1]) { _.label = t[1]; t = op; break; }\n                    if (t && _.label < t[2]) { _.label = t[2]; _.ops.push(op); break; }\n                    if (t[2]) _.ops.pop();\n                    _.trys.pop(); continue;\n            }\n            op = body.call(thisArg, _);\n        } catch (e) { op = [6, e]; y = 0; } finally { f = t = 0; }\n        if (op[0] & 5) throw op[1]; return { value: op[0] ? op[1] : void 0, done: true };\n    }\n};\nObject.defineProperty(exports, \"__esModule\", { value: true });\nvar tfconv = require(\"@tensorflow/tfjs-converter\");\nvar tf = require(\"@tensorflow/tfjs-core\");\nvar constants_1 = require(\"../calculators/constants\");\nvar convert_image_to_tensor_1 = require(\"../calculators/convert_image_to_tensor\");\nvar image_utils_1 = require(\"../calculators/image_utils\");\nvar is_video_1 = require(\"../calculators/is_video\");\nvar keypoints_smoothing_1 = require(\"../calculators/keypoints_smoothing\");\nvar normalized_keypoints_to_keypoints_1 = require(\"../calculators/normalized_keypoints_to_keypoints\");\nvar shift_image_value_1 = require(\"../calculators/shift_image_value\");\nvar constants_2 = require(\"../constants\");\nvar calculate_alignment_points_rects_1 = require(\"./calculators/calculate_alignment_points_rects\");\nvar calculate_landmark_projection_1 = require(\"./calculators/calculate_landmark_projection\");\nvar create_ssd_anchors_1 = require(\"./calculators/create_ssd_anchors\");\nvar detector_inference_1 = require(\"./calculators/detector_inference\");\nvar landmarks_to_detection_1 = require(\"./calculators/landmarks_to_detection\");\nvar non_max_suppression_1 = require(\"./calculators/non_max_suppression\");\nvar refine_landmarks_from_heatmap_1 = require(\"./calculators/refine_landmarks_from_heatmap\");\nvar remove_detection_letterbox_1 = require(\"./calculators/remove_detection_letterbox\");\nvar remove_landmark_letterbox_1 = require(\"./calculators/remove_landmark_letterbox\");\nvar tensors_to_detections_1 = require(\"./calculators/tensors_to_detections\");\nvar tensors_to_landmarks_1 = require(\"./calculators/tensors_to_landmarks\");\nvar transform_rect_1 = require(\"./calculators/transform_rect\");\nvar visibility_smoothing_1 = require(\"./calculators/visibility_smoothing\");\nvar constants = require(\"./constants\");\nvar detector_utils_1 = require(\"./detector_utils\");\n/**\n * BlazePose detector class.\n */\nvar BlazePoseTfjsDetector = /** @class */ (function () {\n    function BlazePoseTfjsDetector(detectorModel, landmarkModel, enableSmoothing, modelType) {\n        this.detectorModel = detectorModel;\n        this.landmarkModel = landmarkModel;\n        this.enableSmoothing = enableSmoothing;\n        this.modelType = modelType;\n        // Store global states.\n        this.regionOfInterest = null;\n        this.anchors =\n            create_ssd_anchors_1.createSsdAnchors(constants.BLAZEPOSE_DETECTOR_ANCHOR_CONFIGURATION);\n        var anchorW = tf.tensor1d(this.anchors.map(function (a) { return a.width; }));\n        var anchorH = tf.tensor1d(this.anchors.map(function (a) { return a.height; }));\n        var anchorX = tf.tensor1d(this.anchors.map(function (a) { return a.xCenter; }));\n        var anchorY = tf.tensor1d(this.anchors.map(function (a) { return a.yCenter; }));\n        this.anchorTensor = { x: anchorX, y: anchorY, w: anchorW, h: anchorH };\n    }\n    /**\n     * Estimates poses for an image or video frame.\n     *\n     * It returns a single pose or multiple poses based on the maxPose parameter\n     * from the `config`.\n     *\n     * @param image\n     * ImageData|HTMLImageElement|HTMLCanvasElement|HTMLVideoElement The input\n     * image to feed through the network.\n     *\n     * @param estimationConfig Optional. See `BlazePoseTfjsEstimationConfig`\n     *       documentation for detail.\n     *\n     * @param timestamp Optional. In milliseconds. This is useful when image is\n     *     a tensor, which doesn't have timestamp info. Or to override timestamp\n     *     in a video.\n     *\n     * @return An array of `Pose`s.\n     */\n    // TF.js implementation of the mediapipe pose detection pipeline.\n    // ref graph:\n    // https://github.com/google/mediapipe/blob/master/mediapipe/modules/pose_landmark/pose_landmark_cpu.pbtxt\n    BlazePoseTfjsDetector.prototype.estimatePoses = function (image, estimationConfig, timestamp) {\n        return __awaiter(this, void 0, void 0, function () {\n            var config, imageSize, image3d, poseRect, detections, firstDetection, poseLandmarks, actualLandmarks, auxiliaryLandmarks, poseScore, _a, actualLandmarksFiltered, auxiliaryLandmarksFiltered, poseRectFromLandmarks, keypoints, pose;\n            return __generator(this, function (_b) {\n                switch (_b.label) {\n                    case 0:\n                        config = detector_utils_1.validateEstimationConfig(estimationConfig);\n                        if (image == null) {\n                            this.reset();\n                            return [2 /*return*/, []];\n                        }\n                        this.maxPoses = config.maxPoses;\n                        // User provided timestamp will override video's timestamp.\n                        if (timestamp != null) {\n                            this.timestamp = timestamp * constants_1.MILLISECOND_TO_MICRO_SECONDS;\n                        }\n                        else {\n                            // For static images, timestamp should be null.\n                            this.timestamp =\n                                is_video_1.isVideo(image) ? image.currentTime * constants_1.SECOND_TO_MICRO_SECONDS : null;\n                        }\n                        imageSize = image_utils_1.getImageSize(image);\n                        image3d = tf.tidy(function () { return tf.cast(image_utils_1.toImageTensor(image), 'float32'); });\n                        poseRect = this.regionOfInterest;\n                        if (!(poseRect == null)) return [3 /*break*/, 2];\n                        return [4 /*yield*/, this.detectPose(image3d)];\n                    case 1:\n                        detections = _b.sent();\n                        if (detections.length === 0) {\n                            this.reset();\n                            image3d.dispose();\n                            return [2 /*return*/, []];\n                        }\n                        firstDetection = detections[0];\n                        // Calculates region of interest based on pose detection, so that can be\n                        // used to detect landmarks.\n                        poseRect = this.poseDetectionToRoi(firstDetection, imageSize);\n                        _b.label = 2;\n                    case 2: return [4 /*yield*/, this.poseLandmarksByRoi(poseRect, image3d)];\n                    case 3:\n                        poseLandmarks = _b.sent();\n                        image3d.dispose();\n                        if (poseLandmarks == null) {\n                            this.reset();\n                            return [2 /*return*/, []];\n                        }\n                        actualLandmarks = poseLandmarks.actualLandmarks, auxiliaryLandmarks = poseLandmarks.auxiliaryLandmarks, poseScore = poseLandmarks.poseScore;\n                        _a = this.poseLandmarkFiltering(actualLandmarks, auxiliaryLandmarks, imageSize), actualLandmarksFiltered = _a.actualLandmarksFiltered, auxiliaryLandmarksFiltered = _a.auxiliaryLandmarksFiltered;\n                        poseRectFromLandmarks = this.poseLandmarksToRoi(auxiliaryLandmarksFiltered, imageSize);\n                        // Cache roi for next image.\n                        this.regionOfInterest = poseRectFromLandmarks;\n                        keypoints = actualLandmarksFiltered != null ?\n                            normalized_keypoints_to_keypoints_1.normalizedKeypointsToKeypoints(actualLandmarksFiltered, imageSize) :\n                            null;\n                        // Add keypoint name.\n                        if (keypoints != null) {\n                            keypoints.forEach(function (keypoint, i) {\n                                keypoint.name = constants_2.BLAZEPOSE_KEYPOINTS[i];\n                            });\n                        }\n                        pose = { score: poseScore, keypoints: keypoints };\n                        return [2 /*return*/, [pose]];\n                }\n            });\n        });\n    };\n    BlazePoseTfjsDetector.prototype.dispose = function () {\n        this.detectorModel.dispose();\n        this.landmarkModel.dispose();\n        tf.dispose([\n            this.anchorTensor.x, this.anchorTensor.y, this.anchorTensor.w,\n            this.anchorTensor.h\n        ]);\n    };\n    BlazePoseTfjsDetector.prototype.reset = function () {\n        this.regionOfInterest = null;\n        this.visibilitySmoothingFilterActual = null;\n        this.visibilitySmoothingFilterAuxiliary = null;\n        this.landmarksSmoothingFilterActual = null;\n        this.landmarksSmoothingFilterAuxiliary = null;\n    };\n    // Detects poses.\n    // Subgraph: PoseDetectionCpu.\n    // ref:\n    // https://github.com/google/mediapipe/blob/master/mediapipe/modules/pose_detection/pose_detection_cpu.pbtxt\n    BlazePoseTfjsDetector.prototype.detectPose = function (image) {\n        return __awaiter(this, void 0, void 0, function () {\n            var _a, imageTensor, padding, imageValueShifted, _b, boxes, scores, detections, selectedDetections, newDetections;\n            return __generator(this, function (_c) {\n                switch (_c.label) {\n                    case 0:\n                        _a = convert_image_to_tensor_1.convertImageToTensor(image, constants.BLAZEPOSE_DETECTOR_IMAGE_TO_TENSOR_CONFIG), imageTensor = _a.imageTensor, padding = _a.padding;\n                        imageValueShifted = shift_image_value_1.shiftImageValue(imageTensor, [-1, 1]);\n                        _b = detector_inference_1.detectorInference(imageValueShifted, this.detectorModel), boxes = _b.boxes, scores = _b.scores;\n                        return [4 /*yield*/, tensors_to_detections_1.tensorsToDetections([scores, boxes], this.anchorTensor, constants.BLAZEPOSE_TENSORS_TO_DETECTION_CONFIGURATION)];\n                    case 1:\n                        detections = _c.sent();\n                        return [4 /*yield*/, non_max_suppression_1.nonMaxSuppression(detections, this.maxPoses, constants.BLAZEPOSE_DETECTOR_NON_MAX_SUPPRESSION_CONFIGURATION\n                                .minSuppressionThreshold, constants.BLAZEPOSE_DETECTOR_NON_MAX_SUPPRESSION_CONFIGURATION\n                                .minScoreThreshold)];\n                    case 2:\n                        selectedDetections = _c.sent();\n                        newDetections = remove_detection_letterbox_1.removeDetectionLetterbox(selectedDetections, padding);\n                        tf.dispose([imageTensor, imageValueShifted, scores, boxes]);\n                        return [2 /*return*/, newDetections];\n                }\n            });\n        });\n    };\n    // Calculates region of interest from a detection.\n    // Subgraph: PoseDetectionToRoi.\n    // ref:\n    // https://github.com/google/mediapipe/blob/master/mediapipe/modules/pose_landmark/pose_detection_to_roi.pbtxt\n    // If detection is not null, imageSize should not be null either.\n    BlazePoseTfjsDetector.prototype.poseDetectionToRoi = function (detection, imageSize) {\n        var startKeypointIndex;\n        var endKeypointIndex;\n        // Converts pose detection into a rectangle based on center and scale\n        // alignment points.\n        startKeypointIndex = 0;\n        endKeypointIndex = 1;\n        // PoseDetectionToRoi: AlignmentPointsRectsCalculator.\n        var rawRoi = calculate_alignment_points_rects_1.calculateAlignmentPointsRects(detection, imageSize, {\n            rotationVectorEndKeypointIndex: endKeypointIndex,\n            rotationVectorStartKeypointIndex: startKeypointIndex,\n            rotationVectorTargetAngleDegree: 90\n        });\n        // Expands pose rect with marging used during training.\n        // PoseDetectionToRoi: RectTransformationCalculation.\n        var roi = transform_rect_1.transformNormalizedRect(rawRoi, imageSize, constants.BLAZEPOSE_DETECTOR_RECT_TRANSFORMATION_CONFIG);\n        return roi;\n    };\n    // Predict pose landmarks.\n    // subgraph: PoseLandmarksByRoiCpu\n    // ref:\n    // https://github.com/google/mediapipe/blob/master/mediapipe/modules/pose_landmark/pose_landmark_by_roi_cpu.pbtxt\n    // When poseRect is not null, image should not be null either.\n    BlazePoseTfjsDetector.prototype.poseLandmarksByRoi = function (poseRect, image) {\n        return __awaiter(this, void 0, void 0, function () {\n            var _a, imageTensor, padding, imageValueShifted, landmarkResult, landmarkTensor, poseFlagTensor, heatmapTensor, poseScore, landmarks, refinedLandmarks, adjustedLandmarks, landmarksProjected, actualLandmarks, auxiliaryLandmarks;\n            return __generator(this, function (_b) {\n                switch (_b.label) {\n                    case 0:\n                        _a = convert_image_to_tensor_1.convertImageToTensor(image, constants.BLAZEPOSE_LANDMARK_IMAGE_TO_TENSOR_CONFIG, poseRect), imageTensor = _a.imageTensor, padding = _a.padding;\n                        imageValueShifted = shift_image_value_1.shiftImageValue(imageTensor, [0, 1]);\n                        landmarkResult = this.landmarkModel.predict(imageValueShifted);\n                        switch (this.modelType) {\n                            case 'lite':\n                                landmarkTensor = landmarkResult[4];\n                                poseFlagTensor = landmarkResult[0];\n                                heatmapTensor = landmarkResult[3];\n                                break;\n                            case 'full':\n                                landmarkTensor = landmarkResult[4];\n                                poseFlagTensor = landmarkResult[3];\n                                heatmapTensor = landmarkResult[2];\n                                break;\n                            case 'heavy':\n                                landmarkTensor = landmarkResult[2];\n                                poseFlagTensor = landmarkResult[4];\n                                heatmapTensor = landmarkResult[1];\n                                break;\n                            default:\n                                throw new Error('Model type must be one of lite, full or heavy,' +\n                                    (\"but got \" + this.modelType));\n                        }\n                        return [4 /*yield*/, poseFlagTensor.data()];\n                    case 1:\n                        poseScore = (_b.sent())[0];\n                        // Applies a threshold to the confidence score to determine whether a pose\n                        // is present.\n                        if (poseScore < constants.BLAZEPOSE_POSE_PRESENCE_SCORE) {\n                            tf.dispose(landmarkResult);\n                            tf.dispose([imageTensor, imageValueShifted]);\n                            return [2 /*return*/, null];\n                        }\n                        return [4 /*yield*/, tensors_to_landmarks_1.tensorsToLandmarks(landmarkTensor, constants.BLAZEPOSE_TENSORS_TO_LANDMARKS_CONFIG)];\n                    case 2:\n                        landmarks = _b.sent();\n                        return [4 /*yield*/, refine_landmarks_from_heatmap_1.refineLandmarksFromHeatmap(landmarks, heatmapTensor, constants.BLAZEPOSE_REFINE_LANDMARKS_FROM_HEATMAP_CONFIG)];\n                    case 3:\n                        refinedLandmarks = _b.sent();\n                        adjustedLandmarks = remove_landmark_letterbox_1.removeLandmarkLetterbox(refinedLandmarks, padding);\n                        landmarksProjected = calculate_landmark_projection_1.calculateLandmarkProjection(adjustedLandmarks, poseRect);\n                        actualLandmarks = landmarksProjected.slice(0, constants.BLAZEPOSE_NUM_KEYPOINTS);\n                        auxiliaryLandmarks = landmarksProjected.slice(constants.BLAZEPOSE_NUM_KEYPOINTS, constants.BLAZEPOSE_NUM_AUXILIARY_KEYPOINTS);\n                        tf.dispose(landmarkResult);\n                        tf.dispose([imageTensor, imageValueShifted]);\n                        return [2 /*return*/, { actualLandmarks: actualLandmarks, auxiliaryLandmarks: auxiliaryLandmarks, poseScore: poseScore }];\n                }\n            });\n        });\n    };\n    // Calculate region of interest (ROI) from landmarks.\n    // Subgraph: PoseLandmarksToRoiCpu\n    // ref:\n    // https://github.com/google/mediapipe/blob/master/mediapipe/modules/pose_landmark/pose_landmarks_to_roi.pbtxt\n    // When landmarks is not null, imageSize should not be null either.\n    BlazePoseTfjsDetector.prototype.poseLandmarksToRoi = function (landmarks, imageSize) {\n        // PoseLandmarksToRoi: LandmarksToDetectionCalculator.\n        var detection = landmarks_to_detection_1.landmarksToDetection(landmarks);\n        // Converts detection into a rectangle based on center and scale alignment\n        // points.\n        // PoseLandmarksToRoi: AlignmentPointsRectsCalculator.\n        var rawRoi = calculate_alignment_points_rects_1.calculateAlignmentPointsRects(detection, imageSize, {\n            rotationVectorStartKeypointIndex: 0,\n            rotationVectorEndKeypointIndex: 1,\n            rotationVectorTargetAngleDegree: 90\n        });\n        // Expands pose rect with marging used during training.\n        // PoseLandmarksToRoi: RectTransformationCalculator.\n        var roi = transform_rect_1.transformNormalizedRect(rawRoi, imageSize, constants.BLAZEPOSE_DETECTOR_RECT_TRANSFORMATION_CONFIG);\n        return roi;\n    };\n    // Filter landmarks temporally to reduce jitter.\n    // Subgraph: PoseLandmarkFiltering\n    // ref:\n    // https://github.com/google/mediapipe/blob/master/mediapipe/modules/pose_landmark/pose_landmark_filtering.pbtxt\n    BlazePoseTfjsDetector.prototype.poseLandmarkFiltering = function (actualLandmarks, auxiliaryLandmarks, imageSize) {\n        var actualLandmarksFiltered;\n        var auxiliaryLandmarksFiltered;\n        if (this.timestamp == null || !this.enableSmoothing) {\n            actualLandmarksFiltered = actualLandmarks;\n            auxiliaryLandmarksFiltered = auxiliaryLandmarks;\n        }\n        else {\n            var auxDetection = landmarks_to_detection_1.landmarksToDetection(auxiliaryLandmarks);\n            var objectScaleROI = calculate_alignment_points_rects_1.calculateAlignmentPointsRects(auxDetection, imageSize, {\n                rotationVectorEndKeypointIndex: 0,\n                rotationVectorStartKeypointIndex: 1,\n                rotationVectorTargetAngleDegree: 90\n            });\n            // Smoothes pose landmark visibilities to reduce jitter.\n            if (this.visibilitySmoothingFilterActual == null) {\n                this.visibilitySmoothingFilterActual = new visibility_smoothing_1.LowPassVisibilityFilter(constants.BLAZEPOSE_VISIBILITY_SMOOTHING_CONFIG);\n            }\n            actualLandmarksFiltered =\n                this.visibilitySmoothingFilterActual.apply(actualLandmarks);\n            if (this.visibilitySmoothingFilterAuxiliary == null) {\n                this.visibilitySmoothingFilterAuxiliary = new visibility_smoothing_1.LowPassVisibilityFilter(constants.BLAZEPOSE_VISIBILITY_SMOOTHING_CONFIG);\n            }\n            auxiliaryLandmarksFiltered =\n                this.visibilitySmoothingFilterAuxiliary.apply(auxiliaryLandmarks);\n            // Smoothes pose landmark coordinates to reduce jitter.\n            if (this.landmarksSmoothingFilterActual == null) {\n                this.landmarksSmoothingFilterActual = new keypoints_smoothing_1.KeypointsSmoothingFilter(constants.BLAZEPOSE_LANDMARKS_SMOOTHING_CONFIG_ACTUAL);\n            }\n            actualLandmarksFiltered = this.landmarksSmoothingFilterActual.apply(actualLandmarksFiltered, this.timestamp, imageSize, true /* normalized */, objectScaleROI);\n            if (this.landmarksSmoothingFilterAuxiliary == null) {\n                this.landmarksSmoothingFilterAuxiliary = new keypoints_smoothing_1.KeypointsSmoothingFilter(constants.BLAZEPOSE_LANDMARKS_SMOOTHING_CONFIG_AUXILIARY);\n            }\n            auxiliaryLandmarksFiltered = this.landmarksSmoothingFilterAuxiliary.apply(auxiliaryLandmarksFiltered, this.timestamp, imageSize, true /* normalized */, objectScaleROI);\n        }\n        return { actualLandmarksFiltered: actualLandmarksFiltered, auxiliaryLandmarksFiltered: auxiliaryLandmarksFiltered };\n    };\n    return BlazePoseTfjsDetector;\n}());\n/**\n * Loads the BlazePose model.\n *\n * @param modelConfig ModelConfig object that contains parameters for\n * the BlazePose loading process. Please find more details of each parameters\n * in the documentation of the `BlazePoseTfjsModelConfig` interface.\n */\nfunction load(modelConfig) {\n    return __awaiter(this, void 0, void 0, function () {\n        var config, detectorFromTFHub, landmarkFromTFHub, _a, detectorModel, landmarkModel;\n        return __generator(this, function (_b) {\n            switch (_b.label) {\n                case 0:\n                    config = detector_utils_1.validateModelConfig(modelConfig);\n                    detectorFromTFHub = (config.detectorModelUrl.indexOf('https://tfhub.dev') > -1);\n                    landmarkFromTFHub = (config.landmarkModelUrl.indexOf('https://tfhub.dev') > -1);\n                    return [4 /*yield*/, Promise.all([\n                            tfconv.loadGraphModel(config.detectorModelUrl, { fromTFHub: detectorFromTFHub }),\n                            tfconv.loadGraphModel(config.landmarkModelUrl, { fromTFHub: landmarkFromTFHub })\n                        ])];\n                case 1:\n                    _a = _b.sent(), detectorModel = _a[0], landmarkModel = _a[1];\n                    return [2 /*return*/, new BlazePoseTfjsDetector(detectorModel, landmarkModel, config.enableSmoothing, config.modelType)];\n            }\n        });\n    });\n}\nexports.load = load;\n//# sourceMappingURL=detector.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\n/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nexports.MICRO_SECONDS_TO_SECOND = 1e-6;\nexports.SECOND_TO_MICRO_SECONDS = 1e6;\nexports.MILLISECOND_TO_MICRO_SECONDS = 1000;\n//# sourceMappingURL=constants.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\n/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nvar tf = require(\"@tensorflow/tfjs-core\");\nvar image_utils_1 = require(\"./image_utils\");\n/**\n * Convert an image or part of it to an image tensor.\n *\n * @param image An image, video frame or image tensor.\n * @param config\n *      inputResolution: The target height and width.\n *      keepAspectRatio?: Whether target tensor should keep aspect ratio.\n * @param normRect A normalized rectangle, representing the subarea to crop from\n *      the image. If normRect is provided, the returned image tensor represents\n *      the subarea.\n */\nfunction convertImageToTensor(image, config, normRect) {\n    var inputResolution = config.inputResolution, keepAspectRatio = config.keepAspectRatio;\n    // Ref:\n    // https://github.com/google/mediapipe/blob/master/mediapipe/calculators/tensor/image_to_tensor_calculator.cc\n    var imageSize = image_utils_1.getImageSize(image);\n    var roi = image_utils_1.getRoi(imageSize, normRect);\n    var padding = image_utils_1.padRoi(roi, inputResolution, keepAspectRatio);\n    var imageTensor = tf.tidy(function () {\n        var $image = image_utils_1.toImageTensor(image);\n        var transformMatrix = tf.tensor2d(image_utils_1.getProjectiveTransformMatrix(roi, imageSize, false, inputResolution), [1, 8]);\n        var imageTransformed = tf.image.transform(\n        // tslint:disable-next-line: no-unnecessary-type-assertion\n        tf.expandDims(tf.cast($image, 'float32')), transformMatrix, 'bilinear', 'nearest', 0, [inputResolution.height, inputResolution.width]);\n        return imageTransformed;\n    });\n    return { imageTensor: imageTensor, padding: padding };\n}\nexports.convertImageToTensor = convertImageToTensor;\n//# sourceMappingURL=convert_image_to_tensor.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\n/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nvar tf = require(\"@tensorflow/tfjs-core\");\nfunction getImageSize(input) {\n    if (input instanceof tf.Tensor) {\n        return { height: input.shape[0], width: input.shape[1] };\n    }\n    else {\n        return { height: input.height, width: input.width };\n    }\n}\nexports.getImageSize = getImageSize;\n/**\n * Normalizes the provided angle to the range -pi to pi.\n * @param angle The angle in radians to be normalized.\n */\nfunction normalizeRadians(angle) {\n    return angle - 2 * Math.PI * Math.floor((angle + Math.PI) / (2 * Math.PI));\n}\nexports.normalizeRadians = normalizeRadians;\n/**\n * Transform value ranges.\n * @param fromMin Min of original value range.\n * @param fromMax Max of original value range.\n * @param toMin New min of transformed value range.\n * @param toMax New max of transformed value range.\n */\nfunction transformValueRange(fromMin, fromMax, toMin, toMax) {\n    var fromRange = fromMax - fromMin;\n    var toRange = toMax - toMin;\n    if (fromRange === 0) {\n        throw new Error(\"Original min and max are both \" + fromMin + \", range cannot be 0.\");\n    }\n    var scale = toRange / fromRange;\n    var offset = toMin - fromMin * scale;\n    return { scale: scale, offset: offset };\n}\nexports.transformValueRange = transformValueRange;\n/**\n * Convert an image to an image tensor representation.\n *\n * The image tensor has a shape [1, height, width, colorChannel].\n *\n * @param input An image, video frame, or image tensor.\n */\nfunction toImageTensor(input) {\n    return input instanceof tf.Tensor ? input : tf.browser.fromPixels(input);\n}\nexports.toImageTensor = toImageTensor;\n/**\n * Padding ratio of left, top, right, bottom, based on the output dimensions.\n *\n * The padding values are non-zero only when the \"keep_aspect_ratio\" is true.\n *\n * For instance, when the input image is 10x10 (width x height) and the\n * output dimensions is 20x40 and \"keep_aspect_ratio\" is true, we should scale\n * the input image to 20x20 and places it in the middle of the output image with\n * an equal padding of 10 pixels at the top and the bottom. The result is\n * therefore {left: 0, top: 0.25, right: 0, bottom: 0.25} (10/40 = 0.25f).\n * @param roi The original rectangle to pad.\n * @param targetSize The target width and height of the result rectangle.\n * @param keepAspectRatio Whether keep aspect ratio. Default to false.\n */\nfunction padRoi(roi, targetSize, keepAspectRatio) {\n    if (keepAspectRatio === void 0) { keepAspectRatio = false; }\n    if (!keepAspectRatio) {\n        return { top: 0, left: 0, right: 0, bottom: 0 };\n    }\n    var targetH = targetSize.height;\n    var targetW = targetSize.width;\n    validateSize(targetSize, 'targetSize');\n    validateSize(roi, 'roi');\n    var tensorAspectRatio = targetH / targetW;\n    var roiAspectRatio = roi.height / roi.width;\n    var newWidth;\n    var newHeight;\n    var horizontalPadding = 0;\n    var verticalPadding = 0;\n    if (tensorAspectRatio > roiAspectRatio) {\n        // pad height;\n        newWidth = roi.width;\n        newHeight = roi.width * tensorAspectRatio;\n        verticalPadding = (1 - roiAspectRatio / tensorAspectRatio) / 2;\n    }\n    else {\n        // pad width.\n        newWidth = roi.height / tensorAspectRatio;\n        newHeight = roi.height;\n        horizontalPadding = (1 - tensorAspectRatio / roiAspectRatio) / 2;\n    }\n    roi.width = newWidth;\n    roi.height = newHeight;\n    return {\n        top: verticalPadding,\n        left: horizontalPadding,\n        right: horizontalPadding,\n        bottom: verticalPadding\n    };\n}\nexports.padRoi = padRoi;\n/**\n * Get the rectangle information of an image, including xCenter, yCenter, width,\n * height and rotation.\n *\n * @param imageSize imageSize is used to calculate the rectangle.\n * @param normRect Optional. If normRect is not null, it will be used to get\n *     a subarea rectangle information in the image. `imageSize` is used to\n *     calculate the actual non-normalized coordinates.\n */\nfunction getRoi(imageSize, normRect) {\n    if (normRect) {\n        return {\n            xCenter: normRect.xCenter * imageSize.width,\n            yCenter: normRect.yCenter * imageSize.height,\n            width: normRect.width * imageSize.width,\n            height: normRect.height * imageSize.height,\n            rotation: normRect.rotation\n        };\n    }\n    else {\n        return {\n            xCenter: 0.5 * imageSize.width,\n            yCenter: 0.5 * imageSize.height,\n            width: imageSize.width,\n            height: imageSize.height,\n            rotation: 0\n        };\n    }\n}\nexports.getRoi = getRoi;\n/**\n * Generate the projective transformation matrix to be used for `tf.transform`.\n *\n * See more documentation in `tf.transform`.\n *\n * @param subRect The rectangle to generate the projective transformation matrix\n *     for.\n * @param imageSize The original image height and width.\n * @param flipHorizontally Whether flip the image horizontally.\n * @param inputResolution The target height and width.\n */\nfunction getProjectiveTransformMatrix(subRect, imageSize, flipHorizontally, inputResolution) {\n    validateSize(inputResolution, 'inputResolution');\n    // Ref:\n    // https://github.com/google/mediapipe/blob/master/mediapipe/calculators/tensor/image_to_tensor_utils.cc\n    // The resulting matrix is multiplication of below matrices:\n    // M = postScaleMatrix * translateMatrix * rotateMatrix * flipMatrix *\n    //     scaleMatrix * initialTranslateMatrix\n    //\n    // For any point in the transformed image p, we can use the above matrix to\n    // calculate the projected point in the original image p'. So that:\n    // p' = p * M;\n    // Note: The transform matrix below assumes image coordinates is normalized\n    // to [0, 1] range.\n    // postScaleMatrix: Matrix to scale x, y to [0, 1] range\n    //   | g  0  0 |\n    //   | 0  h  0 |\n    //   | 0  0  1 |\n    var g = 1 / imageSize.width;\n    var h = 1 / imageSize.height;\n    // translateMatrix: Matrix to move the center to the subRect center.\n    //   | 1  0  e |\n    //   | 0  1  f |\n    //   | 0  0  1 |\n    var e = subRect.xCenter;\n    var f = subRect.yCenter;\n    // rotateMatrix: Matrix to do rotate the image around the subRect center.\n    //   | c -d  0 |\n    //   | d  c  0 |\n    //   | 0  0  1 |\n    var c = Math.cos(subRect.rotation);\n    var d = Math.sin(subRect.rotation);\n    // flipMatrix: Matrix for optional horizontal flip around the subRect center.\n    //   | fl 0  0 |\n    //   | 0  1  0 |\n    //   | 0  0  1 |\n    var flip = flipHorizontally ? -1 : 1;\n    // scaleMatrix: Matrix to scale x, y to subRect size.\n    //   | a  0  0 |\n    //   | 0  b  0 |\n    //   | 0  0  1 |\n    var a = subRect.width;\n    var b = subRect.height;\n    // initialTranslateMatrix: Matrix convert x, y to [-0.5, 0.5] range.\n    //   | 1  0 -0.5 |\n    //   | 0  1 -0.5 |\n    //   | 0  0  1   |\n    // M is a 3 by 3 matrix denoted by:\n    // | a0  a1  a2 |\n    // | b0  b1  b2 |\n    // | 0   0   1  |\n    // To use M with regular x, y coordinates, we need to normalize them first.\n    // Because x' = a0 * x + a1 * y + a2, y' = b0 * x + b1 * y + b2,\n    // we need to use factor (1/inputResolution.width) to normalize x for a0 and\n    // b0, similarly we need to use factor (1/inputResolution.height) to normalize\n    // y for a1 and b1.\n    // Also at the end, we need to de-normalize x' and y' to regular coordinates.\n    // So we need to use factor imageSize.width for a0, a1 and a2, similarly\n    // we need to use factor imageSize.height for b0, b1 and b2.\n    var a0 = (1 / inputResolution.width) * a * c * flip * g * imageSize.width;\n    var a1 = (1 / inputResolution.height) * -b * d * g * imageSize.width;\n    var a2 = (-0.5 * a * c * flip + 0.5 * b * d + e) * g * imageSize.width;\n    var b0 = (1 / inputResolution.width) * a * d * flip * h * imageSize.height;\n    var b1 = (1 / inputResolution.height) * b * c * h * imageSize.height;\n    var b2 = (-0.5 * b * c - 0.5 * a * d * flip + f) * h * imageSize.height;\n    return [a0, a1, a2, b0, b1, b2, 0, 0];\n}\nexports.getProjectiveTransformMatrix = getProjectiveTransformMatrix;\nfunction validateSize(size, name) {\n    tf.util.assert(size.width !== 0, function () { return name + \" width cannot be 0.\"; });\n    tf.util.assert(size.height !== 0, function () { return name + \" height cannot be 0.\"; });\n}\n//# sourceMappingURL=image_utils.js.map","\n/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nObject.defineProperty(exports, \"__esModule\", { value: true });\nfunction isVideo(image) {\n    return (image != null) && image.currentTime != null;\n}\nexports.isVideo = isVideo;\n//# sourceMappingURL=is_video.js.map","\n/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nObject.defineProperty(exports, \"__esModule\", { value: true });\nvar get_object_scale_1 = require(\"./get_object_scale\");\nvar keypoints_one_euro_filter_1 = require(\"./keypoints_one_euro_filter\");\nvar keypoints_to_normalized_keypoints_1 = require(\"./keypoints_to_normalized_keypoints\");\nvar keypoints_velocity_filter_1 = require(\"./keypoints_velocity_filter\");\nvar normalized_keypoints_to_keypoints_1 = require(\"./normalized_keypoints_to_keypoints\");\n/**\n * A Calculator to smooth keypoints over time.\n */\nvar KeypointsSmoothingFilter = /** @class */ (function () {\n    function KeypointsSmoothingFilter(config) {\n        if (config.velocityFilter != null) {\n            this.keypointsFilter = new keypoints_velocity_filter_1.KeypointsVelocityFilter(config.velocityFilter);\n        }\n        else if (config.oneEuroFilter != null) {\n            this.keypointsFilter = new keypoints_one_euro_filter_1.KeypointsOneEuroFilter(config.oneEuroFilter);\n        }\n        else {\n            throw new Error('Either configure velocityFilter or oneEuroFilter, but got ' +\n                (config + \".\"));\n        }\n    }\n    /**\n     * Apply one of the stateful `KeypointsFilter` to keypoints.\n     *\n     * Currently supports `OneEuroFilter` and `VelocityFilter`.\n     * @param keypoints A list of 2D or 3D keypoints, can be normalized or\n     *     non-normalized.\n     * @param timestamp The timestamp of the video frame.\n     * @param imageSize Optional. The imageSize is useful when keypoints are\n     *     normalized.\n     * @param normalized Optional. Whether the keypoints are normalized. Default\n     *     to false.\n     * @param objectScaleROI Optional. The auxiliary ROI to calculate object\n     *     scale. If not set, objectScale defaults to 1.\n     */\n    KeypointsSmoothingFilter.prototype.apply = function (keypoints, timestamp, imageSize, normalized, objectScaleROI) {\n        if (normalized === void 0) { normalized = false; }\n        if (keypoints == null) {\n            this.keypointsFilter.reset();\n            return null;\n        }\n        var objectScale = objectScaleROI != null ? get_object_scale_1.getObjectScale(objectScaleROI, imageSize) : 1;\n        var scaledKeypoints = normalized ?\n            normalized_keypoints_to_keypoints_1.normalizedKeypointsToKeypoints(keypoints, imageSize) :\n            keypoints;\n        var scaledKeypointsFiltered = this.keypointsFilter.apply(scaledKeypoints, timestamp, objectScale);\n        return normalized ?\n            keypoints_to_normalized_keypoints_1.keypointsToNormalizedKeypoints(scaledKeypointsFiltered, imageSize) :\n            scaledKeypointsFiltered;\n    };\n    return KeypointsSmoothingFilter;\n}());\nexports.KeypointsSmoothingFilter = KeypointsSmoothingFilter;\n//# sourceMappingURL=keypoints_smoothing.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\n/**\n * Estimate object scale to allow filter work similarly on nearer or futher\n * objects.\n * @param roi Normalized rectangle.\n * @param imageSize An object that contains width and height.\n * @returns A number representing the object scale.\n */\nfunction getObjectScale(roi, imageSize) {\n    var objectWidth = roi.width * imageSize.width;\n    var objectHeight = roi.height * imageSize.height;\n    return (objectWidth + objectHeight) / 2;\n}\nexports.getObjectScale = getObjectScale;\n//# sourceMappingURL=get_object_scale.js.map","\n/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nvar __assign = (this && this.__assign) || function () {\n    __assign = Object.assign || function(t) {\n        for (var s, i = 1, n = arguments.length; i < n; i++) {\n            s = arguments[i];\n            for (var p in s) if (Object.prototype.hasOwnProperty.call(s, p))\n                t[p] = s[p];\n        }\n        return t;\n    };\n    return __assign.apply(this, arguments);\n};\nObject.defineProperty(exports, \"__esModule\", { value: true });\nvar one_euro_filter_1 = require(\"./one_euro_filter\");\n/**\n * A stateful filter that smoothes keypoints values overtime.\n *\n * More specifically, it uses `OneEuroFilter` to smooth every x, y, z\n * coordinates over time, which as result gives us velocity of how these values\n * change over time. With higher velocity it weights new values higher.\n */\n// ref:\n// https://github.com/google/mediapipe/blob/master/mediapipe/calculators/util/landmarks_smoothing_calculator.cc\nvar KeypointsOneEuroFilter = /** @class */ (function () {\n    function KeypointsOneEuroFilter(config) {\n        this.config = config;\n    }\n    KeypointsOneEuroFilter.prototype.apply = function (keypoints, microSeconds, objectScale) {\n        var _this = this;\n        if (keypoints == null) {\n            this.reset();\n            return null;\n        }\n        // Initialize filters once.\n        this.initializeFiltersIfEmpty(keypoints);\n        // Get value scale as inverse value of the object scale.\n        // If value is too small smoothing will be disabled and keypoints will be\n        // returned as is.\n        var valueScale = 1;\n        if (this.config.minAllowedObjectScale != null) {\n            if (objectScale < this.config.minAllowedObjectScale) {\n                return keypoints.slice();\n            }\n            valueScale = 1.0 / objectScale;\n        }\n        // Filter keypoints. Every axis of every keypoint is filtered separately.\n        return keypoints.map(function (keypoint, i) {\n            var outKeypoint = __assign({}, keypoint, { x: _this.xFilters[i].apply(keypoint.x, microSeconds, valueScale), y: _this.yFilters[i].apply(keypoint.y, microSeconds, valueScale) });\n            if (keypoint.z != null) {\n                outKeypoint.z =\n                    _this.zFilters[i].apply(keypoint.z, microSeconds, valueScale);\n            }\n            return outKeypoint;\n        });\n    };\n    KeypointsOneEuroFilter.prototype.reset = function () {\n        this.xFilters = null;\n        this.yFilters = null;\n        this.zFilters = null;\n    };\n    // Initializes filters for the first time or after reset. If initialized the\n    // check the size.\n    KeypointsOneEuroFilter.prototype.initializeFiltersIfEmpty = function (keypoints) {\n        var _this = this;\n        if (this.xFilters == null || this.xFilters.length !== keypoints.length) {\n            this.xFilters = keypoints.map(function (_) { return new one_euro_filter_1.OneEuroFilter(_this.config); });\n            this.yFilters = keypoints.map(function (_) { return new one_euro_filter_1.OneEuroFilter(_this.config); });\n            this.zFilters = keypoints.map(function (_) { return new one_euro_filter_1.OneEuroFilter(_this.config); });\n        }\n    };\n    return KeypointsOneEuroFilter;\n}());\nexports.KeypointsOneEuroFilter = KeypointsOneEuroFilter;\n//# sourceMappingURL=keypoints_one_euro_filter.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\n/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nvar constants_1 = require(\"./constants\");\nvar low_pass_filter_1 = require(\"./low_pass_filter\");\n/**\n * OneEuroFilter.\n */\n// ref:\n// https://github.com/google/mediapipe/blob/master/mediapipe/util/filtering/one_euro_filter.cc\n// Also ref original paper:\n// https://cristal.univ-lille.fr/~casiez/1euro/\nvar OneEuroFilter = /** @class */ (function () {\n    /**\n     * Constructor of `OneEuroFilter` class.\n     * @param config See documentation of `OneEuroFilterConfig`.\n     */\n    function OneEuroFilter(config) {\n        this.frequency = config.frequency;\n        this.minCutOff = config.minCutOff;\n        this.beta = config.beta;\n        this.thresholdCutOff = config.thresholdCutOff;\n        this.thresholdBeta = config.thresholdBeta;\n        this.derivateCutOff = config.derivateCutOff;\n        this.x = new low_pass_filter_1.LowPassFilter(this.getAlpha(this.minCutOff));\n        this.dx = new low_pass_filter_1.LowPassFilter(this.getAlpha(this.derivateCutOff));\n        this.lastTimestamp = 0;\n    }\n    /**\n     * Applies filter to the value.\n     * @param value valueToFilter.\n     * @param microSeconds timestamp associated with the value (for instance,\n     *     timestamp of the frame where you got value from).\n     */\n    OneEuroFilter.prototype.apply = function (value, microSeconds, valueScale) {\n        if (value == null) {\n            return value;\n        }\n        var $microSeconds = Math.trunc(microSeconds);\n        if (this.lastTimestamp >= $microSeconds) {\n            // Results are unpreditable in this case, so nothing to do but return\n            // same value.\n            return value;\n        }\n        // Update the sampling frequency based on timestamps.\n        if (this.lastTimestamp !== 0 && $microSeconds !== 0) {\n            this.frequency =\n                1 / (($microSeconds - this.lastTimestamp) * constants_1.MICRO_SECONDS_TO_SECOND);\n        }\n        this.lastTimestamp = $microSeconds;\n        // Estimate the current variation per second.\n        var dValue = this.x.hasLastRawValue() ?\n            (value - this.x.lastRawValue()) * valueScale * this.frequency :\n            0;\n        var edValue = this.dx.applyWithAlpha(dValue, this.getAlpha(this.derivateCutOff));\n        var cutOff = this.minCutOff + this.beta * Math.abs(edValue);\n        var threshold = this.thresholdCutOff != null ?\n            this.thresholdCutOff + this.thresholdBeta * Math.abs(edValue) :\n            null;\n        // filter the given value.\n        return this.x.applyWithAlpha(value, this.getAlpha(cutOff), threshold);\n    };\n    OneEuroFilter.prototype.getAlpha = function (cutoff) {\n        // te = 1.0 / this.frequency\n        // tau = 1.0 / (2 * Math.PI * cutoff)\n        // result = 1 / (1.0 + (tau / te))\n        return 1.0 / (1.0 + (this.frequency / (2 * Math.PI * cutoff)));\n    };\n    return OneEuroFilter;\n}());\nexports.OneEuroFilter = OneEuroFilter;\n//# sourceMappingURL=one_euro_filter.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\n/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n/**\n * A stateful filter that smoothes values overtime.\n *\n * More specifically, it stores the previous value, and when there's a new\n * value, a coefficient 'alpha' is applied to the new value, and `1 - alpha` is\n * applied to the previous value. The smaller the alpha is, the smoother result\n * and the bigger lag.\n */\n// ref:\n// https://github.com/google/mediapipe/blob/master/mediapipe/util/filtering/low_pass_filter.cc\nvar LowPassFilter = /** @class */ (function () {\n    function LowPassFilter(alpha) {\n        this.alpha = alpha;\n        this.initialized = false;\n    }\n    LowPassFilter.prototype.apply = function (value, threshold) {\n        var result;\n        if (this.initialized) {\n            if (threshold == null) {\n                // Regular lowpass filter.\n                // result = this.alpha * value + (1 - this.alpha) * this.storedValue;\n                result = this.storedValue + this.alpha * (value - this.storedValue);\n            }\n            else {\n                // We need to reformat the formula to be able to conveniently apply\n                // another optional non-linear function to the\n                // (value - this.storedValue) part.\n                // Add additional non-linearity to cap extreme value.\n                // More specifically, assume x = (value - this.storedValue), when x is\n                // close zero, the derived x is close to x, when x is several magnitudes\n                // larger, the drived x grows much slower then x. It behaves like\n                // sign(x)log(abs(x)).\n                result = this.storedValue +\n                    this.alpha * threshold *\n                        Math.asinh((value - this.storedValue) / threshold);\n            }\n        }\n        else {\n            result = value;\n            this.initialized = true;\n        }\n        this.rawValue = value;\n        this.storedValue = result;\n        return result;\n    };\n    LowPassFilter.prototype.applyWithAlpha = function (value, alpha, threshold) {\n        this.alpha = alpha;\n        return this.apply(value, threshold);\n    };\n    LowPassFilter.prototype.hasLastRawValue = function () {\n        return this.initialized;\n    };\n    LowPassFilter.prototype.lastRawValue = function () {\n        return this.rawValue;\n    };\n    LowPassFilter.prototype.reset = function () {\n        this.initialized = false;\n    };\n    return LowPassFilter;\n}());\nexports.LowPassFilter = LowPassFilter;\n//# sourceMappingURL=low_pass_filter.js.map","\nvar __assign = (this && this.__assign) || function () {\n    __assign = Object.assign || function(t) {\n        for (var s, i = 1, n = arguments.length; i < n; i++) {\n            s = arguments[i];\n            for (var p in s) if (Object.prototype.hasOwnProperty.call(s, p))\n                t[p] = s[p];\n        }\n        return t;\n    };\n    return __assign.apply(this, arguments);\n};\nObject.defineProperty(exports, \"__esModule\", { value: true });\nfunction keypointsToNormalizedKeypoints(keypoints, imageSize) {\n    return keypoints.map(function (keypoint) {\n        var normalizedKeypoint = __assign({}, keypoint, { x: keypoint.x / imageSize.width, y: keypoint.y / imageSize.height });\n        if (keypoint.z != null) {\n            // Scale z the same way as x (using image width).\n            keypoint.z = keypoint.z / imageSize.width;\n        }\n        return normalizedKeypoint;\n    });\n}\nexports.keypointsToNormalizedKeypoints = keypointsToNormalizedKeypoints;\n//# sourceMappingURL=keypoints_to_normalized_keypoints.js.map","\nvar __assign = (this && this.__assign) || function () {\n    __assign = Object.assign || function(t) {\n        for (var s, i = 1, n = arguments.length; i < n; i++) {\n            s = arguments[i];\n            for (var p in s) if (Object.prototype.hasOwnProperty.call(s, p))\n                t[p] = s[p];\n        }\n        return t;\n    };\n    return __assign.apply(this, arguments);\n};\nObject.defineProperty(exports, \"__esModule\", { value: true });\nvar relative_velocity_filter_1 = require(\"./relative_velocity_filter\");\n/**\n * A stateful filter that smoothes landmark values overtime.\n *\n * More specifically, it uses `RelativeVelocityFilter` to smooth every x, y, z\n * coordinates over time, which as result gives us velocity of how these values\n * change over time. With higher velocity it weights new values higher.\n */\n// ref:\n// https://github.com/google/mediapipe/blob/master/mediapipe/calculators/util/landmarks_smoothing_calculator.cc\nvar KeypointsVelocityFilter = /** @class */ (function () {\n    function KeypointsVelocityFilter(config) {\n        this.config = config;\n    }\n    KeypointsVelocityFilter.prototype.apply = function (keypoints, microSeconds, objectScale) {\n        var _this = this;\n        if (keypoints == null) {\n            this.reset();\n            return null;\n        }\n        // Get value scale as inverse value of the object scale.\n        // If value is too small smoothing will be disabled and keypoints will be\n        // returned as is.\n        var valueScale = 1;\n        if (!this.config.disableValueScaling) {\n            if (objectScale < this.config.minAllowedObjectScale) {\n                return keypoints.slice();\n            }\n            valueScale = 1 / objectScale;\n        }\n        // Initialize filters once.\n        this.initializeFiltersIfEmpty(keypoints);\n        // Filter keypoints. Every axis of every keypoint is filtered separately.\n        return keypoints.map(function (keypoint, i) {\n            var outKeypoint = __assign({}, keypoint, { x: _this.xFilters[i].apply(keypoint.x, microSeconds, valueScale), y: _this.yFilters[i].apply(keypoint.y, microSeconds, valueScale) });\n            if (keypoint.z != null) {\n                outKeypoint.z =\n                    _this.zFilters[i].apply(keypoint.z, microSeconds, valueScale);\n            }\n            return outKeypoint;\n        });\n    };\n    KeypointsVelocityFilter.prototype.reset = function () {\n        this.xFilters = null;\n        this.yFilters = null;\n        this.zFilters = null;\n    };\n    // Initializes filters for the first time or after reset. If initialized the\n    // check the size.\n    KeypointsVelocityFilter.prototype.initializeFiltersIfEmpty = function (keypoints) {\n        var _this = this;\n        if (this.xFilters == null || this.xFilters.length !== keypoints.length) {\n            this.xFilters =\n                keypoints.map(function (_) { return new relative_velocity_filter_1.RelativeVelocityFilter(_this.config); });\n            this.yFilters =\n                keypoints.map(function (_) { return new relative_velocity_filter_1.RelativeVelocityFilter(_this.config); });\n            this.zFilters =\n                keypoints.map(function (_) { return new relative_velocity_filter_1.RelativeVelocityFilter(_this.config); });\n        }\n    };\n    return KeypointsVelocityFilter;\n}());\nexports.KeypointsVelocityFilter = KeypointsVelocityFilter;\n//# sourceMappingURL=keypoints_velocity_filter.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\n/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nvar constants_1 = require(\"./constants\");\nvar low_pass_filter_1 = require(\"./low_pass_filter\");\n/**\n * This filter keeps track (on a window of specified size) of value changes\n * over time, which as result gives us velocity of how value changes over time.\n * With higher velocity it weights new values higher.\n *\n * Use `windowSize` and `velocityScale` to tweak this filter for your use case.\n */\n// ref:\n// https://github.com/google/mediapipe/blob/master/mediapipe/util/filtering/relative_velocity_filter.cc\nvar RelativeVelocityFilter = /** @class */ (function () {\n    /**\n     * Constructor of `RelativeVelocityFilter` class.\n     * @param config\n     *        `windowSize`:  Higher windowSize adds to lag and to stability.\n     *        `velocityScale`: Lower velocityScale adds to lag and to stability.\n     */\n    function RelativeVelocityFilter(config) {\n        this.config = config;\n        this.window = [];\n        this.lowPassFilter = new low_pass_filter_1.LowPassFilter(1.0);\n        this.lastValue = 0;\n        this.lastValueScale = 1;\n        this.lastTimestamp = -1;\n    }\n    /**\n     * Applies filter to the value.\n     * @param value valueToFilter.\n     * @param microSeconds timestamp associated with the value (for instance,\n     *     timestamp of the frame where you got value from).\n     * @param valueScale value scale (for instance, if your value is a distance\n     *     detected on a frame, it can look same on different devices but have\n     *     quite different absolute values due to different resolution, you\n     *     should come up with an appropriate parameter for your particular use\n     *     case).\n     */\n    RelativeVelocityFilter.prototype.apply = function (value, microSeconds, valueScale) {\n        if (value == null) {\n            return value;\n        }\n        var $microSeconds = Math.trunc(microSeconds);\n        if (this.lastTimestamp >= $microSeconds) {\n            // Results are unpreditable in this case, so nothing to do but return\n            // same value.\n            return value;\n        }\n        var alpha;\n        if (this.lastTimestamp === -1) {\n            alpha = 1;\n        }\n        else {\n            // Implement the DistanceEstimationMode.kLegacyTransition.\n            // TODO(lina128): Change to kForceCurrentScale or at least add an option\n            // that can be tweaked with parameter.\n            var distance = value * valueScale - this.lastValue * this.lastValueScale;\n            var duration = $microSeconds - this.lastTimestamp;\n            var cumulativeDistance = distance;\n            var cumulativeDuration = duration;\n            // Define max cumulative duration assuming 30 frames per second is a good\n            // frame rate, so assuming 30 values per second or 1 / 30 of a second is\n            // a good duration per window element.\n            var assumedMaxDuration = constants_1.SECOND_TO_MICRO_SECONDS / 30;\n            var maxCumulativeDuration = (1 + this.window.length) * assumedMaxDuration;\n            for (var _i = 0, _a = this.window; _i < _a.length; _i++) {\n                var el = _a[_i];\n                if (cumulativeDuration + el.duration > maxCumulativeDuration) {\n                    // This helps in cases when durations are large and outdated\n                    // window elements have bad impact on filtering results.\n                    break;\n                }\n                cumulativeDistance += el.distance;\n                cumulativeDuration += el.duration;\n            }\n            var velocity = cumulativeDistance / (cumulativeDuration * constants_1.MICRO_SECONDS_TO_SECOND);\n            alpha = 1 - 1 / (1 + this.config.velocityScale * Math.abs(velocity));\n            this.window.unshift({ distance: distance, duration: duration });\n            if (this.window.length > this.config.windowSize) {\n                this.window.pop();\n            }\n        }\n        this.lastValue = value;\n        this.lastValueScale = valueScale;\n        this.lastTimestamp = $microSeconds;\n        return this.lowPassFilter.applyWithAlpha(value, alpha);\n    };\n    return RelativeVelocityFilter;\n}());\nexports.RelativeVelocityFilter = RelativeVelocityFilter;\n//# sourceMappingURL=relative_velocity_filter.js.map","\nvar __assign = (this && this.__assign) || function () {\n    __assign = Object.assign || function(t) {\n        for (var s, i = 1, n = arguments.length; i < n; i++) {\n            s = arguments[i];\n            for (var p in s) if (Object.prototype.hasOwnProperty.call(s, p))\n                t[p] = s[p];\n        }\n        return t;\n    };\n    return __assign.apply(this, arguments);\n};\nObject.defineProperty(exports, \"__esModule\", { value: true });\nfunction normalizedKeypointsToKeypoints(normalizedKeypoints, imageSize) {\n    return normalizedKeypoints.map(function (normalizedKeypoint) {\n        var keypoint = __assign({}, normalizedKeypoint, { x: normalizedKeypoint.x * imageSize.width, y: normalizedKeypoint.y * imageSize.height });\n        if (normalizedKeypoint.z != null) {\n            // Scale z the same way as x (using image width).\n            keypoint.z = normalizedKeypoint.z * imageSize.width;\n        }\n        return keypoint;\n    });\n}\nexports.normalizedKeypointsToKeypoints = normalizedKeypointsToKeypoints;\n//# sourceMappingURL=normalized_keypoints_to_keypoints.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\n/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nvar tf = require(\"@tensorflow/tfjs-core\");\nvar image_utils_1 = require(\"./image_utils\");\nfunction shiftImageValue(image, outputFloatRange) {\n    // Calculate the scale and offset to shift from [0, 255] to [-1, 1].\n    var valueRange = image_utils_1.transformValueRange(0, 255, outputFloatRange[0] /* min */, outputFloatRange[1] /* max */);\n    // Shift value range.\n    return tf.tidy(function () { return tf.add(tf.mul(image, valueRange.scale), valueRange.offset); });\n}\nexports.shiftImageValue = shiftImageValue;\n//# sourceMappingURL=shift_image_value.js.map","\n/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nObject.defineProperty(exports, \"__esModule\", { value: true });\nvar detection_to_rect_1 = require(\"./detection_to_rect\");\n// ref:\n// https://github.com/google/mediapipe/blob/master/mediapipe/calculators/util/alignment_points_to_rects_calculator.cc\nfunction calculateAlignmentPointsRects(detection, imageSize, config) {\n    var startKeypoint = config.rotationVectorStartKeypointIndex;\n    var endKeypoint = config.rotationVectorEndKeypointIndex;\n    var locationData = detection.locationData;\n    var xCenter = locationData.relativeKeypoints[startKeypoint].x * imageSize.width;\n    var yCenter = locationData.relativeKeypoints[startKeypoint].y * imageSize.height;\n    var xScale = locationData.relativeKeypoints[endKeypoint].x * imageSize.width;\n    var yScale = locationData.relativeKeypoints[endKeypoint].y * imageSize.height;\n    // Bounding box size as double distance from center to scale point.\n    var boxSize = Math.sqrt((xScale - xCenter) * (xScale - xCenter) +\n        (yScale - yCenter) * (yScale - yCenter)) *\n        2;\n    var rotation = detection_to_rect_1.computeRotation(detection, imageSize, config);\n    // Set resulting bounding box.\n    return {\n        xCenter: xCenter / imageSize.width,\n        yCenter: yCenter / imageSize.height,\n        width: boxSize / imageSize.width,\n        height: boxSize / imageSize.height,\n        rotation: rotation\n    };\n}\nexports.calculateAlignmentPointsRects = calculateAlignmentPointsRects;\n//# sourceMappingURL=calculate_alignment_points_rects.js.map","\n/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nObject.defineProperty(exports, \"__esModule\", { value: true });\nvar image_utils_1 = require(\"../../calculators/image_utils\");\n// ref:\n// https://github.com/google/mediapipe/blob/master/mediapipe/calculators/util/detections_to_rects_calculator.cc\nfunction computeRotation(detection, imageSize, config) {\n    var locationData = detection.locationData;\n    var startKeypoint = config.rotationVectorStartKeypointIndex;\n    var endKeypoint = config.rotationVectorEndKeypointIndex;\n    var targetAngle;\n    if (config.rotationVectorTargetAngle) {\n        targetAngle = config.rotationVectorTargetAngle;\n    }\n    else {\n        targetAngle = Math.PI * config.rotationVectorTargetAngleDegree / 180;\n    }\n    var x0 = locationData.relativeKeypoints[startKeypoint].x * imageSize.width;\n    var y0 = locationData.relativeKeypoints[startKeypoint].y * imageSize.height;\n    var x1 = locationData.relativeKeypoints[endKeypoint].x * imageSize.width;\n    var y1 = locationData.relativeKeypoints[endKeypoint].y * imageSize.height;\n    var rotation = image_utils_1.normalizeRadians(targetAngle - Math.atan2(-(y1 - y0), x1 - x0));\n    return rotation;\n}\nexports.computeRotation = computeRotation;\n//# sourceMappingURL=detection_to_rect.js.map","\n/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nvar __assign = (this && this.__assign) || function () {\n    __assign = Object.assign || function(t) {\n        for (var s, i = 1, n = arguments.length; i < n; i++) {\n            s = arguments[i];\n            for (var p in s) if (Object.prototype.hasOwnProperty.call(s, p))\n                t[p] = s[p];\n        }\n        return t;\n    };\n    return __assign.apply(this, arguments);\n};\nObject.defineProperty(exports, \"__esModule\", { value: true });\n/**\n * Projects normalized landmarks in a rectangle to its original coordinates. The\n * rectangle must also be in normalized coordinates.\n * @param landmarks A normalized Landmark list representing landmarks in a\n *     normalized rectangle.\n * @param inputRect A normalized rectangle.\n * @param config Config object has one field ignoreRotation, default to false.\n */\n// ref:\n// https://github.com/google/mediapipe/blob/master/mediapipe/calculators/util/landmark_projection_calculator.cc\nfunction calculateLandmarkProjection(landmarks, inputRect, config) {\n    if (config === void 0) { config = {\n        ignoreRotation: false\n    }; }\n    var outputLandmarks = [];\n    for (var _i = 0, landmarks_1 = landmarks; _i < landmarks_1.length; _i++) {\n        var landmark = landmarks_1[_i];\n        var x = landmark.x - 0.5;\n        var y = landmark.y - 0.5;\n        var angle = config.ignoreRotation ? 0 : inputRect.rotation;\n        var newX = Math.cos(angle) * x - Math.sin(angle) * y;\n        var newY = Math.sin(angle) * x + Math.cos(angle) * y;\n        newX = newX * inputRect.width + inputRect.xCenter;\n        newY = newY * inputRect.height + inputRect.yCenter;\n        var newZ = landmark.z * inputRect.width; // Scale Z coordinate as x.\n        var newLandmark = __assign({}, landmark);\n        newLandmark.x = newX;\n        newLandmark.y = newY;\n        newLandmark.z = newZ;\n        outputLandmarks.push(newLandmark);\n    }\n    return outputLandmarks;\n}\nexports.calculateLandmarkProjection = calculateLandmarkProjection;\n//# sourceMappingURL=calculate_landmark_projection.js.map","\n/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nObject.defineProperty(exports, \"__esModule\", { value: true });\n// ref:\n// https://github.com/google/mediapipe/blob/350fbb2100ad531bc110b93aaea23d96af5a5064/mediapipe/calculators/tflite/ssd_anchors_calculator.cc\nfunction createSsdAnchors(config) {\n    var anchors = [];\n    var layerId = 0;\n    while (layerId < config.numLayers) {\n        var anchorHeight = [];\n        var anchorWidth = [];\n        var aspectRatios = [];\n        var scales = [];\n        // For same strides, we merge the anchors in the same order.\n        var lastSameStrideLayer = layerId;\n        while (lastSameStrideLayer < config.strides.length &&\n            config.strides[lastSameStrideLayer] === config.strides[layerId]) {\n            var scale = calculateScale(config.minScale, config.maxScale, lastSameStrideLayer, config.strides.length);\n            if (lastSameStrideLayer === 0 && config.reduceBoxesInLowestLayer) {\n                // For first layer, it can be specified to use predefined anchors.\n                aspectRatios.push(1);\n                aspectRatios.push(2);\n                aspectRatios.push(0.5);\n                scales.push(0.1);\n                scales.push(scale);\n                scales.push(scale);\n            }\n            else {\n                for (var aspectRatioId = 0; aspectRatioId < config.aspectRatios.length; ++aspectRatioId) {\n                    aspectRatios.push(config.aspectRatios[aspectRatioId]);\n                    scales.push(scale);\n                }\n                if (config.interpolatedScaleAspectRatio > 0.0) {\n                    var scaleNext = lastSameStrideLayer === config.strides.length - 1 ?\n                        1.0 :\n                        calculateScale(config.minScale, config.maxScale, lastSameStrideLayer + 1, config.strides.length);\n                    scales.push(Math.sqrt(scale * scaleNext));\n                    aspectRatios.push(config.interpolatedScaleAspectRatio);\n                }\n            }\n            lastSameStrideLayer++;\n        }\n        for (var i = 0; i < aspectRatios.length; ++i) {\n            var ratioSqrts = Math.sqrt(aspectRatios[i]);\n            anchorHeight.push(scales[i] / ratioSqrts);\n            anchorWidth.push(scales[i] * ratioSqrts);\n        }\n        var featureMapHeight = 0;\n        var featureMapWidth = 0;\n        if (config.featureMapHeight.length > 0) {\n            featureMapHeight = config.featureMapHeight[layerId];\n            featureMapWidth = config.featureMapWidth[layerId];\n        }\n        else {\n            var stride = config.strides[layerId];\n            featureMapHeight = Math.ceil(config.inputSizeHeight / stride);\n            featureMapWidth = Math.ceil(config.inputSizeWidth / stride);\n        }\n        for (var y = 0; y < featureMapHeight; ++y) {\n            for (var x = 0; x < featureMapWidth; ++x) {\n                for (var anchorId = 0; anchorId < anchorHeight.length; ++anchorId) {\n                    var xCenter = (x + config.anchorOffsetX) / featureMapWidth;\n                    var yCenter = (y + config.anchorOffsetY) / featureMapHeight;\n                    var newAnchor = { xCenter: xCenter, yCenter: yCenter, width: 0, height: 0 };\n                    if (config.fixedAnchorSize) {\n                        newAnchor.width = 1.0;\n                        newAnchor.height = 1.0;\n                    }\n                    else {\n                        newAnchor.width = anchorWidth[anchorId];\n                        newAnchor.height = anchorHeight[anchorId];\n                    }\n                    anchors.push(newAnchor);\n                }\n            }\n        }\n        layerId = lastSameStrideLayer;\n    }\n    return anchors;\n}\nexports.createSsdAnchors = createSsdAnchors;\nfunction calculateScale(minScale, maxScale, strideIndex, numStrides) {\n    if (numStrides === 1) {\n        return (minScale + maxScale) * 0.5;\n    }\n    else {\n        return minScale + (maxScale - minScale) * strideIndex / (numStrides - 1);\n    }\n}\n//# sourceMappingURL=create_ssd_anchors.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nvar tf = require(\"@tensorflow/tfjs-core\");\nvar split_detection_result_1 = require(\"./split_detection_result\");\nfunction detectorInference(imageTensor, poseDetectorModel) {\n    return tf.tidy(function () {\n        var detectionResult = poseDetectorModel.predict(imageTensor);\n        var _a = split_detection_result_1.splitDetectionResult(detectionResult), scores = _a[0], rawBoxes = _a[1];\n        // Shape [896, 12]\n        var rawBoxes2d = tf.squeeze(rawBoxes);\n        // Shape [896]\n        var scores1d = tf.squeeze(scores);\n        return { boxes: rawBoxes2d, scores: scores1d };\n    });\n}\nexports.detectorInference = detectorInference;\n//# sourceMappingURL=detector_inference.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\n/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nvar tf = require(\"@tensorflow/tfjs-core\");\nfunction splitDetectionResult(detectionResult) {\n    return tf.tidy(function () {\n        // Score is stored in the first element in each anchor data.\n        var logits = tf.slice(detectionResult, [0, 0, 0], [1, -1, 1]);\n        var scores = tf.sigmoid(logits);\n        // Bounding box coords are stored in the next four elements for each anchor\n        // point.\n        var rawBoxes = tf.slice(detectionResult, [0, 0, 1], [1, -1, -1]);\n        return [scores, rawBoxes];\n    });\n}\nexports.splitDetectionResult = splitDetectionResult;\n//# sourceMappingURL=split_detection_result.js.map","\n/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nObject.defineProperty(exports, \"__esModule\", { value: true });\n/**\n * Converts normalized Landmark to `Detection`. A relative bounding box will\n * be created containing all landmarks exactly.\n * @param landmarks List of normalized landmarks.\n *\n * @returns A `Detection`.\n */\n// ref:\n// https://github.com/google/mediapipe/blob/master/mediapipe/calculators/util/landmarks_to_detection_calculator.cc\nfunction landmarksToDetection(landmarks) {\n    var detection = { locationData: { relativeKeypoints: [] } };\n    var xMin = Number.MAX_SAFE_INTEGER;\n    var xMax = Number.MIN_SAFE_INTEGER;\n    var yMin = Number.MAX_SAFE_INTEGER;\n    var yMax = Number.MIN_SAFE_INTEGER;\n    for (var i = 0; i < landmarks.length; ++i) {\n        var landmark = landmarks[i];\n        xMin = Math.min(xMin, landmark.x);\n        xMax = Math.max(xMax, landmark.x);\n        yMin = Math.min(yMin, landmark.y);\n        yMax = Math.max(yMax, landmark.y);\n        detection.locationData.relativeKeypoints.push({ x: landmark.x, y: landmark.y });\n    }\n    detection.locationData.relativeBoundingBox =\n        { xMin: xMin, yMin: yMin, xMax: xMax, yMax: yMax, width: (xMax - xMin), height: (yMax - yMin) };\n    return detection;\n}\nexports.landmarksToDetection = landmarksToDetection;\n//# sourceMappingURL=landmarks_to_detection.js.map","\nvar __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {\n    return new (P || (P = Promise))(function (resolve, reject) {\n        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }\n        function rejected(value) { try { step(generator[\"throw\"](value)); } catch (e) { reject(e); } }\n        function step(result) { result.done ? resolve(result.value) : new P(function (resolve) { resolve(result.value); }).then(fulfilled, rejected); }\n        step((generator = generator.apply(thisArg, _arguments || [])).next());\n    });\n};\nvar __generator = (this && this.__generator) || function (thisArg, body) {\n    var _ = { label: 0, sent: function() { if (t[0] & 1) throw t[1]; return t[1]; }, trys: [], ops: [] }, f, y, t, g;\n    return g = { next: verb(0), \"throw\": verb(1), \"return\": verb(2) }, typeof Symbol === \"function\" && (g[Symbol.iterator] = function() { return this; }), g;\n    function verb(n) { return function (v) { return step([n, v]); }; }\n    function step(op) {\n        if (f) throw new TypeError(\"Generator is already executing.\");\n        while (_) try {\n            if (f = 1, y && (t = op[0] & 2 ? y[\"return\"] : op[0] ? y[\"throw\"] || ((t = y[\"return\"]) && t.call(y), 0) : y.next) && !(t = t.call(y, op[1])).done) return t;\n            if (y = 0, t) op = [op[0] & 2, t.value];\n            switch (op[0]) {\n                case 0: case 1: t = op; break;\n                case 4: _.label++; return { value: op[1], done: false };\n                case 5: _.label++; y = op[1]; op = [0]; continue;\n                case 7: op = _.ops.pop(); _.trys.pop(); continue;\n                default:\n                    if (!(t = _.trys, t = t.length > 0 && t[t.length - 1]) && (op[0] === 6 || op[0] === 2)) { _ = 0; continue; }\n                    if (op[0] === 3 && (!t || (op[1] > t[0] && op[1] < t[3]))) { _.label = op[1]; break; }\n                    if (op[0] === 6 && _.label < t[1]) { _.label = t[1]; t = op; break; }\n                    if (t && _.label < t[2]) { _.label = t[2]; _.ops.push(op); break; }\n                    if (t[2]) _.ops.pop();\n                    _.trys.pop(); continue;\n            }\n            op = body.call(thisArg, _);\n        } catch (e) { op = [6, e]; y = 0; } finally { f = t = 0; }\n        if (op[0] & 5) throw op[1]; return { value: op[0] ? op[1] : void 0, done: true };\n    }\n};\nObject.defineProperty(exports, \"__esModule\", { value: true });\n/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nvar tf = require(\"@tensorflow/tfjs-core\");\nfunction nonMaxSuppression(detections, maxPoses, iouThreshold, scoreThreshold) {\n    return __awaiter(this, void 0, void 0, function () {\n        var detectionsTensor, scoresTensor, selectedIdsTensor, selectedIds, selectedDetections;\n        return __generator(this, function (_a) {\n            switch (_a.label) {\n                case 0:\n                    detectionsTensor = tf.tensor2d(detections.map(function (d) {\n                        return [d.locationData.relativeBoundingBox.yMin,\n                            d.locationData.relativeBoundingBox.xMin,\n                            d.locationData.relativeBoundingBox.yMax,\n                            d.locationData.relativeBoundingBox.xMax];\n                    }));\n                    scoresTensor = tf.tensor1d(detections.map(function (d) { return d.score[0]; }));\n                    return [4 /*yield*/, tf.image.nonMaxSuppressionAsync(detectionsTensor, scoresTensor, maxPoses, iouThreshold, scoreThreshold)];\n                case 1:\n                    selectedIdsTensor = _a.sent();\n                    return [4 /*yield*/, selectedIdsTensor.array()];\n                case 2:\n                    selectedIds = _a.sent();\n                    selectedDetections = detections.filter(function (_, i) { return (selectedIds.indexOf(i) > -1); });\n                    tf.dispose([detectionsTensor, scoresTensor, selectedIdsTensor]);\n                    return [2 /*return*/, selectedDetections];\n            }\n        });\n    });\n}\nexports.nonMaxSuppression = nonMaxSuppression;\n//# sourceMappingURL=non_max_suppression.js.map","\nvar __assign = (this && this.__assign) || function () {\n    __assign = Object.assign || function(t) {\n        for (var s, i = 1, n = arguments.length; i < n; i++) {\n            s = arguments[i];\n            for (var p in s) if (Object.prototype.hasOwnProperty.call(s, p))\n                t[p] = s[p];\n        }\n        return t;\n    };\n    return __assign.apply(this, arguments);\n};\nvar __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {\n    return new (P || (P = Promise))(function (resolve, reject) {\n        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }\n        function rejected(value) { try { step(generator[\"throw\"](value)); } catch (e) { reject(e); } }\n        function step(result) { result.done ? resolve(result.value) : new P(function (resolve) { resolve(result.value); }).then(fulfilled, rejected); }\n        step((generator = generator.apply(thisArg, _arguments || [])).next());\n    });\n};\nvar __generator = (this && this.__generator) || function (thisArg, body) {\n    var _ = { label: 0, sent: function() { if (t[0] & 1) throw t[1]; return t[1]; }, trys: [], ops: [] }, f, y, t, g;\n    return g = { next: verb(0), \"throw\": verb(1), \"return\": verb(2) }, typeof Symbol === \"function\" && (g[Symbol.iterator] = function() { return this; }), g;\n    function verb(n) { return function (v) { return step([n, v]); }; }\n    function step(op) {\n        if (f) throw new TypeError(\"Generator is already executing.\");\n        while (_) try {\n            if (f = 1, y && (t = op[0] & 2 ? y[\"return\"] : op[0] ? y[\"throw\"] || ((t = y[\"return\"]) && t.call(y), 0) : y.next) && !(t = t.call(y, op[1])).done) return t;\n            if (y = 0, t) op = [op[0] & 2, t.value];\n            switch (op[0]) {\n                case 0: case 1: t = op; break;\n                case 4: _.label++; return { value: op[1], done: false };\n                case 5: _.label++; y = op[1]; op = [0]; continue;\n                case 7: op = _.ops.pop(); _.trys.pop(); continue;\n                default:\n                    if (!(t = _.trys, t = t.length > 0 && t[t.length - 1]) && (op[0] === 6 || op[0] === 2)) { _ = 0; continue; }\n                    if (op[0] === 3 && (!t || (op[1] > t[0] && op[1] < t[3]))) { _.label = op[1]; break; }\n                    if (op[0] === 6 && _.label < t[1]) { _.label = t[1]; t = op; break; }\n                    if (t && _.label < t[2]) { _.label = t[2]; _.ops.push(op); break; }\n                    if (t[2]) _.ops.pop();\n                    _.trys.pop(); continue;\n            }\n            op = body.call(thisArg, _);\n        } catch (e) { op = [6, e]; y = 0; } finally { f = t = 0; }\n        if (op[0] & 5) throw op[1]; return { value: op[0] ? op[1] : void 0, done: true };\n    }\n};\nObject.defineProperty(exports, \"__esModule\", { value: true });\n/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nvar tf = require(\"@tensorflow/tfjs-core\");\n/**\n * A calculator that refines landmarks using corresponding heatmap area.\n *\n * High level algorithm\n * For each landmark, we replace original value with a value calculated from the\n * area in heatmap close to original landmark position (the area is defined by\n * config.kernelSize). To calculate new coordinate from heatmap we calculate an\n * weighted average inside the kernel. We update the landmark if heatmap is\n * confident in it's prediction i.e. max(heatmap) in kernel is at least bigger\n * than config.minConfidenceToRefine.\n * @param landmarks List of lardmarks to refine.\n * @param heatmapTensor The heatmap for the landmarks with shape\n *     [height, width, channel]. The channel dimension has to be the same as\n *     the number of landmarks.\n * @param config The config for refineLandmarksFromHeap,\n *     see `RefineLandmarksFromHeatmapConfig` for detail.\n *\n * @returns Normalized landmarks.\n */\nfunction refineLandmarksFromHeatmap(landmarks, heatmapTensor, config) {\n    return __awaiter(this, void 0, void 0, function () {\n        var $heatmapTensor, _a, hmHeight, hmWidth, hmChannels, outLandmarks, heatmapBuf, i, landmark, outLandmark, centerCol, centerRow, offset, beginCol, endCol, beginRow, endRow, sum, weightedCol, weightedRow, maxValue, row, col, confidence;\n        return __generator(this, function (_b) {\n            switch (_b.label) {\n                case 0:\n                    $heatmapTensor = tf.squeeze(heatmapTensor, [0]);\n                    _a = $heatmapTensor.shape, hmHeight = _a[0], hmWidth = _a[1], hmChannels = _a[2];\n                    if (landmarks.length !== hmChannels) {\n                        throw new Error('Expected heatmap to have same number of channels ' +\n                            'as the number of landmarks. But got landmarks length: ' +\n                            (landmarks.length + \", heatmap length: \" + hmChannels));\n                    }\n                    outLandmarks = [];\n                    return [4 /*yield*/, $heatmapTensor.buffer()];\n                case 1:\n                    heatmapBuf = _b.sent();\n                    for (i = 0; i < landmarks.length; i++) {\n                        landmark = landmarks[i];\n                        outLandmark = __assign({}, landmark);\n                        outLandmarks.push(outLandmark);\n                        centerCol = Math.trunc(outLandmark.x * hmWidth);\n                        centerRow = Math.trunc(outLandmark.y * hmHeight);\n                        // Point is outside of the image let's keep it intact.\n                        if (centerCol < 0 || centerCol >= hmWidth || centerRow < 0 ||\n                            centerCol >= hmHeight) {\n                            continue;\n                        }\n                        offset = Math.trunc((config.kernelSize - 1) / 2);\n                        beginCol = Math.max(0, centerCol - offset);\n                        endCol = Math.min(hmWidth, centerCol + offset + 1);\n                        beginRow = Math.max(0, centerRow - offset);\n                        endRow = Math.min(hmHeight, centerRow + offset + 1);\n                        sum = 0;\n                        weightedCol = 0;\n                        weightedRow = 0;\n                        maxValue = 0;\n                        // Main loop. Go over kernel and calculate weighted sum of coordinates,\n                        // sum of weights and max weights.\n                        for (row = beginRow; row < endRow; ++row) {\n                            for (col = beginCol; col < endCol; ++col) {\n                                confidence = heatmapBuf.get(row, col, i);\n                                sum += confidence;\n                                maxValue = Math.max(maxValue, confidence);\n                                weightedCol += col * confidence;\n                                weightedRow += row * confidence;\n                            }\n                        }\n                        if (maxValue >= config.minConfidenceToRefine && sum > 0) {\n                            outLandmark.x = weightedCol / hmWidth / sum;\n                            outLandmark.y = weightedRow / hmHeight / sum;\n                        }\n                    }\n                    $heatmapTensor.dispose();\n                    return [2 /*return*/, outLandmarks];\n            }\n        });\n    });\n}\nexports.refineLandmarksFromHeatmap = refineLandmarksFromHeatmap;\n//# sourceMappingURL=refine_landmarks_from_heatmap.js.map","\n/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nObject.defineProperty(exports, \"__esModule\", { value: true });\n/**\n * Adjusts detection locations on the letterboxed image to the corresponding\n * locations on the same image with the letterbox removed (the input image to\n * the graph before image transformation).\n *\n * @param detections A list of detection boxes on an letterboxed image.\n * @param letterboxPadding A `padding` object representing the letterbox padding\n *     from the 4 sides: left, top, right, bottom, of the letterboxed image,\n *     normalized by the letterboxed image dimensions.\n * @returns detections: A list of detection boxes representing detections with\n *     their locations adjusted to the letterbox-removed (non-padded) image.\n */\n// ref:\n// https://github.com/google/mediapipe/blob/master/mediapipe/calculators/util/detection_letterbox_removal_calculator.cc\nfunction removeDetectionLetterbox(detections, letterboxPadding) {\n    if (detections === void 0) { detections = []; }\n    var left = letterboxPadding.left;\n    var top = letterboxPadding.top;\n    var leftAndRight = letterboxPadding.left + letterboxPadding.right;\n    var topAndBottom = letterboxPadding.top + letterboxPadding.bottom;\n    for (var i = 0; i < detections.length; i++) {\n        var detection = detections[i];\n        var relativeBoundingBox = detection.locationData.relativeBoundingBox;\n        var xMin = (relativeBoundingBox.xMin - left) / (1 - leftAndRight);\n        var yMin = (relativeBoundingBox.yMin - top) / (1 - topAndBottom);\n        var width = relativeBoundingBox.width / (1 - leftAndRight);\n        var height = relativeBoundingBox.height / (1 - topAndBottom);\n        relativeBoundingBox.xMin = xMin;\n        relativeBoundingBox.yMin = yMin;\n        relativeBoundingBox.width = width;\n        relativeBoundingBox.height = height;\n        for (var i_1 = 0; i_1 < detection.locationData.relativeKeypoints.length; ++i_1) {\n            var keypoint = detection.locationData.relativeKeypoints[i_1];\n            var newX = (keypoint.x - left) / (1 - leftAndRight);\n            var newY = (keypoint.y - top) / (1 - topAndBottom);\n            keypoint.x = newX;\n            keypoint.y = newY;\n        }\n    }\n    return detections;\n}\nexports.removeDetectionLetterbox = removeDetectionLetterbox;\n//# sourceMappingURL=remove_detection_letterbox.js.map","\n/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nvar __assign = (this && this.__assign) || function () {\n    __assign = Object.assign || function(t) {\n        for (var s, i = 1, n = arguments.length; i < n; i++) {\n            s = arguments[i];\n            for (var p in s) if (Object.prototype.hasOwnProperty.call(s, p))\n                t[p] = s[p];\n        }\n        return t;\n    };\n    return __assign.apply(this, arguments);\n};\nObject.defineProperty(exports, \"__esModule\", { value: true });\n/**\n * Adjusts landmark locations on a letterboxed image to the corresponding\n * locations on the same image with the letterbox removed.\n * @param rawLandmark A NormalizedLandmarkList representing landmarks on an\n * letterboxed image.\n * @param padding A `padding` representing the letterbox padding from the 4\n *     sides, left, top, right, bottom, of the letterboxed image, normalized by\n *     the letterboxed image dimensions.\n * @returns Normalized landmarks.\n */\n// ref:\n// https://github.com/google/mediapipe/blob/master/mediapipe/calculators/util/landmark_letterbox_removal_calculator.cc\nfunction removeLandmarkLetterbox(rawLandmark, padding) {\n    var left = padding.left;\n    var top = padding.top;\n    var leftAndRight = padding.left + padding.right;\n    var topAndBottom = padding.top + padding.bottom;\n    var outLandmarks = rawLandmark.map(function (landmark) {\n        return __assign({}, landmark, { x: (landmark.x - left) / (1 - leftAndRight), y: (landmark.y - top) / (1 - topAndBottom), z: landmark.z / (1 - leftAndRight) // Scale Z coordinate as X.\n         });\n    });\n    return outLandmarks;\n}\nexports.removeLandmarkLetterbox = removeLandmarkLetterbox;\n//# sourceMappingURL=remove_landmark_letterbox.js.map","\nvar __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {\n    return new (P || (P = Promise))(function (resolve, reject) {\n        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }\n        function rejected(value) { try { step(generator[\"throw\"](value)); } catch (e) { reject(e); } }\n        function step(result) { result.done ? resolve(result.value) : new P(function (resolve) { resolve(result.value); }).then(fulfilled, rejected); }\n        step((generator = generator.apply(thisArg, _arguments || [])).next());\n    });\n};\nvar __generator = (this && this.__generator) || function (thisArg, body) {\n    var _ = { label: 0, sent: function() { if (t[0] & 1) throw t[1]; return t[1]; }, trys: [], ops: [] }, f, y, t, g;\n    return g = { next: verb(0), \"throw\": verb(1), \"return\": verb(2) }, typeof Symbol === \"function\" && (g[Symbol.iterator] = function() { return this; }), g;\n    function verb(n) { return function (v) { return step([n, v]); }; }\n    function step(op) {\n        if (f) throw new TypeError(\"Generator is already executing.\");\n        while (_) try {\n            if (f = 1, y && (t = op[0] & 2 ? y[\"return\"] : op[0] ? y[\"throw\"] || ((t = y[\"return\"]) && t.call(y), 0) : y.next) && !(t = t.call(y, op[1])).done) return t;\n            if (y = 0, t) op = [op[0] & 2, t.value];\n            switch (op[0]) {\n                case 0: case 1: t = op; break;\n                case 4: _.label++; return { value: op[1], done: false };\n                case 5: _.label++; y = op[1]; op = [0]; continue;\n                case 7: op = _.ops.pop(); _.trys.pop(); continue;\n                default:\n                    if (!(t = _.trys, t = t.length > 0 && t[t.length - 1]) && (op[0] === 6 || op[0] === 2)) { _ = 0; continue; }\n                    if (op[0] === 3 && (!t || (op[1] > t[0] && op[1] < t[3]))) { _.label = op[1]; break; }\n                    if (op[0] === 6 && _.label < t[1]) { _.label = t[1]; t = op; break; }\n                    if (t && _.label < t[2]) { _.label = t[2]; _.ops.push(op); break; }\n                    if (t[2]) _.ops.pop();\n                    _.trys.pop(); continue;\n            }\n            op = body.call(thisArg, _);\n        } catch (e) { op = [6, e]; y = 0; } finally { f = t = 0; }\n        if (op[0] & 5) throw op[1]; return { value: op[0] ? op[1] : void 0, done: true };\n    }\n};\nObject.defineProperty(exports, \"__esModule\", { value: true });\n/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nvar tf = require(\"@tensorflow/tfjs-core\");\n/**\n * Convert result Tensors from object detection models into Detection boxes.\n *\n * @param detectionTensors List of Tensors of type Float32. The list of tensors\n *     can have 2 or 3 tensors. First tensor is the predicted raw\n *     boxes/keypoints. The size of the values must be\n *     (num_boxes * num_predicted_values). Second tensor is the score tensor.\n *     The size of the valuse must be (num_boxes * num_classes). It's optional\n *     to pass in a third tensor for anchors (e.g. for SSD models) depend on the\n *     outputs of the detection model. The size of anchor tensor must be\n *     (num_boxes * 4).\n * @param anchor A tensor for anchors. The size of anchor tensor must be\n *     (num_boxes * 4).\n * @param config\n */\nfunction tensorsToDetections(detectionTensors, anchor, config) {\n    return __awaiter(this, void 0, void 0, function () {\n        var rawScoreTensor, rawBoxTensor, boxes, normalizedScore, outputDetections;\n        return __generator(this, function (_a) {\n            switch (_a.label) {\n                case 0:\n                    rawScoreTensor = detectionTensors[0];\n                    rawBoxTensor = detectionTensors[1];\n                    boxes = decodeBoxes(rawBoxTensor, anchor, config);\n                    normalizedScore = tf.tidy(function () {\n                        var normalizedScore = rawScoreTensor;\n                        if (config.sigmoidScore) {\n                            if (config.scoreClippingThresh != null) {\n                                normalizedScore = tf.clipByValue(rawScoreTensor, -config.scoreClippingThresh, config.scoreClippingThresh);\n                            }\n                            normalizedScore = tf.sigmoid(normalizedScore);\n                            return normalizedScore;\n                        }\n                        return normalizedScore;\n                    });\n                    return [4 /*yield*/, convertToDetections(boxes, normalizedScore, config)];\n                case 1:\n                    outputDetections = _a.sent();\n                    tf.dispose([boxes, normalizedScore]);\n                    return [2 /*return*/, outputDetections];\n            }\n        });\n    });\n}\nexports.tensorsToDetections = tensorsToDetections;\nfunction convertToDetections(detectionBoxes, detectionScore, config) {\n    return __awaiter(this, void 0, void 0, function () {\n        var outputDetections, detectionBoxesData, detectionScoresData, i, boxOffset, detection, bbox, locationData, totalIdx, kpId, keypointIndex, keypoint;\n        return __generator(this, function (_a) {\n            switch (_a.label) {\n                case 0:\n                    outputDetections = [];\n                    return [4 /*yield*/, detectionBoxes.data()];\n                case 1:\n                    detectionBoxesData = _a.sent();\n                    return [4 /*yield*/, detectionScore.data()];\n                case 2:\n                    detectionScoresData = _a.sent();\n                    for (i = 0; i < config.numBoxes; ++i) {\n                        if (config.minScoreThresh != null &&\n                            detectionScoresData[i] < config.minScoreThresh) {\n                            continue;\n                        }\n                        boxOffset = i * config.numCoords;\n                        detection = convertToDetection(detectionBoxesData[boxOffset + 0] /* boxYMin */, detectionBoxesData[boxOffset + 1] /* boxXMin */, detectionBoxesData[boxOffset + 2] /* boxYMax */, detectionBoxesData[boxOffset + 3] /* boxXMax */, detectionScoresData[i], config.flipVertically, i);\n                        bbox = detection.locationData.relativeBoundingBox;\n                        if (bbox.width < 0 || bbox.height < 0) {\n                            // Decoded detection boxes could have negative values for width/height\n                            // due to model prediction. Filter out those boxes since some\n                            // downstream calculators may assume non-negative values.\n                            continue;\n                        }\n                        // Add keypoints.\n                        if (config.numKeypoints > 0) {\n                            locationData = detection.locationData;\n                            locationData.relativeKeypoints = [];\n                            totalIdx = config.numKeypoints * config.numValuesPerKeypoint;\n                            for (kpId = 0; kpId < totalIdx; kpId += config.numValuesPerKeypoint) {\n                                keypointIndex = boxOffset + config.keypointCoordOffset + kpId;\n                                keypoint = {\n                                    x: detectionBoxesData[keypointIndex + 0],\n                                    y: config.flipVertically ? 1 - detectionBoxesData[keypointIndex + 1] :\n                                        detectionBoxesData[keypointIndex + 1]\n                                };\n                                locationData.relativeKeypoints.push(keypoint);\n                            }\n                        }\n                        outputDetections.push(detection);\n                    }\n                    return [2 /*return*/, outputDetections];\n            }\n        });\n    });\n}\nexports.convertToDetections = convertToDetections;\nfunction convertToDetection(boxYMin, boxXMin, boxYMax, boxXMax, score, flipVertically, i) {\n    return {\n        score: [score],\n        ind: i,\n        locationData: {\n            relativeBoundingBox: {\n                xMin: boxXMin,\n                yMin: flipVertically ? 1 - boxYMax : boxYMin,\n                xMax: boxXMax,\n                yMax: flipVertically ? 1 - boxYMin : boxYMax,\n                width: boxXMax - boxXMin,\n                height: boxYMax - boxYMin\n            }\n        }\n    };\n}\n//[xCenter, yCenter, w, h, kp1, kp2, kp3, kp4]\n//[yMin, xMin, yMax, xMax, kpX, kpY, kpX, kpY]\nfunction decodeBoxes(rawBoxes, anchor, config) {\n    return tf.tidy(function () {\n        var yCenter;\n        var xCenter;\n        var h;\n        var w;\n        if (config.reverseOutputOrder) {\n            // Shape [numOfBoxes, 1].\n            xCenter = tf.squeeze(tf.slice(rawBoxes, [0, config.boxCoordOffset + 0], [-1, 1]));\n            yCenter = tf.squeeze(tf.slice(rawBoxes, [0, config.boxCoordOffset + 1], [-1, 1]));\n            w = tf.squeeze(tf.slice(rawBoxes, [0, config.boxCoordOffset + 2], [-1, 1]));\n            h = tf.squeeze(tf.slice(rawBoxes, [0, config.boxCoordOffset + 3], [-1, 1]));\n        }\n        else {\n            yCenter = tf.squeeze(tf.slice(rawBoxes, [0, config.boxCoordOffset + 0], [-1, 1]));\n            xCenter = tf.squeeze(tf.slice(rawBoxes, [0, config.boxCoordOffset + 1], [-1, 1]));\n            h = tf.squeeze(tf.slice(rawBoxes, [0, config.boxCoordOffset + 2], [-1, 1]));\n            w = tf.squeeze(tf.slice(rawBoxes, [0, config.boxCoordOffset + 3], [-1, 1]));\n        }\n        xCenter =\n            tf.add(tf.mul(tf.div(xCenter, config.xScale), anchor.w), anchor.x);\n        yCenter =\n            tf.add(tf.mul(tf.div(yCenter, config.yScale), anchor.h), anchor.y);\n        if (config.applyExponentialOnBoxSize) {\n            h = tf.mul(tf.exp(tf.div(h, config.hScale)), anchor.h);\n            w = tf.mul(tf.exp(tf.div(w, config.wScale)), anchor.w);\n        }\n        else {\n            h = tf.mul(tf.div(h, config.hScale), anchor.h);\n            w = tf.mul(tf.div(w, config.wScale), anchor.h);\n        }\n        var yMin = tf.sub(yCenter, tf.div(h, 2));\n        var xMin = tf.sub(xCenter, tf.div(w, 2));\n        var yMax = tf.add(yCenter, tf.div(h, 2));\n        var xMax = tf.add(xCenter, tf.div(w, 2));\n        // Shape [numOfBoxes, 4].\n        var boxes = tf.concat([\n            tf.reshape(yMin, [config.numBoxes, 1]),\n            tf.reshape(xMin, [config.numBoxes, 1]),\n            tf.reshape(yMax, [config.numBoxes, 1]),\n            tf.reshape(xMax, [config.numBoxes, 1])\n        ], 1);\n        if (config.numKeypoints) {\n            for (var k = 0; k < config.numKeypoints; ++k) {\n                var keypointOffset = config.keypointCoordOffset + k * config.numValuesPerKeypoint;\n                var keypointX = void 0;\n                var keypointY = void 0;\n                if (config.reverseOutputOrder) {\n                    keypointX =\n                        tf.squeeze(tf.slice(rawBoxes, [0, keypointOffset], [-1, 1]));\n                    keypointY =\n                        tf.squeeze(tf.slice(rawBoxes, [0, keypointOffset + 1], [-1, 1]));\n                }\n                else {\n                    keypointY =\n                        tf.squeeze(tf.slice(rawBoxes, [0, keypointOffset], [-1, 1]));\n                    keypointX =\n                        tf.squeeze(tf.slice(rawBoxes, [0, keypointOffset + 1], [-1, 1]));\n                }\n                var keypointXNormalized = tf.add(tf.mul(tf.div(keypointX, config.xScale), anchor.w), anchor.x);\n                var keypointYNormalized = tf.add(tf.mul(tf.div(keypointY, config.yScale), anchor.h), anchor.y);\n                boxes = tf.concat([\n                    boxes, tf.reshape(keypointXNormalized, [config.numBoxes, 1]),\n                    tf.reshape(keypointYNormalized, [config.numBoxes, 1])\n                ], 1);\n            }\n        }\n        // Shape [numOfBoxes, 4] || [numOfBoxes, 12].\n        return boxes;\n    });\n}\n//# sourceMappingURL=tensors_to_detections.js.map","\nvar __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {\n    return new (P || (P = Promise))(function (resolve, reject) {\n        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }\n        function rejected(value) { try { step(generator[\"throw\"](value)); } catch (e) { reject(e); } }\n        function step(result) { result.done ? resolve(result.value) : new P(function (resolve) { resolve(result.value); }).then(fulfilled, rejected); }\n        step((generator = generator.apply(thisArg, _arguments || [])).next());\n    });\n};\nvar __generator = (this && this.__generator) || function (thisArg, body) {\n    var _ = { label: 0, sent: function() { if (t[0] & 1) throw t[1]; return t[1]; }, trys: [], ops: [] }, f, y, t, g;\n    return g = { next: verb(0), \"throw\": verb(1), \"return\": verb(2) }, typeof Symbol === \"function\" && (g[Symbol.iterator] = function() { return this; }), g;\n    function verb(n) { return function (v) { return step([n, v]); }; }\n    function step(op) {\n        if (f) throw new TypeError(\"Generator is already executing.\");\n        while (_) try {\n            if (f = 1, y && (t = op[0] & 2 ? y[\"return\"] : op[0] ? y[\"throw\"] || ((t = y[\"return\"]) && t.call(y), 0) : y.next) && !(t = t.call(y, op[1])).done) return t;\n            if (y = 0, t) op = [op[0] & 2, t.value];\n            switch (op[0]) {\n                case 0: case 1: t = op; break;\n                case 4: _.label++; return { value: op[1], done: false };\n                case 5: _.label++; y = op[1]; op = [0]; continue;\n                case 7: op = _.ops.pop(); _.trys.pop(); continue;\n                default:\n                    if (!(t = _.trys, t = t.length > 0 && t[t.length - 1]) && (op[0] === 6 || op[0] === 2)) { _ = 0; continue; }\n                    if (op[0] === 3 && (!t || (op[1] > t[0] && op[1] < t[3]))) { _.label = op[1]; break; }\n                    if (op[0] === 6 && _.label < t[1]) { _.label = t[1]; t = op; break; }\n                    if (t && _.label < t[2]) { _.label = t[2]; _.ops.push(op); break; }\n                    if (t[2]) _.ops.pop();\n                    _.trys.pop(); continue;\n            }\n            op = body.call(thisArg, _);\n        } catch (e) { op = [6, e]; y = 0; } finally { f = t = 0; }\n        if (op[0] & 5) throw op[1]; return { value: op[0] ? op[1] : void 0, done: true };\n    }\n};\nObject.defineProperty(exports, \"__esModule\", { value: true });\nvar sigmoid_1 = require(\"../../calculators/sigmoid\");\n/**\n * A calculator for converting Tensors from regression models into landmarks.\n * Note that if the landmarks in the tensor has more than 5 dimensions, only the\n * first 5 dimensions will be converted to [x,y,z, visibility, presence]. The\n * latter two fields may also stay unset if such attributes are not supported in\n * the model.\n * @param landmarkTensor List of Tensors of type float32. Only the first tensor\n * will be used. The size of the values must be (num_dimension x num_landmarks).\n * @param config\n * @param flipHorizontally Optional. Whether to flip landmarks horizontally or\n * not. Overrides corresponding side packet and/or field in the calculator\n * options.\n * @param flipVertically Optional. Whether to flip landmarks vertically or not.\n * Overrides corresponding side packet and/or field in the calculator options.\n *\n * @returns Normalized landmarks.\n */\nfunction tensorsToLandmarks(landmarkTensor, config, flipHorizontally, flipVertically) {\n    if (flipHorizontally === void 0) { flipHorizontally = false; }\n    if (flipVertically === void 0) { flipVertically = false; }\n    return __awaiter(this, void 0, void 0, function () {\n        var numValues, numDimensions, rawLandmarks, outputLandmarks, ld, offset, landmark, i, landmark;\n        return __generator(this, function (_a) {\n            switch (_a.label) {\n                case 0:\n                    numValues = landmarkTensor.size;\n                    numDimensions = numValues / config.numLandmarks;\n                    return [4 /*yield*/, landmarkTensor.data()];\n                case 1:\n                    rawLandmarks = _a.sent();\n                    outputLandmarks = [];\n                    for (ld = 0; ld < config.numLandmarks; ++ld) {\n                        offset = ld * numDimensions;\n                        landmark = { x: 0, y: 0 };\n                        if (flipHorizontally) {\n                            landmark.x = config.inputImageWidth - rawLandmarks[offset];\n                        }\n                        else {\n                            landmark.x = rawLandmarks[offset];\n                        }\n                        if (numDimensions > 1) {\n                            if (flipVertically) {\n                                landmark.y = config.inputImageHeight - rawLandmarks[offset + 1];\n                            }\n                            else {\n                                landmark.y = rawLandmarks[offset + 1];\n                            }\n                        }\n                        if (numDimensions > 2) {\n                            landmark.z = rawLandmarks[offset + 2];\n                        }\n                        if (numDimensions > 3) {\n                            landmark.score = sigmoid_1.sigmoid(rawLandmarks[offset + 3]);\n                        }\n                        // presence is in rawLandmarks[offset + 4], we don't expose it.\n                        outputLandmarks.push(landmark);\n                    }\n                    for (i = 0; i < outputLandmarks.length; ++i) {\n                        landmark = outputLandmarks[i];\n                        landmark.x = landmark.x / config.inputImageWidth;\n                        landmark.y = landmark.y / config.inputImageHeight;\n                        // Scale Z coordinate as X + allow additional uniform normalization.\n                        landmark.z = landmark.z / config.inputImageWidth / (config.normalizeZ || 1);\n                    }\n                    return [2 /*return*/, outputLandmarks];\n            }\n        });\n    });\n}\nexports.tensorsToLandmarks = tensorsToLandmarks;\n//# sourceMappingURL=tensors_to_landmarks.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\n/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nfunction sigmoid(value) {\n    return 1 / (1 + Math.exp(-value));\n}\nexports.sigmoid = sigmoid;\n//# sourceMappingURL=sigmoid.js.map","\n/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nObject.defineProperty(exports, \"__esModule\", { value: true });\nvar image_utils_1 = require(\"../../calculators/image_utils\");\n/**\n * Performs geometric transformation to the input normalized rectangle,\n * correpsonding to input normalized rectangle respectively.\n * @param rect The normalized rectangle.\n * @param imageSize The original imageSize.\n * @param config See documentation in `RectTransformationConfig`.\n */\n// ref:\n// https://github.com/google/mediapipe/blob/master/mediapipe/calculators/util/rect_transformation_calculator.cc\nfunction transformNormalizedRect(rect, imageSize, config) {\n    var width = rect.width;\n    var height = rect.height;\n    var rotation = rect.rotation;\n    if (config.rotation != null || config.rotationDegree != null) {\n        rotation = computeNewRotation(rotation, config);\n    }\n    if (rotation === 0) {\n        rect.xCenter = rect.xCenter + width * config.shiftX;\n        rect.yCenter = rect.yCenter + height * config.shiftY;\n    }\n    else {\n        var xShift = (imageSize.width * width * config.shiftX * Math.cos(rotation) -\n            imageSize.height * height * config.shiftY * Math.sin(rotation)) /\n            imageSize.width;\n        var yShift = (imageSize.width * width * config.shiftX * Math.sin(rotation) +\n            imageSize.height * height * config.shiftY * Math.cos(rotation)) /\n            imageSize.height;\n        rect.xCenter = rect.xCenter + xShift;\n        rect.yCenter = rect.yCenter + yShift;\n    }\n    if (config.squareLong) {\n        var longSide = Math.max(width * imageSize.width, height * imageSize.height);\n        width = longSide / imageSize.width;\n        height = longSide / imageSize.height;\n    }\n    else if (config.squareShort) {\n        var shortSide = Math.min(width * imageSize.width, height * imageSize.height);\n        width = shortSide / imageSize.width;\n        height = shortSide / imageSize.height;\n    }\n    rect.width = width * config.scaleX;\n    rect.height = height * config.scaleY;\n    return rect;\n}\nexports.transformNormalizedRect = transformNormalizedRect;\nfunction computeNewRotation(rotation, config) {\n    if (config.rotation != null) {\n        rotation += config.rotation;\n    }\n    else if (config.rotationDegree != null) {\n        rotation += Math.PI * config.rotationDegree / 180;\n    }\n    return image_utils_1.normalizeRadians(rotation);\n}\nexports.computeNewRotation = computeNewRotation;\n//# sourceMappingURL=transform_rect.js.map","\n/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nvar __assign = (this && this.__assign) || function () {\n    __assign = Object.assign || function(t) {\n        for (var s, i = 1, n = arguments.length; i < n; i++) {\n            s = arguments[i];\n            for (var p in s) if (Object.prototype.hasOwnProperty.call(s, p))\n                t[p] = s[p];\n        }\n        return t;\n    };\n    return __assign.apply(this, arguments);\n};\nObject.defineProperty(exports, \"__esModule\", { value: true });\nvar low_pass_filter_1 = require(\"../../calculators/low_pass_filter\");\n/**\n * Smoothing visibility using a `LowPassFilter` for each landmark.\n */\nvar LowPassVisibilityFilter = /** @class */ (function () {\n    function LowPassVisibilityFilter(config) {\n        this.alpha = config.alpha;\n    }\n    LowPassVisibilityFilter.prototype.apply = function (landmarks) {\n        var _this = this;\n        if (landmarks == null) {\n            // Reset filters.\n            this.visibilityFilters = null;\n            return null;\n        }\n        if (this.visibilityFilters == null ||\n            (this.visibilityFilters.length !== landmarks.length)) {\n            // Initialize new filters.\n            this.visibilityFilters =\n                landmarks.map(function (_) { return new low_pass_filter_1.LowPassFilter(_this.alpha); });\n        }\n        var outLandmarks = [];\n        // Filter visibilities.\n        for (var i = 0; i < landmarks.length; ++i) {\n            var landmark = landmarks[i];\n            var outLandmark = __assign({}, landmark);\n            outLandmark.score = this.visibilityFilters[i].apply(landmark.score);\n            outLandmarks.push(outLandmark);\n        }\n        return outLandmarks;\n    };\n    return LowPassVisibilityFilter;\n}());\nexports.LowPassVisibilityFilter = LowPassVisibilityFilter;\n//# sourceMappingURL=visibility_smoothing.js.map","\n/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.DEFAULT_BLAZEPOSE_DETECTOR_MODEL_URL = 'https://tfhub.dev/mediapipe/tfjs-model/blazeposedetector/1/default/1';\nexports.DEFAULT_BLAZEPOSE_LANDMARK_MODEL_URL_FULL = 'https://tfhub.dev/mediapipe/tfjs-model/blazeposelandmark_full/2/default/2';\nexports.DEFAULT_BLAZEPOSE_LANDMARK_MODEL_URL_LITE = 'https://tfhub.dev/mediapipe/tfjs-model/blazeposelandmark_lite/2/default/2';\nexports.DEFAULT_BLAZEPOSE_LANDMARK_MODEL_URL_HEAVY = 'https://tfhub.dev/mediapipe/tfjs-model/blazeposelandmark_heavy/2/default/2';\nexports.BLAZEPOSE_DETECTOR_ANCHOR_CONFIGURATION = {\n    reduceBoxesInLowestlayer: false,\n    interpolatedScaleAspectRatio: 1.0,\n    featureMapHeight: [],\n    featureMapWidth: [],\n    numLayers: 5,\n    minScale: 0.1484375,\n    maxScale: 0.75,\n    inputSizeHeight: 224,\n    inputSizeWidth: 224,\n    anchorOffsetX: 0.5,\n    anchorOffsetY: 0.5,\n    strides: [8, 16, 32, 32, 32],\n    aspectRatios: [1.0],\n    fixedAnchorSize: true\n};\nexports.DEFAULT_BLAZEPOSE_MODEL_CONFIG = {\n    runtime: 'tfjs',\n    modelType: 'full',\n    enableSmoothing: true,\n    detectorModelUrl: exports.DEFAULT_BLAZEPOSE_DETECTOR_MODEL_URL,\n    landmarkModelUrl: exports.DEFAULT_BLAZEPOSE_LANDMARK_MODEL_URL_FULL\n};\nexports.DEFAULT_BLAZEPOSE_ESTIMATION_CONFIG = {\n    maxPoses: 1,\n    flipHorizontal: false\n};\nexports.BLAZEPOSE_TENSORS_TO_DETECTION_CONFIGURATION = {\n    applyExponentialOnBoxSize: false,\n    flipVertically: false,\n    ignoreClasses: [],\n    numClasses: 1,\n    numBoxes: 2254,\n    numCoords: 12,\n    boxCoordOffset: 0,\n    keypointCoordOffset: 4,\n    numKeypoints: 4,\n    numValuesPerKeypoint: 2,\n    sigmoidScore: true,\n    scoreClippingThresh: 100.0,\n    reverseOutputOrder: true,\n    xScale: 224.0,\n    yScale: 224.0,\n    hScale: 224.0,\n    wScale: 224.0,\n    minScoreThresh: 0.5\n};\nexports.BLAZEPOSE_DETECTOR_NON_MAX_SUPPRESSION_CONFIGURATION = {\n    minScoreThreshold: -1.0,\n    minSuppressionThreshold: 0.3\n};\nexports.BLAZEPOSE_DETECTOR_RECT_TRANSFORMATION_CONFIG = {\n    shiftX: 0,\n    shiftY: 0,\n    scaleX: 1.25,\n    scaleY: 1.25,\n    squareLong: true\n};\nexports.BLAZEPOSE_DETECTOR_IMAGE_TO_TENSOR_CONFIG = {\n    inputResolution: { width: 224, height: 224 },\n    keepAspectRatio: true\n};\nexports.BLAZEPOSE_LANDMARK_IMAGE_TO_TENSOR_CONFIG = {\n    inputResolution: { width: 256, height: 256 },\n    keepAspectRatio: true\n};\nexports.BLAZEPOSE_POSE_PRESENCE_SCORE = 0.5;\nexports.BLAZEPOSE_TENSORS_TO_LANDMARKS_CONFIG = {\n    numLandmarks: 39,\n    inputImageWidth: 256,\n    inputImageHeight: 256\n};\nexports.BLAZEPOSE_REFINE_LANDMARKS_FROM_HEATMAP_CONFIG = {\n    kernelSize: 7,\n    minConfidenceToRefine: 0.5\n};\nexports.BLAZEPOSE_NUM_KEYPOINTS = 33;\nexports.BLAZEPOSE_NUM_AUXILIARY_KEYPOINTS = 35;\nexports.BLAZEPOSE_VISIBILITY_SMOOTHING_CONFIG = {\n    alpha: 0.1\n};\nexports.BLAZEPOSE_LANDMARKS_SMOOTHING_CONFIG_ACTUAL = {\n    oneEuroFilter: {\n        frequency: 30,\n        minCutOff: 0.05,\n        // filter when landmark is static.\n        beta: 80,\n        // alpha in landmark EMA filter when landmark is moving fast.\n        derivateCutOff: 1.0,\n        // landmark velocity EMA filter.,\n        minAllowedObjectScale: 1e-6\n    }\n};\n// Auxiliary landmarks are smoothed heavier than main landmarks to make ROI\n// crop for pose landmarks prediction very stable when object is not moving but\n// responsive enough in case of sudden movements.\nexports.BLAZEPOSE_LANDMARKS_SMOOTHING_CONFIG_AUXILIARY = {\n    oneEuroFilter: {\n        frequency: 30,\n        minCutOff: 0.01,\n        // EMA filter when landmark is static.\n        beta: 10.0,\n        // ~0.68 alpha in landmark EMA filter when landmark is moving\n        // fast.\n        derivateCutOff: 1.0,\n        // landmark velocity EMA filter.\n        minAllowedObjectScale: 1e-6\n    }\n};\n//# sourceMappingURL=constants.js.map","\n/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nvar __assign = (this && this.__assign) || function () {\n    __assign = Object.assign || function(t) {\n        for (var s, i = 1, n = arguments.length; i < n; i++) {\n            s = arguments[i];\n            for (var p in s) if (Object.prototype.hasOwnProperty.call(s, p))\n                t[p] = s[p];\n        }\n        return t;\n    };\n    return __assign.apply(this, arguments);\n};\nObject.defineProperty(exports, \"__esModule\", { value: true });\nvar constants_1 = require(\"./constants\");\nfunction validateModelConfig(modelConfig) {\n    var config = modelConfig == null ?\n        __assign({}, constants_1.DEFAULT_BLAZEPOSE_MODEL_CONFIG) : __assign({}, modelConfig);\n    if (config.enableSmoothing == null) {\n        config.enableSmoothing = constants_1.DEFAULT_BLAZEPOSE_MODEL_CONFIG.enableSmoothing;\n    }\n    if (config.modelType == null) {\n        config.modelType = constants_1.DEFAULT_BLAZEPOSE_MODEL_CONFIG.modelType;\n    }\n    if (config.detectorModelUrl == null) {\n        config.detectorModelUrl = constants_1.DEFAULT_BLAZEPOSE_MODEL_CONFIG.detectorModelUrl;\n    }\n    if (config.landmarkModelUrl == null) {\n        switch (config.modelType) {\n            case 'lite':\n                config.landmarkModelUrl = constants_1.DEFAULT_BLAZEPOSE_LANDMARK_MODEL_URL_LITE;\n                break;\n            case 'heavy':\n                config.landmarkModelUrl = constants_1.DEFAULT_BLAZEPOSE_LANDMARK_MODEL_URL_HEAVY;\n                break;\n            case 'full':\n            default:\n                config.landmarkModelUrl = constants_1.DEFAULT_BLAZEPOSE_LANDMARK_MODEL_URL_FULL;\n                break;\n        }\n    }\n    return config;\n}\nexports.validateModelConfig = validateModelConfig;\nfunction validateEstimationConfig(estimationConfig) {\n    var config;\n    if (estimationConfig == null) {\n        config = constants_1.DEFAULT_BLAZEPOSE_ESTIMATION_CONFIG;\n    }\n    else {\n        config = __assign({}, estimationConfig);\n    }\n    if (config.maxPoses == null) {\n        config.maxPoses = 1;\n    }\n    if (config.maxPoses <= 0) {\n        throw new Error(\"Invalid maxPoses \" + config.maxPoses + \". Should be > 0.\");\n    }\n    if (config.maxPoses > 1) {\n        throw new Error('Multi-pose detection is not implemented yet. Please set maxPoses ' +\n            'to 1.');\n    }\n    return config;\n}\nexports.validateEstimationConfig = validateEstimationConfig;\n//# sourceMappingURL=detector_utils.js.map","\n/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nvar __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {\n    return new (P || (P = Promise))(function (resolve, reject) {\n        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }\n        function rejected(value) { try { step(generator[\"throw\"](value)); } catch (e) { reject(e); } }\n        function step(result) { result.done ? resolve(result.value) : new P(function (resolve) { resolve(result.value); }).then(fulfilled, rejected); }\n        step((generator = generator.apply(thisArg, _arguments || [])).next());\n    });\n};\nvar __generator = (this && this.__generator) || function (thisArg, body) {\n    var _ = { label: 0, sent: function() { if (t[0] & 1) throw t[1]; return t[1]; }, trys: [], ops: [] }, f, y, t, g;\n    return g = { next: verb(0), \"throw\": verb(1), \"return\": verb(2) }, typeof Symbol === \"function\" && (g[Symbol.iterator] = function() { return this; }), g;\n    function verb(n) { return function (v) { return step([n, v]); }; }\n    function step(op) {\n        if (f) throw new TypeError(\"Generator is already executing.\");\n        while (_) try {\n            if (f = 1, y && (t = op[0] & 2 ? y[\"return\"] : op[0] ? y[\"throw\"] || ((t = y[\"return\"]) && t.call(y), 0) : y.next) && !(t = t.call(y, op[1])).done) return t;\n            if (y = 0, t) op = [op[0] & 2, t.value];\n            switch (op[0]) {\n                case 0: case 1: t = op; break;\n                case 4: _.label++; return { value: op[1], done: false };\n                case 5: _.label++; y = op[1]; op = [0]; continue;\n                case 7: op = _.ops.pop(); _.trys.pop(); continue;\n                default:\n                    if (!(t = _.trys, t = t.length > 0 && t[t.length - 1]) && (op[0] === 6 || op[0] === 2)) { _ = 0; continue; }\n                    if (op[0] === 3 && (!t || (op[1] > t[0] && op[1] < t[3]))) { _.label = op[1]; break; }\n                    if (op[0] === 6 && _.label < t[1]) { _.label = t[1]; t = op; break; }\n                    if (t && _.label < t[2]) { _.label = t[2]; _.ops.push(op); break; }\n                    if (t[2]) _.ops.pop();\n                    _.trys.pop(); continue;\n            }\n            op = body.call(thisArg, _);\n        } catch (e) { op = [6, e]; y = 0; } finally { f = t = 0; }\n        if (op[0] & 5) throw op[1]; return { value: op[0] ? op[1] : void 0, done: true };\n    }\n};\nObject.defineProperty(exports, \"__esModule\", { value: true });\nvar tfc = require(\"@tensorflow/tfjs-converter\");\nvar tf = require(\"@tensorflow/tfjs-core\");\nvar constants_1 = require(\"../calculators/constants\");\nvar image_utils_1 = require(\"../calculators/image_utils\");\nvar is_video_1 = require(\"../calculators/is_video\");\nvar keypoints_one_euro_filter_1 = require(\"../calculators/keypoints_one_euro_filter\");\nvar low_pass_filter_1 = require(\"../calculators/low_pass_filter\");\nvar constants_2 = require(\"../constants\");\nvar types_1 = require(\"../types\");\nvar util_1 = require(\"../util\");\nvar constants_3 = require(\"./constants\");\nvar detector_utils_1 = require(\"./detector_utils\");\n/**\n * MoveNet detector class.\n */\nvar MoveNetDetector = /** @class */ (function () {\n    function MoveNetDetector(moveNetModel, config) {\n        this.moveNetModel = moveNetModel;\n        this.modelInputResolution = { height: 0, width: 0 };\n        this.keypointIndexByName = util_1.getKeypointIndexByName(types_1.SupportedModels.MoveNet);\n        // Global states.\n        this.keypointsFilter = new keypoints_one_euro_filter_1.KeypointsOneEuroFilter(constants_3.KEYPOINT_FILTER_CONFIG);\n        this.cropRegionFilterYMin = new low_pass_filter_1.LowPassFilter(constants_3.CROP_FILTER_ALPHA);\n        this.cropRegionFilterXMin = new low_pass_filter_1.LowPassFilter(constants_3.CROP_FILTER_ALPHA);\n        this.cropRegionFilterYMax = new low_pass_filter_1.LowPassFilter(constants_3.CROP_FILTER_ALPHA);\n        this.cropRegionFilterXMax = new low_pass_filter_1.LowPassFilter(constants_3.CROP_FILTER_ALPHA);\n        if (config.modelType === constants_3.SINGLEPOSE_LIGHTNING) {\n            this.modelInputResolution.width = constants_3.MOVENET_SINGLEPOSE_LIGHTNING_RESOLUTION;\n            this.modelInputResolution.height =\n                constants_3.MOVENET_SINGLEPOSE_LIGHTNING_RESOLUTION;\n        }\n        else if (config.modelType === constants_3.SINGLEPOSE_THUNDER) {\n            this.modelInputResolution.width = constants_3.MOVENET_SINGLEPOSE_THUNDER_RESOLUTION;\n            this.modelInputResolution.height = constants_3.MOVENET_SINGLEPOSE_THUNDER_RESOLUTION;\n        }\n        this.enableSmoothing = config.enableSmoothing;\n    }\n    /**\n     * Runs inference on an image using a model that is assumed to be a person\n     * keypoint model that outputs 17 keypoints.\n     * @param inputImage 4D tensor containing the input image. Should be of size\n     *     [1, modelHeight, modelWidth, 3].\n     * @param executeSync Whether to execute the model synchronously.\n     * @return An InferenceResult with keypoints and scores, or null if the\n     *     inference call could not be executed (for example when the model was\n     *     not initialized yet) or if it produced an unexpected tensor size.\n     */\n    MoveNetDetector.prototype.detectKeypoints = function (inputImage, executeSync) {\n        if (executeSync === void 0) { executeSync = true; }\n        return __awaiter(this, void 0, void 0, function () {\n            var numKeypoints, outputTensor, inferenceResult, keypoints, i;\n            return __generator(this, function (_a) {\n                switch (_a.label) {\n                    case 0:\n                        if (!this.moveNetModel) {\n                            return [2 /*return*/, null];\n                        }\n                        numKeypoints = 17;\n                        if (!executeSync) return [3 /*break*/, 1];\n                        outputTensor = this.moveNetModel.execute(inputImage);\n                        return [3 /*break*/, 3];\n                    case 1: return [4 /*yield*/, this.moveNetModel.executeAsync(inputImage)];\n                    case 2:\n                        outputTensor =\n                            (_a.sent());\n                        _a.label = 3;\n                    case 3:\n                        // We expect an output array of shape [1, 1, 17, 3] (batch, person,\n                        // keypoint, (y, x, score)).\n                        if (!outputTensor || outputTensor.shape.length !== 4 ||\n                            outputTensor.shape[0] !== 1 || outputTensor.shape[1] !== 1 ||\n                            outputTensor.shape[2] !== numKeypoints || outputTensor.shape[3] !== 3) {\n                            outputTensor.dispose();\n                            return [2 /*return*/, null];\n                        }\n                        if (!(tf.getBackend() !== 'webgpu')) return [3 /*break*/, 4];\n                        inferenceResult = outputTensor.dataSync();\n                        return [3 /*break*/, 6];\n                    case 4: return [4 /*yield*/, outputTensor.data()];\n                    case 5:\n                        inferenceResult = _a.sent();\n                        _a.label = 6;\n                    case 6:\n                        outputTensor.dispose();\n                        keypoints = [];\n                        for (i = 0; i < numKeypoints; ++i) {\n                            keypoints[i] = {\n                                y: inferenceResult[i * 3],\n                                x: inferenceResult[i * 3 + 1],\n                                score: inferenceResult[i * 3 + 2]\n                            };\n                        }\n                        return [2 /*return*/, keypoints];\n                }\n            });\n        });\n    };\n    /**\n     * Estimates poses for an image or video frame.\n     *\n     * This does standard ImageNet pre-processing before inferring through the\n     * model. The image should pixels should have values [0-255]. It returns a\n     * single pose.\n     *\n     * @param image ImageData|HTMLImageElement|HTMLCanvasElement|HTMLVideoElement\n     * The input image to feed through the network.\n     *\n     * @param config Optional. A configuration object with the following\n     * properties:\n     *  `maxPoses`: Optional. Has to be set to 1.\n     *\n     * @param timestamp Optional. In milliseconds. This is useful when image is\n     *     a tensor, which doesn't have timestamp info. Or to override timestamp\n     *     in a video.\n     *\n     * @return An array of `Pose`s.\n     */\n    MoveNetDetector.prototype.estimatePoses = function (image, estimationConfig, timestamp) {\n        if (estimationConfig === void 0) { estimationConfig = constants_3.MOVENET_SINGLE_POSE_ESTIMATION_CONFIG; }\n        return __awaiter(this, void 0, void 0, function () {\n            var imageTensor3D, imageSize, imageTensor4D, croppedImage, keypoints, i, newCropRegion, numValidKeypoints, poseScore, i, pose;\n            var _this = this;\n            return __generator(this, function (_a) {\n                switch (_a.label) {\n                    case 0:\n                        estimationConfig = detector_utils_1.validateEstimationConfig(estimationConfig);\n                        if (image == null) {\n                            this.reset();\n                            return [2 /*return*/, []];\n                        }\n                        if (timestamp == null) {\n                            if (is_video_1.isVideo(image)) {\n                                timestamp = image.currentTime * constants_1.SECOND_TO_MICRO_SECONDS;\n                            }\n                        }\n                        else {\n                            timestamp = timestamp * constants_1.MILLISECOND_TO_MICRO_SECONDS;\n                        }\n                        imageTensor3D = image_utils_1.toImageTensor(image);\n                        imageSize = image_utils_1.getImageSize(imageTensor3D);\n                        imageTensor4D = tf.expandDims(imageTensor3D, 0);\n                        // Make sure we don't dispose the input image if it's already a tensor.\n                        if (!(image instanceof tf.Tensor)) {\n                            imageTensor3D.dispose();\n                        }\n                        if (!this.cropRegion) {\n                            this.cropRegion = this.initCropRegion(imageSize.width, imageSize.height);\n                        }\n                        croppedImage = tf.tidy(function () {\n                            // Crop region is a [batch, 4] size tensor.\n                            var cropRegionTensor = tf.tensor2d([[\n                                    _this.cropRegion.yMin, _this.cropRegion.xMin, _this.cropRegion.yMax,\n                                    _this.cropRegion.xMax\n                                ]]);\n                            // The batch index that the crop should operate on. A [batch] size\n                            // tensor.\n                            var boxInd = tf.zeros([1], 'int32');\n                            // Target size of each crop.\n                            var cropSize = [_this.modelInputResolution.height, _this.modelInputResolution.width];\n                            return tf.cast(tf.image.cropAndResize(imageTensor4D, cropRegionTensor, boxInd, cropSize, 'bilinear', 0), 'int32');\n                        });\n                        imageTensor4D.dispose();\n                        return [4 /*yield*/, this.detectKeypoints(croppedImage)];\n                    case 1:\n                        keypoints = _a.sent();\n                        croppedImage.dispose();\n                        if (keypoints == null) {\n                            this.reset();\n                            return [2 /*return*/, []];\n                        }\n                        // Convert keypoints from crop coordinates to image coordinates.\n                        for (i = 0; i < keypoints.length; ++i) {\n                            keypoints[i].y =\n                                this.cropRegion.yMin + keypoints[i].y * this.cropRegion.height;\n                            keypoints[i].x =\n                                this.cropRegion.xMin + keypoints[i].x * this.cropRegion.width;\n                        }\n                        // Apply the sequential filter before estimating the cropping area to make\n                        // it more stable.\n                        if (timestamp != null && this.enableSmoothing) {\n                            keypoints =\n                                this.keypointsFilter.apply(keypoints, timestamp, 1 /* objectScale */);\n                        }\n                        newCropRegion = this.determineCropRegion(keypoints, imageSize.height, imageSize.width);\n                        this.cropRegion = this.filterCropRegion(newCropRegion);\n                        numValidKeypoints = 0.0;\n                        poseScore = 0.0;\n                        for (i = 0; i < keypoints.length; ++i) {\n                            keypoints[i].name = constants_2.COCO_KEYPOINTS[i];\n                            keypoints[i].y *= imageSize.height;\n                            keypoints[i].x *= imageSize.width;\n                            if (keypoints[i].score > constants_3.MIN_CROP_KEYPOINT_SCORE) {\n                                ++numValidKeypoints;\n                                poseScore += keypoints[i].score;\n                            }\n                        }\n                        if (numValidKeypoints > 0) {\n                            poseScore /= numValidKeypoints;\n                        }\n                        else {\n                            // No pose detected, so reset all filters.\n                            this.resetFilters();\n                        }\n                        pose = { score: poseScore, keypoints: keypoints };\n                        return [2 /*return*/, [pose]];\n                }\n            });\n        });\n    };\n    MoveNetDetector.prototype.filterCropRegion = function (newCropRegion) {\n        if (!newCropRegion) {\n            this.cropRegionFilterYMin.reset();\n            this.cropRegionFilterXMin.reset();\n            this.cropRegionFilterYMax.reset();\n            this.cropRegionFilterXMax.reset();\n            return null;\n        }\n        else {\n            var filteredYMin = this.cropRegionFilterYMin.apply(newCropRegion.yMin);\n            var filteredXMin = this.cropRegionFilterXMin.apply(newCropRegion.xMin);\n            var filteredYMax = this.cropRegionFilterYMax.apply(newCropRegion.yMax);\n            var filteredXMax = this.cropRegionFilterXMax.apply(newCropRegion.xMax);\n            return {\n                yMin: filteredYMin,\n                xMin: filteredXMin,\n                yMax: filteredYMax,\n                xMax: filteredXMax,\n                height: filteredYMax - filteredYMin,\n                width: filteredXMax - filteredXMin\n            };\n        }\n    };\n    MoveNetDetector.prototype.dispose = function () {\n        this.moveNetModel.dispose();\n    };\n    MoveNetDetector.prototype.reset = function () {\n        this.cropRegion = null;\n        this.resetFilters();\n    };\n    MoveNetDetector.prototype.resetFilters = function () {\n        this.keypointsFilter.reset();\n        this.cropRegionFilterYMin.reset();\n        this.cropRegionFilterXMin.reset();\n        this.cropRegionFilterYMax.reset();\n        this.cropRegionFilterXMax.reset();\n    };\n    MoveNetDetector.prototype.torsoVisible = function (keypoints) {\n        return ((keypoints[this.keypointIndexByName['left_hip']].score >\n            constants_3.MIN_CROP_KEYPOINT_SCORE ||\n            keypoints[this.keypointIndexByName['right_hip']].score >\n                constants_3.MIN_CROP_KEYPOINT_SCORE) &&\n            (keypoints[this.keypointIndexByName['left_shoulder']].score >\n                constants_3.MIN_CROP_KEYPOINT_SCORE ||\n                keypoints[this.keypointIndexByName['right_shoulder']].score >\n                    constants_3.MIN_CROP_KEYPOINT_SCORE));\n    };\n    /**\n     * Calculates the maximum distance from each keypoints to the center location.\n     * The function returns the maximum distances from the two sets of keypoints:\n     * full 17 keypoints and 4 torso keypoints. The returned information will be\n     * used to determine the crop size. See determineCropRegion for more detail.\n     *\n     * @param targetKeypoints Maps from joint names to coordinates.\n     */\n    MoveNetDetector.prototype.determineTorsoAndBodyRange = function (keypoints, targetKeypoints, centerY, centerX) {\n        var torsoJoints = ['left_shoulder', 'right_shoulder', 'left_hip', 'right_hip'];\n        var maxTorsoYrange = 0.0;\n        var maxTorsoXrange = 0.0;\n        for (var i = 0; i < torsoJoints.length; i++) {\n            var distY = Math.abs(centerY - targetKeypoints[torsoJoints[i]][0]);\n            var distX = Math.abs(centerX - targetKeypoints[torsoJoints[i]][1]);\n            if (distY > maxTorsoYrange) {\n                maxTorsoYrange = distY;\n            }\n            if (distX > maxTorsoXrange) {\n                maxTorsoXrange = distX;\n            }\n        }\n        var maxBodyYrange = 0.0;\n        var maxBodyXrange = 0.0;\n        for (var _i = 0, _a = Object.keys(targetKeypoints); _i < _a.length; _i++) {\n            var key = _a[_i];\n            if (keypoints[this.keypointIndexByName[key]].score <\n                constants_3.MIN_CROP_KEYPOINT_SCORE) {\n                continue;\n            }\n            var distY = Math.abs(centerY - targetKeypoints[key][0]);\n            var distX = Math.abs(centerX - targetKeypoints[key][1]);\n            if (distY > maxBodyYrange) {\n                maxBodyYrange = distY;\n            }\n            if (distX > maxBodyXrange) {\n                maxBodyXrange = distX;\n            }\n        }\n        return [maxTorsoYrange, maxTorsoXrange, maxBodyYrange, maxBodyXrange];\n    };\n    /**\n     * Determines the region to crop the image for the model to run inference on.\n     * The algorithm uses the detected joints from the previous frame to estimate\n     * the square region that encloses the full body of the target person and\n     * centers at the midpoint of two hip joints. The crop size is determined by\n     * the distances between each joints and the center point.\n     * When the model is not confident with the four torso joint predictions, the\n     * function returns a default crop which is the full image padded to square.\n     */\n    MoveNetDetector.prototype.determineCropRegion = function (keypoints, imageHeight, imageWidth) {\n        var targetKeypoints = {};\n        for (var _i = 0, COCO_KEYPOINTS_1 = constants_2.COCO_KEYPOINTS; _i < COCO_KEYPOINTS_1.length; _i++) {\n            var key = COCO_KEYPOINTS_1[_i];\n            targetKeypoints[key] = [\n                keypoints[this.keypointIndexByName[key]].y * imageHeight,\n                keypoints[this.keypointIndexByName[key]].x * imageWidth\n            ];\n        }\n        if (this.torsoVisible(keypoints)) {\n            var centerY = (targetKeypoints['left_hip'][0] + targetKeypoints['right_hip'][0]) /\n                2;\n            var centerX = (targetKeypoints['left_hip'][1] + targetKeypoints['right_hip'][1]) /\n                2;\n            var _a = this.determineTorsoAndBodyRange(keypoints, targetKeypoints, centerY, centerX), maxTorsoYrange = _a[0], maxTorsoXrange = _a[1], maxBodyYrange = _a[2], maxBodyXrange = _a[3];\n            var cropLengthHalf = Math.max(maxTorsoXrange * 1.9, maxTorsoYrange * 1.9, maxBodyYrange * 1.2, maxBodyXrange * 1.2);\n            cropLengthHalf = Math.min(cropLengthHalf, Math.max(centerX, imageWidth - centerX, centerY, imageHeight - centerY));\n            var cropCorner = [centerY - cropLengthHalf, centerX - cropLengthHalf];\n            if (cropLengthHalf > Math.max(imageWidth, imageHeight) / 2) {\n                return this.initCropRegion(imageHeight, imageWidth);\n            }\n            else {\n                var cropLength = cropLengthHalf * 2;\n                return {\n                    yMin: cropCorner[0] / imageHeight,\n                    xMin: cropCorner[1] / imageWidth,\n                    yMax: (cropCorner[0] + cropLength) / imageHeight,\n                    xMax: (cropCorner[1] + cropLength) / imageWidth,\n                    height: (cropCorner[0] + cropLength) / imageHeight -\n                        cropCorner[0] / imageHeight,\n                    width: (cropCorner[1] + cropLength) / imageWidth -\n                        cropCorner[1] / imageWidth\n                };\n            }\n        }\n        else {\n            return this.initCropRegion(imageHeight, imageWidth);\n        }\n    };\n    /**\n     * Provides initial crop region.\n     *\n     * The function provides the initial crop region when the algorithm cannot\n     * reliably determine the crop region from the previous frame. There are two\n     * scenarios:\n     *   1) The very first frame: the function returns the best quess by cropping\n     *      a square in the middle of the image.\n     *   2) Not enough reliable keypoints detected from the previous frame: the\n     *      function pads the full image from both sides to make it a square\n     *      image.\n     */\n    MoveNetDetector.prototype.initCropRegion = function (imageHeight, imageWidth) {\n        var boxHeight, boxWidth, yMin, xMin;\n        if (!this.cropRegion) {\n            // If it is the first frame, perform a best guess by making the square\n            // crop at the image center to better utilize the image pixels and\n            // create higher chance to enter the cropping loop.\n            if (imageWidth > imageHeight) {\n                boxHeight = 1.0;\n                boxWidth = imageHeight / imageWidth;\n                yMin = 0.0;\n                xMin = (imageWidth / 2 - imageHeight / 2) / imageWidth;\n            }\n            else {\n                boxHeight = imageWidth / imageHeight;\n                boxWidth = 1.0;\n                yMin = (imageHeight / 2 - imageWidth / 2) / imageHeight;\n                xMin = 0.0;\n            }\n        }\n        else {\n            // No cropRegion was available from a previous estimatePoses() call, so\n            // run the model on the full image with padding on both sides.\n            if (imageWidth > imageHeight) {\n                boxHeight = imageWidth / imageHeight;\n                boxWidth = 1.0;\n                yMin = (imageHeight / 2 - imageWidth / 2) / imageHeight;\n                xMin = 0.0;\n            }\n            else {\n                boxHeight = 1.0;\n                boxWidth = imageHeight / imageWidth;\n                yMin = 0.0;\n                xMin = (imageWidth / 2 - imageHeight / 2) / imageWidth;\n            }\n        }\n        return {\n            yMin: yMin,\n            xMin: xMin,\n            yMax: yMin + boxHeight,\n            xMax: xMin + boxWidth,\n            height: boxHeight,\n            width: boxWidth\n        };\n    };\n    return MoveNetDetector;\n}());\n/**\n * Loads the MoveNet model instance from a checkpoint. The model to be loaded\n * is configurable using the config dictionary `ModelConfig`. Please find more\n * details in the documentation of the `ModelConfig`.\n *\n * @param config `ModelConfig` dictionary that contains parameters for\n * the MoveNet loading process. Please find more details of each parameter\n * in the documentation of the `ModelConfig` interface.\n */\nfunction load(modelConfig) {\n    if (modelConfig === void 0) { modelConfig = constants_3.MOVENET_CONFIG; }\n    return __awaiter(this, void 0, void 0, function () {\n        var config, model, modelUrl;\n        return __generator(this, function (_a) {\n            switch (_a.label) {\n                case 0:\n                    config = detector_utils_1.validateModelConfig(modelConfig);\n                    if (!config.modelUrl) return [3 /*break*/, 2];\n                    return [4 /*yield*/, tfc.loadGraphModel(config.modelUrl)];\n                case 1:\n                    model = _a.sent();\n                    return [3 /*break*/, 4];\n                case 2:\n                    modelUrl = void 0;\n                    if (config.modelType === constants_3.SINGLEPOSE_LIGHTNING) {\n                        modelUrl = constants_3.MOVENET_SINGLEPOSE_LIGHTNING_URL;\n                    }\n                    else if (config.modelType === constants_3.SINGLEPOSE_THUNDER) {\n                        modelUrl = constants_3.MOVENET_SINGLEPOSE_THUNDER_URL;\n                    }\n                    return [4 /*yield*/, tfc.loadGraphModel(modelUrl, { fromTFHub: true })];\n                case 3:\n                    model = _a.sent();\n                    _a.label = 4;\n                case 4: return [2 /*return*/, new MoveNetDetector(model, config)];\n            }\n        });\n    });\n}\nexports.load = load;\n//# sourceMappingURL=detector.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nvar SupportedModels;\n(function (SupportedModels) {\n    SupportedModels[\"MoveNet\"] = \"MoveNet\";\n    SupportedModels[\"BlazePose\"] = \"BlazePose\";\n    SupportedModels[\"PoseNet\"] = \"PoseNet\";\n})(SupportedModels = exports.SupportedModels || (exports.SupportedModels = {}));\n//# sourceMappingURL=types.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\n/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nvar constants = require(\"./constants\");\nvar types_1 = require(\"./types\");\nfunction getKeypointIndexBySide(model) {\n    switch (model) {\n        case types_1.SupportedModels.BlazePose:\n            return constants.BLAZEPOSE_KEYPOINTS_BY_SIDE;\n        case types_1.SupportedModels.PoseNet:\n        case types_1.SupportedModels.MoveNet:\n            return constants.COCO_KEYPOINTS_BY_SIDE;\n        default:\n            throw new Error(\"Model \" + model + \" is not supported.\");\n    }\n}\nexports.getKeypointIndexBySide = getKeypointIndexBySide;\nfunction getAdjacentPairs(model) {\n    switch (model) {\n        case types_1.SupportedModels.BlazePose:\n            return constants.BLAZEPOSE_CONNECTED_KEYPOINTS_PAIRS;\n        case types_1.SupportedModels.PoseNet:\n        case types_1.SupportedModels.MoveNet:\n            return constants.COCO_CONNECTED_KEYPOINTS_PAIRS;\n        default:\n            throw new Error(\"Model \" + model + \" is not supported.\");\n    }\n}\nexports.getAdjacentPairs = getAdjacentPairs;\nfunction getKeypointIndexByName(model) {\n    switch (model) {\n        case types_1.SupportedModels.BlazePose:\n            return constants.BLAZEPOSE_KEYPOINTS.reduce(function (map, name, i) {\n                map[name] = i;\n                return map;\n            }, {});\n        case types_1.SupportedModels.PoseNet:\n        case types_1.SupportedModels.MoveNet:\n            return constants.COCO_KEYPOINTS.reduce(function (map, name, i) {\n                map[name] = i;\n                return map;\n            }, {});\n        default:\n            throw new Error(\"Model \" + model + \" is not supported.\");\n    }\n}\nexports.getKeypointIndexByName = getKeypointIndexByName;\n//# sourceMappingURL=util.js.map","\n/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.SINGLEPOSE_LIGHTNING = 'SinglePose.Lightning';\nexports.SINGLEPOSE_THUNDER = 'SinglePose.Thunder';\nexports.VALID_MODELS = [exports.SINGLEPOSE_LIGHTNING, exports.SINGLEPOSE_THUNDER];\nexports.MOVENET_SINGLEPOSE_LIGHTNING_URL = 'https://tfhub.dev/google/tfjs-model/movenet/singlepose/lightning/4';\nexports.MOVENET_SINGLEPOSE_THUNDER_URL = 'https://tfhub.dev/google/tfjs-model/movenet/singlepose/thunder/4';\nexports.MOVENET_SINGLEPOSE_LIGHTNING_RESOLUTION = 192;\nexports.MOVENET_SINGLEPOSE_THUNDER_RESOLUTION = 256;\n// The default configuration for loading MoveNet.\nexports.MOVENET_CONFIG = {\n    modelType: exports.SINGLEPOSE_LIGHTNING,\n    enableSmoothing: true\n};\nexports.MOVENET_SINGLE_POSE_ESTIMATION_CONFIG = {\n    maxPoses: 1\n};\nexports.KEYPOINT_FILTER_CONFIG = {\n    frequency: 30,\n    minCutOff: 6.36,\n    beta: 636.61,\n    derivateCutOff: 4.77,\n    thresholdCutOff: 0.5,\n    thresholdBeta: 5.0\n};\nexports.CROP_FILTER_ALPHA = 0.9;\nexports.MIN_CROP_KEYPOINT_SCORE = 0.2;\n//# sourceMappingURL=constants.js.map","\n/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nvar __assign = (this && this.__assign) || function () {\n    __assign = Object.assign || function(t) {\n        for (var s, i = 1, n = arguments.length; i < n; i++) {\n            s = arguments[i];\n            for (var p in s) if (Object.prototype.hasOwnProperty.call(s, p))\n                t[p] = s[p];\n        }\n        return t;\n    };\n    return __assign.apply(this, arguments);\n};\nObject.defineProperty(exports, \"__esModule\", { value: true });\nvar constants_1 = require(\"./constants\");\nfunction validateModelConfig(modelConfig) {\n    var config = modelConfig == null ? constants_1.MOVENET_CONFIG : __assign({}, modelConfig);\n    if (!modelConfig.modelType) {\n        modelConfig.modelType = 'SinglePose.Lightning';\n    }\n    else if (constants_1.VALID_MODELS.indexOf(config.modelType) < 0) {\n        throw new Error(\"Invalid architecture \" + config.modelType + \". \" +\n            (\"Should be one of \" + constants_1.VALID_MODELS));\n    }\n    if (config.enableSmoothing == null) {\n        config.enableSmoothing = true;\n    }\n    return config;\n}\nexports.validateModelConfig = validateModelConfig;\nfunction validateEstimationConfig(estimationConfig) {\n    var config = estimationConfig == null ?\n        constants_1.MOVENET_SINGLE_POSE_ESTIMATION_CONFIG : __assign({}, estimationConfig);\n    if (!config.maxPoses) {\n        config.maxPoses = 1;\n    }\n    if (config.maxPoses <= 0 || config.maxPoses > 1) {\n        throw new Error(\"Invalid maxPoses \" + config.maxPoses + \". Should be 1.\");\n    }\n    return config;\n}\nexports.validateEstimationConfig = validateEstimationConfig;\n//# sourceMappingURL=detector_utils.js.map","\n/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nvar __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {\n    return new (P || (P = Promise))(function (resolve, reject) {\n        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }\n        function rejected(value) { try { step(generator[\"throw\"](value)); } catch (e) { reject(e); } }\n        function step(result) { result.done ? resolve(result.value) : new P(function (resolve) { resolve(result.value); }).then(fulfilled, rejected); }\n        step((generator = generator.apply(thisArg, _arguments || [])).next());\n    });\n};\nvar __generator = (this && this.__generator) || function (thisArg, body) {\n    var _ = { label: 0, sent: function() { if (t[0] & 1) throw t[1]; return t[1]; }, trys: [], ops: [] }, f, y, t, g;\n    return g = { next: verb(0), \"throw\": verb(1), \"return\": verb(2) }, typeof Symbol === \"function\" && (g[Symbol.iterator] = function() { return this; }), g;\n    function verb(n) { return function (v) { return step([n, v]); }; }\n    function step(op) {\n        if (f) throw new TypeError(\"Generator is already executing.\");\n        while (_) try {\n            if (f = 1, y && (t = op[0] & 2 ? y[\"return\"] : op[0] ? y[\"throw\"] || ((t = y[\"return\"]) && t.call(y), 0) : y.next) && !(t = t.call(y, op[1])).done) return t;\n            if (y = 0, t) op = [op[0] & 2, t.value];\n            switch (op[0]) {\n                case 0: case 1: t = op; break;\n                case 4: _.label++; return { value: op[1], done: false };\n                case 5: _.label++; y = op[1]; op = [0]; continue;\n                case 7: op = _.ops.pop(); _.trys.pop(); continue;\n                default:\n                    if (!(t = _.trys, t = t.length > 0 && t[t.length - 1]) && (op[0] === 6 || op[0] === 2)) { _ = 0; continue; }\n                    if (op[0] === 3 && (!t || (op[1] > t[0] && op[1] < t[3]))) { _.label = op[1]; break; }\n                    if (op[0] === 6 && _.label < t[1]) { _.label = t[1]; t = op; break; }\n                    if (t && _.label < t[2]) { _.label = t[2]; _.ops.push(op); break; }\n                    if (t[2]) _.ops.pop();\n                    _.trys.pop(); continue;\n            }\n            op = body.call(thisArg, _);\n        } catch (e) { op = [6, e]; y = 0; } finally { f = t = 0; }\n        if (op[0] & 5) throw op[1]; return { value: op[0] ? op[1] : void 0, done: true };\n    }\n};\nObject.defineProperty(exports, \"__esModule\", { value: true });\nvar tfconv = require(\"@tensorflow/tfjs-converter\");\nvar tf = require(\"@tensorflow/tfjs-core\");\nvar convert_image_to_tensor_1 = require(\"../calculators/convert_image_to_tensor\");\nvar image_utils_1 = require(\"../calculators/image_utils\");\nvar shift_image_value_1 = require(\"../calculators/shift_image_value\");\nvar decode_multiple_poses_1 = require(\"./calculators/decode_multiple_poses\");\nvar decode_single_pose_1 = require(\"./calculators/decode_single_pose\");\nvar flip_poses_1 = require(\"./calculators/flip_poses\");\nvar scale_poses_1 = require(\"./calculators/scale_poses\");\nvar constants_1 = require(\"./constants\");\nvar detector_utils_1 = require(\"./detector_utils\");\nvar load_utils_1 = require(\"./load_utils\");\n/**\n * PoseNet detector class.\n */\nvar PosenetDetector = /** @class */ (function () {\n    function PosenetDetector(posenetModel, config) {\n        this.posenetModel = posenetModel;\n        // validate params.\n        var inputShape = this.posenetModel.inputs[0].shape;\n        tf.util.assert((inputShape[1] === -1) && (inputShape[2] === -1), function () { return \"Input shape [\" + inputShape[1] + \", \" + inputShape[2] + \"] \" +\n            \"must both be equal to or -1\"; });\n        var validInputResolution = load_utils_1.getValidInputResolutionDimensions(config.inputResolution, config.outputStride);\n        detector_utils_1.assertValidOutputStride(config.outputStride);\n        detector_utils_1.assertValidResolution(validInputResolution, config.outputStride);\n        this.inputResolution = validInputResolution;\n        this.outputStride = config.outputStride;\n        this.architecture = config.architecture;\n    }\n    /**\n     * Estimates poses for an image or video frame.\n     *\n     * This does standard ImageNet pre-processing before inferring through the\n     * model. The image should pixels should have values [0-255]. It returns a\n     * single pose or multiple poses based on the maxPose parameter from the\n     * `config`.\n     *\n     * @param image\n     * ImageData|HTMLImageElement|HTMLCanvasElement|HTMLVideoElement The input\n     * image to feed through the network.\n     *\n     * @param config\n     *       maxPoses: Optional. Max number of poses to estimate.\n     *       When maxPoses = 1, a single pose is detected, it is usually much more\n     *       efficient than maxPoses > 1. When maxPoses > 1, multiple poses are\n     *       detected.\n     *\n     *       flipHorizontal: Optional. Default to false. When image data comes\n     *       from camera, the result has to flip horizontally.\n     *\n     * @return An array of `Pose`s.\n     */\n    PosenetDetector.prototype.estimatePoses = function (image, estimationConfig) {\n        if (estimationConfig === void 0) { estimationConfig = constants_1.SINGLE_PERSON_ESTIMATION_CONFIG; }\n        return __awaiter(this, void 0, void 0, function () {\n            var config, _a, imageTensor, padding, imageValueShifted, results, offsets, heatmap, displacementFwd, displacementBwd, heatmapScores, poses, pose, imageSize, scaledPoses;\n            return __generator(this, function (_b) {\n                switch (_b.label) {\n                    case 0:\n                        config = detector_utils_1.validateEstimationConfig(estimationConfig);\n                        if (image == null) {\n                            return [2 /*return*/, []];\n                        }\n                        this.maxPoses = config.maxPoses;\n                        _a = convert_image_to_tensor_1.convertImageToTensor(image, { inputResolution: this.inputResolution, keepAspectRatio: true }), imageTensor = _a.imageTensor, padding = _a.padding;\n                        imageValueShifted = this.architecture === 'ResNet50' ?\n                            tf.add(imageTensor, constants_1.RESNET_MEAN) :\n                            shift_image_value_1.shiftImageValue(imageTensor, [-1, 1]);\n                        results = this.posenetModel.predict(imageValueShifted);\n                        if (this.architecture === 'ResNet50') {\n                            offsets = tf.squeeze(results[2], [0]);\n                            heatmap = tf.squeeze(results[3], [0]);\n                            displacementFwd = tf.squeeze(results[0], [0]);\n                            displacementBwd = tf.squeeze(results[1], [0]);\n                        }\n                        else {\n                            offsets = tf.squeeze(results[0], [0]);\n                            heatmap = tf.squeeze(results[1], [0]);\n                            displacementFwd = tf.squeeze(results[2], [0]);\n                            displacementBwd = tf.squeeze(results[3], [0]);\n                        }\n                        heatmapScores = tf.sigmoid(heatmap);\n                        if (!(this.maxPoses === 1)) return [3 /*break*/, 2];\n                        return [4 /*yield*/, decode_single_pose_1.decodeSinglePose(heatmapScores, offsets, this.outputStride)];\n                    case 1:\n                        pose = _b.sent();\n                        poses = [pose];\n                        return [3 /*break*/, 4];\n                    case 2: return [4 /*yield*/, decode_multiple_poses_1.decodeMultiplePoses(heatmapScores, offsets, displacementFwd, displacementBwd, this.outputStride, this.maxPoses, config.scoreThreshold, config.nmsRadius)];\n                    case 3:\n                        poses = _b.sent();\n                        _b.label = 4;\n                    case 4:\n                        imageSize = image_utils_1.getImageSize(image);\n                        scaledPoses = scale_poses_1.scalePoses(poses, imageSize, this.inputResolution, padding);\n                        if (config.flipHorizontal) {\n                            scaledPoses = flip_poses_1.flipPosesHorizontal(scaledPoses, imageSize);\n                        }\n                        imageTensor.dispose();\n                        imageValueShifted.dispose();\n                        tf.dispose(results);\n                        offsets.dispose();\n                        heatmap.dispose();\n                        displacementFwd.dispose();\n                        displacementBwd.dispose();\n                        heatmapScores.dispose();\n                        return [2 /*return*/, scaledPoses];\n                }\n            });\n        });\n    };\n    PosenetDetector.prototype.dispose = function () {\n        this.posenetModel.dispose();\n    };\n    PosenetDetector.prototype.reset = function () {\n        // No-op. There's no global state.\n    };\n    return PosenetDetector;\n}());\n/**\n * Loads the PoseNet model instance from a checkpoint, with the ResNet\n * or MobileNet architecture. The model to be loaded is configurable using the\n * config dictionary ModelConfig. Please find more details in the\n * documentation of the ModelConfig.\n *\n * @param config ModelConfig dictionary that contains parameters for\n * the PoseNet loading process. Please find more details of each parameters\n * in the documentation of the ModelConfig interface. The predefined\n * `MOBILENET_V1_CONFIG` and `RESNET_CONFIG` can also be used as references\n * for defining your customized config.\n */\nfunction load(modelConfig) {\n    if (modelConfig === void 0) { modelConfig = constants_1.MOBILENET_V1_CONFIG; }\n    return __awaiter(this, void 0, void 0, function () {\n        var config, defaultUrl_1, model_1, defaultUrl, model;\n        return __generator(this, function (_a) {\n            switch (_a.label) {\n                case 0:\n                    config = detector_utils_1.validateModelConfig(modelConfig);\n                    if (!(config.architecture === 'ResNet50')) return [3 /*break*/, 2];\n                    defaultUrl_1 = load_utils_1.resNet50Checkpoint(config.outputStride, config.quantBytes);\n                    return [4 /*yield*/, tfconv.loadGraphModel(config.modelUrl || defaultUrl_1)];\n                case 1:\n                    model_1 = _a.sent();\n                    return [2 /*return*/, new PosenetDetector(model_1, config)];\n                case 2:\n                    defaultUrl = load_utils_1.mobileNetCheckpoint(config.outputStride, config.multiplier, config.quantBytes);\n                    return [4 /*yield*/, tfconv.loadGraphModel(config.modelUrl || defaultUrl)];\n                case 3:\n                    model = _a.sent();\n                    return [2 /*return*/, new PosenetDetector(model, config)];\n            }\n        });\n    });\n}\nexports.load = load;\n//# sourceMappingURL=detector.js.map","\n/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nvar __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {\n    return new (P || (P = Promise))(function (resolve, reject) {\n        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }\n        function rejected(value) { try { step(generator[\"throw\"](value)); } catch (e) { reject(e); } }\n        function step(result) { result.done ? resolve(result.value) : new P(function (resolve) { resolve(result.value); }).then(fulfilled, rejected); }\n        step((generator = generator.apply(thisArg, _arguments || [])).next());\n    });\n};\nvar __generator = (this && this.__generator) || function (thisArg, body) {\n    var _ = { label: 0, sent: function() { if (t[0] & 1) throw t[1]; return t[1]; }, trys: [], ops: [] }, f, y, t, g;\n    return g = { next: verb(0), \"throw\": verb(1), \"return\": verb(2) }, typeof Symbol === \"function\" && (g[Symbol.iterator] = function() { return this; }), g;\n    function verb(n) { return function (v) { return step([n, v]); }; }\n    function step(op) {\n        if (f) throw new TypeError(\"Generator is already executing.\");\n        while (_) try {\n            if (f = 1, y && (t = op[0] & 2 ? y[\"return\"] : op[0] ? y[\"throw\"] || ((t = y[\"return\"]) && t.call(y), 0) : y.next) && !(t = t.call(y, op[1])).done) return t;\n            if (y = 0, t) op = [op[0] & 2, t.value];\n            switch (op[0]) {\n                case 0: case 1: t = op; break;\n                case 4: _.label++; return { value: op[1], done: false };\n                case 5: _.label++; y = op[1]; op = [0]; continue;\n                case 7: op = _.ops.pop(); _.trys.pop(); continue;\n                default:\n                    if (!(t = _.trys, t = t.length > 0 && t[t.length - 1]) && (op[0] === 6 || op[0] === 2)) { _ = 0; continue; }\n                    if (op[0] === 3 && (!t || (op[1] > t[0] && op[1] < t[3]))) { _.label = op[1]; break; }\n                    if (op[0] === 6 && _.label < t[1]) { _.label = t[1]; t = op; break; }\n                    if (t && _.label < t[2]) { _.label = t[2]; _.ops.push(op); break; }\n                    if (t[2]) _.ops.pop();\n                    _.trys.pop(); continue;\n            }\n            op = body.call(thisArg, _);\n        } catch (e) { op = [6, e]; y = 0; } finally { f = t = 0; }\n        if (op[0] & 5) throw op[1]; return { value: op[0] ? op[1] : void 0, done: true };\n    }\n};\nObject.defineProperty(exports, \"__esModule\", { value: true });\nvar constants_1 = require(\"../constants\");\nvar build_part_with_score_queue_1 = require(\"./build_part_with_score_queue\");\nvar decode_multiple_poses_util_1 = require(\"./decode_multiple_poses_util\");\n/**\n * Detects multiple poses and finds their parts from part scores and\n * displacement vectors. It returns up to `maxDetections` object instance\n * detections in decreasing root score order. It works as follows: We first\n * create a priority queue with local part score maxima above\n * `scoreThreshold`, considering all parts at the same time. Then we\n * iteratively pull the top  element of the queue (in decreasing score order)\n * and treat it as a root candidate for a new object instance. To avoid\n * duplicate detections, we reject the root candidate if it is within a disk\n * of `nmsRadius` pixels from the corresponding part of a previously detected\n * instance, which is a form of part-based non-maximum suppression (NMS). If\n * the root candidate passes the NMS check, we start a new object instance\n * detection, treating the corresponding part as root and finding the\n * positions of the remaining parts by following the displacement vectors\n * along the tree-structured part graph. We assign to the newly detected\n * instance a score equal to the sum of scores of its parts which have not\n * been claimed by a previous instance (i.e., those at least `nmsRadius`\n * pixels away from the corresponding part of all previously detected\n * instances), divided by the total number of parts `numParts`.\n *\n * @param heatmapScores 3-D tensor with shape `[height, width, numParts]`.\n * The value of heatmapScores[y, x, k]` is the score of placing the `k`-th\n * object part at position `(y, x)`.\n *\n * @param offsets 3-D tensor with shape `[height, width, numParts * 2]`.\n * The value of [offsets[y, x, k], offsets[y, x, k + numParts]]` is the\n * short range offset vector of the `k`-th  object part at heatmap\n * position `(y, x)`.\n *\n * @param displacementsFwd 3-D tensor of shape\n * `[height, width, 2 * num_edges]`, where `num_edges = num_parts - 1` is the\n * number of edges (parent-child pairs) in the tree. It contains the forward\n * displacements between consecutive part from the root towards the leaves.\n *\n * @param displacementsBwd 3-D tensor of shape\n * `[height, width, 2 * num_edges]`, where `num_edges = num_parts - 1` is the\n * number of edges (parent-child pairs) in the tree. It contains the backward\n * displacements between consecutive part from the root towards the leaves.\n *\n * @param outputStride The output stride that was used when feed-forwarding\n * through the PoseNet model.  Must be 32, 16, or 8.\n *\n * @param maxPoseDetections Maximum number of returned instance detections per\n * image.\n *\n * @param scoreThreshold Only return instance detections that have root part\n * score greater or equal to this value. Defaults to 0.5.\n *\n * @param nmsRadius Non-maximum suppression part distance. It needs to be\n * strictly positive. Two parts suppress each other if they are less than\n * `nmsRadius` pixels away. Defaults to 20.\n *\n * @return An array of poses and their scores, each containing keypoints and\n * the corresponding keypoint scores.\n */\nfunction decodeMultiplePoses(heatmapScores, offsets, displacementFwd, displacementBwd, outputStride, maxPoseDetections, scoreThreshold, nmsRadius) {\n    if (scoreThreshold === void 0) { scoreThreshold = 0.5; }\n    if (nmsRadius === void 0) { nmsRadius = 20; }\n    return __awaiter(this, void 0, void 0, function () {\n        var _a, scoresBuffer, offsetsBuffer, displacementsFwdBuffer, displacementsBwdBuffer, poses, queue, squaredNmsRadius, root, rootImageCoords, keypoints, score;\n        return __generator(this, function (_b) {\n            switch (_b.label) {\n                case 0: return [4 /*yield*/, decode_multiple_poses_util_1.toTensorBuffers3D([heatmapScores, offsets, displacementFwd, displacementBwd])];\n                case 1:\n                    _a = _b.sent(), scoresBuffer = _a[0], offsetsBuffer = _a[1], displacementsFwdBuffer = _a[2], displacementsBwdBuffer = _a[3];\n                    poses = [];\n                    queue = build_part_with_score_queue_1.buildPartWithScoreQueue(scoreThreshold, constants_1.K_LOCAL_MAXIMUM_RADIUS, scoresBuffer);\n                    squaredNmsRadius = nmsRadius * nmsRadius;\n                    // Generate at most maxDetections object instances per image in\n                    // decreasing root part score order.\n                    while (poses.length < maxPoseDetections && !queue.empty()) {\n                        root = queue.dequeue();\n                        rootImageCoords = decode_multiple_poses_util_1.getImageCoords(root.part, outputStride, offsetsBuffer);\n                        if (decode_multiple_poses_util_1.withinNmsRadiusOfCorrespondingPoint(poses, squaredNmsRadius, rootImageCoords, root.part.id)) {\n                            continue;\n                        }\n                        keypoints = decode_multiple_poses_util_1.decodePose(root, scoresBuffer, offsetsBuffer, outputStride, displacementsFwdBuffer, displacementsBwdBuffer);\n                        score = decode_multiple_poses_util_1.getInstanceScore(poses, squaredNmsRadius, keypoints);\n                        poses.push({ keypoints: keypoints, score: score });\n                    }\n                    return [2 /*return*/, poses];\n            }\n        });\n    });\n}\nexports.decodeMultiplePoses = decodeMultiplePoses;\n//# sourceMappingURL=decode_multiple_poses.js.map","\n/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nObject.defineProperty(exports, \"__esModule\", { value: true });\n// The default configuration for loading MobileNetV1 based PoseNet.\n//\n// (And for references, the default configuration for loading ResNet\n// based PoseNet is also included).\n//\n// ```\n// const RESNET_CONFIG = {\n//   architecture: 'ResNet50',\n//   outputStride: 32,\n//   quantBytes: 2,\n// } as ModelConfig;\n// ```\nexports.MOBILENET_V1_CONFIG = {\n    architecture: 'MobileNetV1',\n    outputStride: 16,\n    multiplier: 0.75,\n    inputResolution: { height: 257, width: 257 }\n};\nexports.VALID_ARCHITECTURE = ['MobileNetV1', 'ResNet50'];\nexports.VALID_STRIDE = {\n    'MobileNetV1': [8, 16],\n    'ResNet50': [16]\n};\nexports.VALID_OUTPUT_STRIDES = [8, 16, 32];\nexports.VALID_MULTIPLIER = {\n    'MobileNetV1': [0.50, 0.75, 1.0],\n    'ResNet50': [1.0]\n};\nexports.VALID_QUANT_BYTES = [1, 2, 4];\nexports.SINGLE_PERSON_ESTIMATION_CONFIG = {\n    maxPoses: 1,\n    flipHorizontal: false\n};\nexports.MULTI_PERSON_ESTIMATION_CONFIG = {\n    maxPoses: 5,\n    flipHorizontal: false,\n    scoreThreshold: 0.5,\n    nmsRadius: 20\n};\nexports.RESNET_MEAN = [-123.15, -115.90, -103.06];\n// A point (y, x) is considered as root part candidate if its score is a\n// maximum in a window |y - y'| <= kLocalMaximumRadius, |x - x'| <=\n// kLocalMaximumRadius.\nexports.K_LOCAL_MAXIMUM_RADIUS = 1;\nexports.NUM_KEYPOINTS = 17;\n/*\n * Define the skeleton. This defines the parent->child relationships of our\n * tree. Arbitrarily this defines the nose as the root of the tree, however\n * since we will infer the displacement for both parent->child and\n * child->parent, we can define the tree root as any node.\n */\nexports.POSE_CHAIN = [\n    ['nose', 'left_eye'], ['left_eye', 'left_ear'], ['nose', 'right_eye'],\n    ['right_eye', 'right_ear'], ['nose', 'left_shoulder'],\n    ['left_shoulder', 'left_elbow'], ['left_elbow', 'left_wrist'],\n    ['left_shoulder', 'left_hip'], ['left_hip', 'left_knee'],\n    ['left_knee', 'left_ankle'], ['nose', 'right_shoulder'],\n    ['right_shoulder', 'right_elbow'], ['right_elbow', 'right_wrist'],\n    ['right_shoulder', 'right_hip'], ['right_hip', 'right_knee'],\n    ['right_knee', 'right_ankle']\n];\n//# sourceMappingURL=constants.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nvar max_heap_1 = require(\"./max_heap\");\nfunction scoreIsMaximumInLocalWindow(keypointId, score, heatmapY, heatmapX, localMaximumRadius, scores) {\n    var _a = scores.shape, height = _a[0], width = _a[1];\n    var localMaximum = true;\n    var yStart = Math.max(heatmapY - localMaximumRadius, 0);\n    var yEnd = Math.min(heatmapY + localMaximumRadius + 1, height);\n    for (var yCurrent = yStart; yCurrent < yEnd; ++yCurrent) {\n        var xStart = Math.max(heatmapX - localMaximumRadius, 0);\n        var xEnd = Math.min(heatmapX + localMaximumRadius + 1, width);\n        for (var xCurrent = xStart; xCurrent < xEnd; ++xCurrent) {\n            if (scores.get(yCurrent, xCurrent, keypointId) > score) {\n                localMaximum = false;\n                break;\n            }\n        }\n        if (!localMaximum) {\n            break;\n        }\n    }\n    return localMaximum;\n}\n/**\n * Builds a priority queue with part candidate positions for a specific image in\n * the batch. For this we find all local maxima in the score maps with score\n * values above a threshold. We create a single priority queue across all parts.\n */\nfunction buildPartWithScoreQueue(scoreThreshold, localMaximumRadius, scores) {\n    var _a = scores.shape, height = _a[0], width = _a[1], numKeypoints = _a[2];\n    var queue = new max_heap_1.MaxHeap(height * width * numKeypoints, function (_a) {\n        var score = _a.score;\n        return score;\n    });\n    for (var heatmapY = 0; heatmapY < height; ++heatmapY) {\n        for (var heatmapX = 0; heatmapX < width; ++heatmapX) {\n            for (var keypointId = 0; keypointId < numKeypoints; ++keypointId) {\n                var score = scores.get(heatmapY, heatmapX, keypointId);\n                // Only consider parts with score greater or equal to threshold as\n                // root candidates.\n                if (score < scoreThreshold) {\n                    continue;\n                }\n                // Only consider keypoints whose score is maximum in a local window.\n                if (scoreIsMaximumInLocalWindow(keypointId, score, heatmapY, heatmapX, localMaximumRadius, scores)) {\n                    queue.enqueue({ score: score, part: { heatmapY: heatmapY, heatmapX: heatmapX, id: keypointId } });\n                }\n            }\n        }\n    }\n    return queue;\n}\nexports.buildPartWithScoreQueue = buildPartWithScoreQueue;\n//# sourceMappingURL=build_part_with_score_queue.js.map","\n/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nObject.defineProperty(exports, \"__esModule\", { value: true });\n// algorithm based on Coursera Lecture from Algorithms, Part 1:\n// https://www.coursera.org/learn/algorithms-part1/lecture/ZjoSM/heapsort\nfunction half(k) {\n    return Math.floor(k / 2);\n}\nvar MaxHeap = /** @class */ (function () {\n    function MaxHeap(maxSize, getElementValue) {\n        this.priorityQueue = new Array(maxSize);\n        this.numberOfElements = -1;\n        this.getElementValue = getElementValue;\n    }\n    MaxHeap.prototype.enqueue = function (x) {\n        this.priorityQueue[++this.numberOfElements] = x;\n        this.swim(this.numberOfElements);\n    };\n    MaxHeap.prototype.dequeue = function () {\n        var max = this.priorityQueue[0];\n        this.exchange(0, this.numberOfElements--);\n        this.sink(0);\n        this.priorityQueue[this.numberOfElements + 1] = null;\n        return max;\n    };\n    MaxHeap.prototype.empty = function () {\n        return this.numberOfElements === -1;\n    };\n    MaxHeap.prototype.size = function () {\n        return this.numberOfElements + 1;\n    };\n    MaxHeap.prototype.all = function () {\n        return this.priorityQueue.slice(0, this.numberOfElements + 1);\n    };\n    MaxHeap.prototype.max = function () {\n        return this.priorityQueue[0];\n    };\n    MaxHeap.prototype.swim = function (k) {\n        while (k > 0 && this.less(half(k), k)) {\n            this.exchange(k, half(k));\n            k = half(k);\n        }\n    };\n    MaxHeap.prototype.sink = function (k) {\n        while (2 * k <= this.numberOfElements) {\n            var j = 2 * k;\n            if (j < this.numberOfElements && this.less(j, j + 1)) {\n                j++;\n            }\n            if (!this.less(k, j)) {\n                break;\n            }\n            this.exchange(k, j);\n            k = j;\n        }\n    };\n    MaxHeap.prototype.getValueAt = function (i) {\n        return this.getElementValue(this.priorityQueue[i]);\n    };\n    MaxHeap.prototype.less = function (i, j) {\n        return this.getValueAt(i) < this.getValueAt(j);\n    };\n    MaxHeap.prototype.exchange = function (i, j) {\n        var t = this.priorityQueue[i];\n        this.priorityQueue[i] = this.priorityQueue[j];\n        this.priorityQueue[j] = t;\n    };\n    return MaxHeap;\n}());\nexports.MaxHeap = MaxHeap;\n//# sourceMappingURL=max_heap.js.map","\nvar __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {\n    return new (P || (P = Promise))(function (resolve, reject) {\n        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }\n        function rejected(value) { try { step(generator[\"throw\"](value)); } catch (e) { reject(e); } }\n        function step(result) { result.done ? resolve(result.value) : new P(function (resolve) { resolve(result.value); }).then(fulfilled, rejected); }\n        step((generator = generator.apply(thisArg, _arguments || [])).next());\n    });\n};\nvar __generator = (this && this.__generator) || function (thisArg, body) {\n    var _ = { label: 0, sent: function() { if (t[0] & 1) throw t[1]; return t[1]; }, trys: [], ops: [] }, f, y, t, g;\n    return g = { next: verb(0), \"throw\": verb(1), \"return\": verb(2) }, typeof Symbol === \"function\" && (g[Symbol.iterator] = function() { return this; }), g;\n    function verb(n) { return function (v) { return step([n, v]); }; }\n    function step(op) {\n        if (f) throw new TypeError(\"Generator is already executing.\");\n        while (_) try {\n            if (f = 1, y && (t = op[0] & 2 ? y[\"return\"] : op[0] ? y[\"throw\"] || ((t = y[\"return\"]) && t.call(y), 0) : y.next) && !(t = t.call(y, op[1])).done) return t;\n            if (y = 0, t) op = [op[0] & 2, t.value];\n            switch (op[0]) {\n                case 0: case 1: t = op; break;\n                case 4: _.label++; return { value: op[1], done: false };\n                case 5: _.label++; y = op[1]; op = [0]; continue;\n                case 7: op = _.ops.pop(); _.trys.pop(); continue;\n                default:\n                    if (!(t = _.trys, t = t.length > 0 && t[t.length - 1]) && (op[0] === 6 || op[0] === 2)) { _ = 0; continue; }\n                    if (op[0] === 3 && (!t || (op[1] > t[0] && op[1] < t[3]))) { _.label = op[1]; break; }\n                    if (op[0] === 6 && _.label < t[1]) { _.label = t[1]; t = op; break; }\n                    if (t && _.label < t[2]) { _.label = t[2]; _.ops.push(op); break; }\n                    if (t[2]) _.ops.pop();\n                    _.trys.pop(); continue;\n            }\n            op = body.call(thisArg, _);\n        } catch (e) { op = [6, e]; y = 0; } finally { f = t = 0; }\n        if (op[0] & 5) throw op[1]; return { value: op[0] ? op[1] : void 0, done: true };\n    }\n};\nObject.defineProperty(exports, \"__esModule\", { value: true });\nvar constants_1 = require(\"../../constants\");\nvar constants_2 = require(\"../constants\");\nfunction toTensorBuffers3D(tensors) {\n    return __awaiter(this, void 0, void 0, function () {\n        return __generator(this, function (_a) {\n            return [2 /*return*/, Promise.all(tensors.map(function (tensor) { return tensor.buffer(); }))];\n        });\n    });\n}\nexports.toTensorBuffers3D = toTensorBuffers3D;\nfunction getOffsetPoint(y, x, keypoint, offsets) {\n    return {\n        y: offsets.get(y, x, keypoint),\n        x: offsets.get(y, x, keypoint + constants_2.NUM_KEYPOINTS)\n    };\n}\nexports.getOffsetPoint = getOffsetPoint;\nfunction getImageCoords(part, outputStride, offsets) {\n    var heatmapY = part.heatmapY, heatmapX = part.heatmapX, keypoint = part.id;\n    var _a = getOffsetPoint(heatmapY, heatmapX, keypoint, offsets), y = _a.y, x = _a.x;\n    return {\n        x: part.heatmapX * outputStride + x,\n        y: part.heatmapY * outputStride + y\n    };\n}\nexports.getImageCoords = getImageCoords;\nfunction squaredDistance(y1, x1, y2, x2) {\n    var dy = y2 - y1;\n    var dx = x2 - x1;\n    return dy * dy + dx * dx;\n}\nexports.squaredDistance = squaredDistance;\nfunction withinNmsRadiusOfCorrespondingPoint(poses, squaredNmsRadius, _a, keypointId) {\n    var x = _a.x, y = _a.y;\n    return poses.some(function (_a) {\n        var keypoints = _a.keypoints;\n        return squaredDistance(y, x, keypoints[keypointId].y, keypoints[keypointId].x) <=\n            squaredNmsRadius;\n    });\n}\nexports.withinNmsRadiusOfCorrespondingPoint = withinNmsRadiusOfCorrespondingPoint;\nvar partIds = \n// tslint:disable-next-line: no-unnecessary-type-assertion\nconstants_1.COCO_KEYPOINTS.reduce(function (result, jointName, i) {\n    result[jointName] = i;\n    return result;\n}, {});\nvar parentChildrenTuples = constants_2.POSE_CHAIN.map(function (_a) {\n    var parentJoinName = _a[0], childJoinName = _a[1];\n    return ([partIds[parentJoinName], partIds[childJoinName]]);\n});\nvar parentToChildEdges = parentChildrenTuples.map(function (_a) {\n    var childJointId = _a[1];\n    return childJointId;\n});\nvar childToParentEdges = parentChildrenTuples.map(function (_a) {\n    var parentJointId = _a[0];\n    return parentJointId;\n});\nfunction clamp(a, min, max) {\n    if (a < min) {\n        return min;\n    }\n    if (a > max) {\n        return max;\n    }\n    return a;\n}\nfunction getStridedIndexNearPoint(point, outputStride, height, width) {\n    return {\n        y: clamp(Math.round(point.y / outputStride), 0, height - 1),\n        x: clamp(Math.round(point.x / outputStride), 0, width - 1)\n    };\n}\nfunction getDisplacement(edgeId, point, displacements) {\n    var numEdges = displacements.shape[2] / 2;\n    return {\n        y: displacements.get(point.y, point.x, edgeId),\n        x: displacements.get(point.y, point.x, numEdges + edgeId)\n    };\n}\nfunction addVectors(a, b) {\n    return { x: a.x + b.x, y: a.y + b.y };\n}\nexports.addVectors = addVectors;\n/**\n * We get a new keypoint along the `edgeId` for the pose instance, assuming\n * that the position of the `idSource` part is already known. For this, we\n * follow the displacement vector from the source to target part (stored in\n * the `i`-t channel of the displacement tensor). The displaced keypoint\n * vector is refined using the offset vector by `offsetRefineStep` times.\n */\nfunction traverseToTargetKeypoint(edgeId, sourceKeypoint, targetKeypointId, scoresBuffer, offsets, outputStride, displacements, offsetRefineStep) {\n    if (offsetRefineStep === void 0) { offsetRefineStep = 2; }\n    var _a = scoresBuffer.shape, height = _a[0], width = _a[1];\n    var point = { y: sourceKeypoint.y, x: sourceKeypoint.x };\n    // Nearest neighbor interpolation for the source->target displacements.\n    var sourceKeypointIndices = getStridedIndexNearPoint(point, outputStride, height, width);\n    var displacement = getDisplacement(edgeId, sourceKeypointIndices, displacements);\n    var displacedPoint = addVectors(point, displacement);\n    var targetKeypoint = displacedPoint;\n    for (var i = 0; i < offsetRefineStep; i++) {\n        var targetKeypointIndices = getStridedIndexNearPoint(targetKeypoint, outputStride, height, width);\n        var offsetPoint = getOffsetPoint(targetKeypointIndices.y, targetKeypointIndices.x, targetKeypointId, offsets);\n        targetKeypoint = addVectors({\n            x: targetKeypointIndices.x * outputStride,\n            y: targetKeypointIndices.y * outputStride\n        }, { x: offsetPoint.x, y: offsetPoint.y });\n    }\n    var targetKeyPointIndices = getStridedIndexNearPoint(targetKeypoint, outputStride, height, width);\n    var score = scoresBuffer.get(targetKeyPointIndices.y, targetKeyPointIndices.x, targetKeypointId);\n    return {\n        y: targetKeypoint.y,\n        x: targetKeypoint.x,\n        name: constants_1.COCO_KEYPOINTS[targetKeypointId],\n        score: score\n    };\n}\n/**\n * Follows the displacement fields to decode the full pose of the object\n * instance given the position of a part that acts as root.\n *\n * @return An array of decoded keypoints and their scores for a single pose\n */\nfunction decodePose(root, scores, offsets, outputStride, displacementsFwd, displacementsBwd) {\n    var numParts = scores.shape[2];\n    var numEdges = parentToChildEdges.length;\n    var instanceKeypoints = new Array(numParts);\n    // Start a new detection instance at the position of the root.\n    var rootPart = root.part, rootScore = root.score;\n    var rootPoint = getImageCoords(rootPart, outputStride, offsets);\n    instanceKeypoints[rootPart.id] = {\n        score: rootScore,\n        name: constants_1.COCO_KEYPOINTS[rootPart.id],\n        y: rootPoint.y,\n        x: rootPoint.x\n    };\n    // Decode the part positions upwards in the tree, following the backward\n    // displacements.\n    for (var edge = numEdges - 1; edge >= 0; --edge) {\n        var sourceKeypointId = parentToChildEdges[edge];\n        var targetKeypointId = childToParentEdges[edge];\n        if (instanceKeypoints[sourceKeypointId] &&\n            !instanceKeypoints[targetKeypointId]) {\n            instanceKeypoints[targetKeypointId] = traverseToTargetKeypoint(edge, instanceKeypoints[sourceKeypointId], targetKeypointId, scores, offsets, outputStride, displacementsBwd);\n        }\n    }\n    // Decode the part positions downwards in the tree, following the forward\n    // displacements.\n    for (var edge = 0; edge < numEdges; ++edge) {\n        var sourceKeypointId = childToParentEdges[edge];\n        var targetKeypointId = parentToChildEdges[edge];\n        if (instanceKeypoints[sourceKeypointId] &&\n            !instanceKeypoints[targetKeypointId]) {\n            instanceKeypoints[targetKeypointId] = traverseToTargetKeypoint(edge, instanceKeypoints[sourceKeypointId], targetKeypointId, scores, offsets, outputStride, displacementsFwd);\n        }\n    }\n    return instanceKeypoints;\n}\nexports.decodePose = decodePose;\n/* Score the newly proposed object instance without taking into account\n * the scores of the parts that overlap with any previously detected\n * instance.\n */\nfunction getInstanceScore(existingPoses, squaredNmsRadius, instanceKeypoints) {\n    var notOverlappedKeypointScores = instanceKeypoints.reduce(function (result, _a, keypointId) {\n        var y = _a.y, x = _a.x, score = _a.score;\n        if (!withinNmsRadiusOfCorrespondingPoint(existingPoses, squaredNmsRadius, { y: y, x: x }, keypointId)) {\n            result += score;\n        }\n        return result;\n    }, 0.0);\n    return notOverlappedKeypointScores /= instanceKeypoints.length;\n}\nexports.getInstanceScore = getInstanceScore;\n//# sourceMappingURL=decode_multiple_poses_util.js.map","\n/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nvar __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {\n    return new (P || (P = Promise))(function (resolve, reject) {\n        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }\n        function rejected(value) { try { step(generator[\"throw\"](value)); } catch (e) { reject(e); } }\n        function step(result) { result.done ? resolve(result.value) : new P(function (resolve) { resolve(result.value); }).then(fulfilled, rejected); }\n        step((generator = generator.apply(thisArg, _arguments || [])).next());\n    });\n};\nvar __generator = (this && this.__generator) || function (thisArg, body) {\n    var _ = { label: 0, sent: function() { if (t[0] & 1) throw t[1]; return t[1]; }, trys: [], ops: [] }, f, y, t, g;\n    return g = { next: verb(0), \"throw\": verb(1), \"return\": verb(2) }, typeof Symbol === \"function\" && (g[Symbol.iterator] = function() { return this; }), g;\n    function verb(n) { return function (v) { return step([n, v]); }; }\n    function step(op) {\n        if (f) throw new TypeError(\"Generator is already executing.\");\n        while (_) try {\n            if (f = 1, y && (t = op[0] & 2 ? y[\"return\"] : op[0] ? y[\"throw\"] || ((t = y[\"return\"]) && t.call(y), 0) : y.next) && !(t = t.call(y, op[1])).done) return t;\n            if (y = 0, t) op = [op[0] & 2, t.value];\n            switch (op[0]) {\n                case 0: case 1: t = op; break;\n                case 4: _.label++; return { value: op[1], done: false };\n                case 5: _.label++; y = op[1]; op = [0]; continue;\n                case 7: op = _.ops.pop(); _.trys.pop(); continue;\n                default:\n                    if (!(t = _.trys, t = t.length > 0 && t[t.length - 1]) && (op[0] === 6 || op[0] === 2)) { _ = 0; continue; }\n                    if (op[0] === 3 && (!t || (op[1] > t[0] && op[1] < t[3]))) { _.label = op[1]; break; }\n                    if (op[0] === 6 && _.label < t[1]) { _.label = t[1]; t = op; break; }\n                    if (t && _.label < t[2]) { _.label = t[2]; _.ops.push(op); break; }\n                    if (t[2]) _.ops.pop();\n                    _.trys.pop(); continue;\n            }\n            op = body.call(thisArg, _);\n        } catch (e) { op = [6, e]; y = 0; } finally { f = t = 0; }\n        if (op[0] & 5) throw op[1]; return { value: op[0] ? op[1] : void 0, done: true };\n    }\n};\nObject.defineProperty(exports, \"__esModule\", { value: true });\nvar constants_1 = require(\"../../constants\");\nvar decode_single_pose_util_1 = require(\"./decode_single_pose_util\");\n/**\n * Detects a single pose and finds its parts from part scores and offset\n * vectors. It returns a single pose detection. It works as follows:\n * argmax2d is done on the scores to get the y and x index in the heatmap\n * with the highest score for each part, which is essentially where the\n * part is most likely to exist. This produces a tensor of size 17x2, with\n * each row being the y and x index in the heatmap for each keypoint.\n * The offset vector for each part is retrieved by getting the\n * y and x from the offsets corresponding to the y and x index in the\n * heatmap for that part. This produces a tensor of size 17x2, with each\n * row being the offset vector for the corresponding keypoint.\n * To get the keypoint, each parts heatmap y and x are multiplied\n * by the output stride then added to their corresponding offset vector,\n * which is in the same scale as the original image.\n *\n * @param heatmapScores 3-D tensor with shape `[height, width, numParts]`.\n * The value of heatmapScores[y, x, k]` is the score of placing the `k`-th\n * object part at position `(y, x)`.\n *\n * @param offsets 3-D tensor with shape `[height, width, numParts * 2]`.\n * The value of [offsets[y, x, k], offsets[y, x, k + numParts]]` is the\n * short range offset vector of the `k`-th  object part at heatmap\n * position `(y, x)`.\n *\n * @param outputStride The output stride that was used when feed-forwarding\n * through the PoseNet model.  Must be 32, 16, or 8.\n *\n * @return A promise that resolves with single pose with a confidence score,\n * which contains an array of keypoints indexed by part id, each with a score\n * and position.\n */\nfunction decodeSinglePose(heatmapScores, offsets, outputStride) {\n    return __awaiter(this, void 0, void 0, function () {\n        var totalScore, heatmapValues, allTensorBuffers, scoresBuffer, offsetsBuffer, heatmapValuesBuffer, offsetPoints, offsetPointsBuffer, keypointConfidence, keypoints;\n        return __generator(this, function (_a) {\n            switch (_a.label) {\n                case 0:\n                    totalScore = 0.0;\n                    heatmapValues = decode_single_pose_util_1.argmax2d(heatmapScores);\n                    return [4 /*yield*/, Promise.all([heatmapScores.buffer(), offsets.buffer(), heatmapValues.buffer()])];\n                case 1:\n                    allTensorBuffers = _a.sent();\n                    scoresBuffer = allTensorBuffers[0];\n                    offsetsBuffer = allTensorBuffers[1];\n                    heatmapValuesBuffer = allTensorBuffers[2];\n                    offsetPoints = decode_single_pose_util_1.getOffsetPoints(heatmapValuesBuffer, outputStride, offsetsBuffer);\n                    return [4 /*yield*/, offsetPoints.buffer()];\n                case 2:\n                    offsetPointsBuffer = _a.sent();\n                    keypointConfidence = Array.from(decode_single_pose_util_1.getPointsConfidence(scoresBuffer, heatmapValuesBuffer));\n                    keypoints = keypointConfidence.map(function (score, keypointId) {\n                        totalScore += score;\n                        return {\n                            y: offsetPointsBuffer.get(keypointId, 0),\n                            x: offsetPointsBuffer.get(keypointId, 1),\n                            score: score,\n                            name: constants_1.COCO_KEYPOINTS[keypointId]\n                        };\n                    });\n                    heatmapValues.dispose();\n                    offsetPoints.dispose();\n                    return [2 /*return*/, { keypoints: keypoints, score: totalScore / keypoints.length }];\n            }\n        });\n    });\n}\nexports.decodeSinglePose = decodeSinglePose;\n//# sourceMappingURL=decode_single_pose.js.map","\n/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nObject.defineProperty(exports, \"__esModule\", { value: true });\nvar tf = require(\"@tensorflow/tfjs-core\");\nvar constants_1 = require(\"../../constants\");\nfunction mod(a, b) {\n    return tf.tidy(function () {\n        var floored = tf.div(a, tf.scalar(b, 'int32'));\n        return tf.sub(a, tf.mul(floored, tf.scalar(b, 'int32')));\n    });\n}\nfunction argmax2d(inputs) {\n    var _a = inputs.shape, height = _a[0], width = _a[1], depth = _a[2];\n    return tf.tidy(function () {\n        var reshaped = tf.reshape(inputs, [height * width, depth]);\n        var coords = tf.argMax(reshaped, 0);\n        var yCoords = tf.expandDims(tf.div(coords, tf.scalar(width, 'int32')), 1);\n        var xCoords = tf.expandDims(mod(coords, width), 1);\n        return tf.concat([yCoords, xCoords], 1);\n    });\n}\nexports.argmax2d = argmax2d;\nfunction getPointsConfidence(heatmapScores, heatMapCoords) {\n    var numKeypoints = heatMapCoords.shape[0];\n    var result = new Float32Array(numKeypoints);\n    for (var keypoint = 0; keypoint < numKeypoints; keypoint++) {\n        var y = heatMapCoords.get(keypoint, 0);\n        var x = heatMapCoords.get(keypoint, 1);\n        result[keypoint] = heatmapScores.get(y, x, keypoint);\n    }\n    return result;\n}\nexports.getPointsConfidence = getPointsConfidence;\nfunction getOffsetPoints(heatMapCoordsBuffer, outputStride, offsetsBuffer) {\n    return tf.tidy(function () {\n        var offsetVectors = getOffsetVectors(heatMapCoordsBuffer, offsetsBuffer);\n        return tf.add(tf.cast(tf.mul(heatMapCoordsBuffer.toTensor(), tf.scalar(outputStride, 'int32')), 'float32'), offsetVectors);\n    });\n}\nexports.getOffsetPoints = getOffsetPoints;\nfunction getOffsetVectors(heatMapCoordsBuffer, offsetsBuffer) {\n    var result = [];\n    for (var keypoint = 0; keypoint < constants_1.COCO_KEYPOINTS.length; keypoint++) {\n        var heatmapY = heatMapCoordsBuffer.get(keypoint, 0).valueOf();\n        var heatmapX = heatMapCoordsBuffer.get(keypoint, 1).valueOf();\n        var _a = getOffsetPoint(heatmapY, heatmapX, keypoint, offsetsBuffer), x = _a.x, y = _a.y;\n        result.push(y);\n        result.push(x);\n    }\n    return tf.tensor2d(result, [constants_1.COCO_KEYPOINTS.length, 2]);\n}\nexports.getOffsetVectors = getOffsetVectors;\nfunction getOffsetPoint(y, x, keypoint, offsetsBuffer) {\n    return {\n        y: offsetsBuffer.get(y, x, keypoint),\n        x: offsetsBuffer.get(y, x, keypoint + constants_1.COCO_KEYPOINTS.length)\n    };\n}\n//# sourceMappingURL=decode_single_pose_util.js.map","\n/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nObject.defineProperty(exports, \"__esModule\", { value: true });\nfunction flipPosesHorizontal(poses, imageSize) {\n    for (var _i = 0, poses_1 = poses; _i < poses_1.length; _i++) {\n        var pose = poses_1[_i];\n        for (var _a = 0, _b = pose.keypoints; _a < _b.length; _a++) {\n            var kp = _b[_a];\n            kp.x = imageSize.width - 1 - kp.x;\n        }\n    }\n    return poses;\n}\nexports.flipPosesHorizontal = flipPosesHorizontal;\n//# sourceMappingURL=flip_poses.js.map","\n/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nObject.defineProperty(exports, \"__esModule\", { value: true });\nfunction scalePoses(poses, imageSize, inputResolution, padding) {\n    var height = imageSize.height, width = imageSize.width;\n    var scaleY = height / (inputResolution.height * (1 - padding.top - padding.bottom));\n    var scaleX = width / (inputResolution.width * (1 - padding.left - padding.right));\n    var offsetY = -padding.top * inputResolution.height;\n    var offsetX = -padding.left * inputResolution.width;\n    if (scaleX === 1 && scaleY === 1 && offsetY === 0 && offsetX === 0) {\n        return poses;\n    }\n    for (var _i = 0, poses_1 = poses; _i < poses_1.length; _i++) {\n        var pose = poses_1[_i];\n        for (var _a = 0, _b = pose.keypoints; _a < _b.length; _a++) {\n            var kp = _b[_a];\n            kp.x = (kp.x + offsetX) * scaleX;\n            kp.y = (kp.y + offsetY) * scaleY;\n        }\n    }\n    return poses;\n}\nexports.scalePoses = scalePoses;\n//# sourceMappingURL=scale_poses.js.map","\n/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nvar __assign = (this && this.__assign) || function () {\n    __assign = Object.assign || function(t) {\n        for (var s, i = 1, n = arguments.length; i < n; i++) {\n            s = arguments[i];\n            for (var p in s) if (Object.prototype.hasOwnProperty.call(s, p))\n                t[p] = s[p];\n        }\n        return t;\n    };\n    return __assign.apply(this, arguments);\n};\nObject.defineProperty(exports, \"__esModule\", { value: true });\nvar tf = require(\"@tensorflow/tfjs-core\");\nvar constants_1 = require(\"./constants\");\nfunction validateModelConfig(modelConfig) {\n    var config = modelConfig || constants_1.MOBILENET_V1_CONFIG;\n    if (config.architecture == null) {\n        config.architecture = 'MobileNetV1';\n    }\n    if (constants_1.VALID_ARCHITECTURE.indexOf(config.architecture) < 0) {\n        throw new Error(\"Invalid architecture \" + config.architecture + \". \" +\n            (\"Should be one of \" + constants_1.VALID_ARCHITECTURE));\n    }\n    if (config.inputResolution == null) {\n        config.inputResolution = { height: 257, width: 257 };\n    }\n    if (config.outputStride == null) {\n        config.outputStride = 16;\n    }\n    if (constants_1.VALID_STRIDE[config.architecture].indexOf(config.outputStride) < 0) {\n        throw new Error(\"Invalid outputStride \" + config.outputStride + \". \" +\n            (\"Should be one of \" + constants_1.VALID_STRIDE[config.architecture] + \" \") +\n            (\"for architecture \" + config.architecture + \".\"));\n    }\n    if (config.multiplier == null) {\n        config.multiplier = 1.0;\n    }\n    if (constants_1.VALID_MULTIPLIER[config.architecture].indexOf(config.multiplier) < 0) {\n        throw new Error(\"Invalid multiplier \" + config.multiplier + \". \" +\n            (\"Should be one of \" + constants_1.VALID_MULTIPLIER[config.architecture] + \" \") +\n            (\"for architecture \" + config.architecture + \".\"));\n    }\n    if (config.quantBytes == null) {\n        config.quantBytes = 4;\n    }\n    if (constants_1.VALID_QUANT_BYTES.indexOf(config.quantBytes) < 0) {\n        throw new Error(\"Invalid quantBytes \" + config.quantBytes + \". \" +\n            (\"Should be one of \" + constants_1.VALID_QUANT_BYTES + \" \") +\n            (\"for architecture \" + config.architecture + \".\"));\n    }\n    if (config.architecture === 'MobileNetV1' && config.outputStride === 32 &&\n        config.multiplier !== 1) {\n        throw new Error(\"When using an output stride of 32, \" +\n            \"you must select 1 as the multiplier.\");\n    }\n    return config;\n}\nexports.validateModelConfig = validateModelConfig;\nfunction assertValidOutputStride(outputStride) {\n    tf.util.assert(constants_1.VALID_OUTPUT_STRIDES.indexOf(outputStride) >= 0, function () { return \"outputStride of \" + outputStride + \" is invalid. \" +\n        \"It must be either 8 or 16.\"; });\n}\nexports.assertValidOutputStride = assertValidOutputStride;\nfunction isValidInputResolution(resolution, outputStride) {\n    return (resolution - 1) % outputStride === 0;\n}\nfunction assertValidResolution(resolution, outputStride) {\n    tf.util.assert(isValidInputResolution(resolution.height, outputStride), function () { return \"height of \" + resolution.height + \" is invalid for output stride \" +\n        (outputStride + \".\"); });\n    tf.util.assert(isValidInputResolution(resolution.width, outputStride), function () { return \"width of \" + resolution.width + \" is invalid for output stride \" +\n        (outputStride + \".\"); });\n}\nexports.assertValidResolution = assertValidResolution;\nfunction validateEstimationConfig(estimationConfig) {\n    var config = estimationConfig;\n    if (config.maxPoses == null) {\n        config.maxPoses = 1;\n    }\n    if (config.maxPoses <= 0) {\n        throw new Error(\"Invalid maxPoses \" + config.maxPoses + \". Should be > 0.\");\n    }\n    if (config.maxPoses > 1) {\n        // Multi-poses estimation, needs additional check for multi-poses\n        // parameters.\n        config = __assign({}, constants_1.MULTI_PERSON_ESTIMATION_CONFIG, config);\n        if (config.scoreThreshold < 0.0 || config.scoreThreshold > 1.0) {\n            throw new Error(\"Invalid scoreThreshold \" + config.scoreThreshold + \". \" +\n                \"Should be in range [0.0, 1.0]\");\n        }\n        if (config.nmsRadius <= 0) {\n            throw new Error(\"Invalid nmsRadius \" + config.nmsRadius + \".\");\n        }\n    }\n    return config;\n}\nexports.validateEstimationConfig = validateEstimationConfig;\n//# sourceMappingURL=detector_utils.js.map","\n/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nObject.defineProperty(exports, \"__esModule\", { value: true });\nvar MOBILENET_BASE_URL = 'https://storage.googleapis.com/tfjs-models/savedmodel/posenet/mobilenet/';\nvar RESNET50_BASE_URL = 'https://storage.googleapis.com/tfjs-models/savedmodel/posenet/resnet50/';\n// The PoseNet 2.0 ResNet50 models use the latest TensorFlow.js 1.0 model\n// format.\nfunction resNet50Checkpoint(stride, quantBytes) {\n    var graphJson = \"model-stride\" + stride + \".json\";\n    // quantBytes=4 corresponding to the non-quantized full-precision checkpoints.\n    if (quantBytes === 4) {\n        return RESNET50_BASE_URL + \"float/\" + graphJson;\n    }\n    else {\n        return RESNET50_BASE_URL + (\"quant\" + quantBytes + \"/\") + graphJson;\n    }\n}\nexports.resNet50Checkpoint = resNet50Checkpoint;\n// The PoseNet 2.0 MobileNetV1 models use the latest TensorFlow.js 1.0 model\n// format.\nfunction mobileNetCheckpoint(stride, multiplier, quantBytes) {\n    var toStr = { 1.0: '100', 0.75: '075', 0.50: '050' };\n    var graphJson = \"model-stride\" + stride + \".json\";\n    // quantBytes=4 corresponding to the non-quantized full-precision checkpoints.\n    if (quantBytes === 4) {\n        return MOBILENET_BASE_URL + (\"float/\" + toStr[multiplier] + \"/\") + graphJson;\n    }\n    else {\n        return MOBILENET_BASE_URL + (\"quant\" + quantBytes + \"/\" + toStr[multiplier] + \"/\") +\n            graphJson;\n    }\n}\nexports.mobileNetCheckpoint = mobileNetCheckpoint;\nfunction getValidInputResolutionDimensions(inputResolution, outputStride) {\n    return {\n        height: toValidInputResolution(inputResolution.height, outputStride),\n        width: toValidInputResolution(inputResolution.width, outputStride)\n    };\n}\nexports.getValidInputResolutionDimensions = getValidInputResolutionDimensions;\nfunction toValidInputResolution(inputResolution, outputStride) {\n    if (isValidInputResolution(inputResolution, outputStride)) {\n        return inputResolution;\n    }\n    return Math.floor(inputResolution / outputStride) * outputStride + 1;\n}\nexports.toValidInputResolution = toValidInputResolution;\nfunction isValidInputResolution(resolution, outputStride) {\n    return (resolution - 1) % outputStride === 0;\n}\n//# sourceMappingURL=load_utils.js.map"]}